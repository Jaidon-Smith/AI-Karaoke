{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "##### <a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/STT%20JVS%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo fusermount -u ~/general-304503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503’: File exists\n",
      "Using mount point: /home/jsjsrobert500/general-304503\n",
      "2021/03/16 01:42:10.768570 Opening GCS connection...\n",
      "2021/03/16 01:42:10.945469 Mounting file system...\n",
      "2021/03/16 01:42:10.948109 File system has been successfully mounted.\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/general-304503\n",
    "!gcsfuse general-304503 ~/general-304503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnmItTWZe0ay",
    "outputId": "7a27ca64-ee73-4f15-d8f6-f6ae8e3689eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfds-nightly in /home/jsjsrobert500/.local/lib/python3.7/site-packages (4.2.0.dev202103150106)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.15.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.25.1)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.10.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (4.58.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (20.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.19.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (5.1.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.27.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.18.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (1.26.3)\n",
      "Requirement already satisfied: zipp>=0.4 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->tfds-nightly) (3.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tfds-nightly) (1.52.0)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.7/site-packages (0.25.1)\n",
      "Requirement already satisfied: tensorflow_io in /opt/conda/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_io) (2.4.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.15.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.6.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/jsjsrobert500/.local/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.15.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.3.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.27.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (54.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user tfds-nightly\n",
    "!pip install --user pydub\n",
    "!pip install --user tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "CaxFPa1ne2Xm"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpLtjhXMepL-",
    "outputId": "0c698c66-b28c-48bc-c73d-1b6ad75e3900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'public_datasets' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Jaidon-Smith/public_datasets.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8OZoOWZCXvm"
   },
   "source": [
    "# Setting Up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "AbmSdYojCgDz"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "WZrsymk7E5DH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/notebook_logs/’: File exists\n",
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/notebook_logs/jvs_transformer_stt_12_3_21’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Initialise Logs\n",
    "\n",
    "!rm -r ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/logs\n",
    "\n",
    "%mkdir ~/general-304503/notebook_logs/\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "SdZusIv6_LiI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/notebook_logs/’: File exists\n",
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/notebook_logs/jvs_transformer_stt_12_3_21’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Initialise Checkpoints\n",
    "!rm -r ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkpoints\n",
    "\n",
    "%mkdir ~/general-304503/notebook_logs/\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "LP5tugbE-vbI"
   },
   "outputs": [],
   "source": [
    "logdir = 'gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/logs'\n",
    "checkpoint_path = 'gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "VimJg7KRCb3G"
   },
   "outputs": [],
   "source": [
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkMA6i1TVy1S"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-16 01:42:20--  https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq\n",
      "Resolving docs.google.com (docs.google.com)... 108.177.120.113, 108.177.120.102, 108.177.120.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|108.177.120.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2inik6330hh37q5hl5iiv59i8j5m1h73/1615858875000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-03-16 01:42:20--  https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2inik6330hh37q5hl5iiv59i8j5m1h73/1615858875000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download\n",
      "Resolving doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)... 173.194.195.132, 2607:f8b0:4001:c11::84\n",
      "Connecting to doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)|173.194.195.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 276245 (270K) [application/octet-stream]\n",
      "Saving to: ‘hiragana_jsut.model’\n",
      "\n",
      "hiragana_jsut.model 100%[===================>] 269.77K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-03-16 01:42:21 (101 MB/s) - ‘hiragana_jsut.model’ saved [276245/276245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq' -O hiragana_jsut.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBFpGwTo9kUs",
    "outputId": "cb5cf1d0-af18-4d6d-c9ae-6dccf80c8347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet tensorflow-text\n",
    "\n",
    "import tensorflow_text as text\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "model_file = 'hiragana_jsut.model'\n",
    "model = gfile.GFile(model_file, 'rb').read()\n",
    "\n",
    "tokenizer = text.SentencepieceTokenizer(model=model)\n",
    "\n",
    "input_vocab_size = tokenizer.vocab_size().numpy()\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyeaq-FTCB7X",
    "outputId": "97ce5020-8122-4856-b85d-e28483b8843e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pykakasi in /home/jsjsrobert500/.local/lib/python3.7/site-packages (2.0.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from pykakasi) (3.7.0)\n",
      "Requirement already satisfied: klepto in /home/jsjsrobert500/.local/lib/python3.7/site-packages (from pykakasi) (0.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (3.7.4.3)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/lib/python3.7/site-packages (from klepto->pykakasi) (0.3.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /opt/conda/lib/python3.7/site-packages (from klepto->pykakasi) (0.2.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "rWcfthvAEbZI"
   },
   "outputs": [],
   "source": [
    "import pykakasi\n",
    "converter = pykakasi.kakasi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "154oNA4E6hEg"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(old_text):\n",
    "  text = old_text\n",
    "  text = tokenizer.tokenize(text)\n",
    "  text = tf.pad(text, paddings=[[0 , 200 - tf.shape(text)[0]]])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "LiqQD4F8j8YJ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_no_pad(old_text):\n",
    "  text = old_text\n",
    "  text = tokenizer.tokenize(text)\n",
    "  text = tf.shape(text)[0]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "VxIOOV5_pS58"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(old_audio, original_sample_rate):\n",
    "  audio = old_audio/tf.int16.max\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
    "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
    "  audio = tf.pad(audio, paddings=[[0 , 500 - tf.shape(audio)[0]], [0,0]])\n",
    "  return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "nCRZuRVb4H_d"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio_no_pad(old_audio, original_sample_rate):\n",
    "  audio = old_audio/tf.int16.max\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
    "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
    "  audio = tf.shape(audio)[0]\n",
    "  return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "SSaiGkuUZYFL"
   },
   "outputs": [],
   "source": [
    "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
    "\n",
    "jsut_dataset, info = tfds.load(\n",
    "                    \"jsut\",\n",
    "                    split=\"train\",\n",
    "                    data_dir='gs://general-304503/public_datasets',\n",
    "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
    "                    with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "p4dWKtfoEyKy"
   },
   "outputs": [],
   "source": [
    "def convert_to_kana(text):\n",
    "  #kana = converter.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')\n",
    "  new_text = text.numpy().decode('utf-8')\n",
    "  kana = converter.convert(new_text)\n",
    "  return ''.join([i['hira'] for i in kana])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "dsITXisWE81g"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": x[\"speech\"],\n",
    "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "5TEZzObuMtUW"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
    "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
    "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
    "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "htZ7O_L6ZPMm"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.batch(8)\n",
    "jsut_dataset = jsut_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lm95V3vntL6"
   },
   "source": [
    "## jvs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "u1GQRngMnrUx"
   },
   "outputs": [],
   "source": [
    "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
    "\n",
    "jvs_dataset, info = tfds.load(\n",
    "                    \"jvs\",\n",
    "                    split=\"train\",\n",
    "                    data_dir='gs://general-304503/public_datasets',\n",
    "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
    "                    with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "Yc2xw1UvnrUx"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": x[\"speech\"],\n",
    "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "pOMQs6x_nrUx"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
    "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
    "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
    "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "Iv1ibtCpnrUy"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.batch(8)\n",
    "jvs_dataset = jvs_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5WxMKAI-CF3"
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "ds = jvs_dataset.take(1)\n",
    "\n",
    "for i in ds:\n",
    "  data = i\n",
    "  break\n",
    "\n",
    "speech = data['speech']\n",
    "text = data['text']\n",
    "id = data['id']\n",
    "speech_lengths = data['speech_lengths']\n",
    "text_lengths = data['text_lengths']\n",
    "\n",
    "tokenizer.detokenize(text.numpy()[1]).numpy().decode('utf-8')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-EgkkRmnrUy",
    "outputId": "9638a475-079e-4dbc-c36d-236d49bbb039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2000, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = tokenizer.vocab_size()\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjiiyX82XU5"
   },
   "source": [
    "# Using Transformer Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "xCSRf4_0wZHZ"
   },
   "outputs": [],
   "source": [
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "CJ1HYBRd6NQu"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "_fHkHLsK22m5"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "wWLWmM3MPr5V"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "eR9vt3slYa6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "JpqOGPpKYfuf"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "F2lvYoMO4j1w"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.embedding = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training=True):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Makes input larger by a constant\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "rIkO24ZTsYng"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "kFvfDRy_sgBc"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "MaKn44ztspmJ"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "instant_loss = tf.keras.metrics.Mean(name='instant_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "GVPnnLizwA3m"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           maximum_position_encoding)\n",
    "    self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "  def call(self, inp, training=True):\n",
    "    enc_output = self.encoder(inp, training)  # (batch_size, inp_seq_len, d_model)\n",
    "    final_output = self.final_layer(enc_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    return final_output\n",
    "\n",
    "transformer = Transformer(num_layers=6,\n",
    "          d_model=d_model,\n",
    "          num_heads=8,\n",
    "          dff=2048, \n",
    "          maximum_position_encoding=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "1Y1kDSm9st_Q"
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "hXw3bhf7tGfJ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(speech, text, speech_lengths, text_lengths):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = transformer(speech)\n",
    "\n",
    "    labels = text\n",
    "    label_length = text_lengths\n",
    "    logit_length = speech_lengths\n",
    "    unique = tf.nn.ctc_unique_labels(text)\n",
    "\n",
    "\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
    "    )\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  instant_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "VgTWTimuzAtQ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(speech, text, speech_lengths, text_lengths):\n",
    "  logits = transformer(speech)\n",
    "\n",
    "  labels = text\n",
    "  label_length = text_lengths\n",
    "  logit_length = speech_lengths\n",
    "  unique = tf.nn.ctc_unique_labels(text)\n",
    "\n",
    "\n",
    "  loss = tf.nn.ctc_loss(\n",
    "      labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
    "  )\n",
    "\n",
    "\n",
    "  validation_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "XsdWdYE11Kea"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "validate_period = 30\n",
    "save_period = 200\n",
    "print_period = 1\n",
    "train_period = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "Xn0oW1kyp0qr"
   },
   "outputs": [],
   "source": [
    "dataset = jvs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AhEzjP3ztJYz",
    "outputId": "ed3e04ec-8c39-4d7b-ca5d-c21bf516bf59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 568.7057\n",
      "Epoch 1 Batch 1 Loss 534.1376\n",
      "Epoch 1 Batch 2 Loss 581.6332\n",
      "Epoch 1 Batch 3 Loss 540.1230\n",
      "Epoch 1 Batch 4 Loss 580.5500\n",
      "Epoch 1 Batch 5 Loss 715.5375\n",
      "Epoch 1 Batch 6 Loss 432.8978\n",
      "Epoch 1 Batch 7 Loss 491.5380\n",
      "Epoch 1 Batch 8 Loss 648.2237\n",
      "Epoch 1 Batch 9 Loss 499.2842\n",
      "Epoch 1 Batch 10 Loss 469.4774\n",
      "Epoch 1 Batch 11 Loss 471.4719\n",
      "Epoch 1 Batch 12 Loss 632.8846\n",
      "Epoch 1 Batch 13 Loss 475.6606\n",
      "Epoch 1 Batch 14 Loss 661.6863\n",
      "Epoch 1 Batch 15 Loss 442.8964\n",
      "Epoch 1 Batch 16 Loss 539.0316\n",
      "Epoch 1 Batch 17 Loss 455.4348\n",
      "Epoch 1 Batch 18 Loss 529.8572\n",
      "Epoch 1 Batch 19 Loss 426.7677\n",
      "Epoch 1 Batch 20 Loss 354.0236\n",
      "Epoch 1 Batch 21 Loss 559.0055\n",
      "Epoch 1 Batch 22 Loss 359.3776\n",
      "Epoch 1 Batch 23 Loss 453.7595\n",
      "Epoch 1 Batch 24 Loss 473.1254\n",
      "Epoch 1 Batch 25 Loss 438.0790\n",
      "Epoch 1 Batch 26 Loss 381.3424\n",
      "Epoch 1 Batch 27 Loss 417.4186\n",
      "Epoch 1 Batch 28 Loss 335.9699\n",
      "Epoch 1 Batch 29 Loss 334.9075\n",
      "Training Print: Epoch 1 Batch 29 Loss 493.4936\n",
      "Validation Print: Epoch 1 Batch 29 Loss 442.0328\n",
      "Epoch 1 Batch 30 Loss 261.2659\n",
      "Epoch 1 Batch 31 Loss 336.5190\n",
      "Epoch 1 Batch 32 Loss 267.3220\n",
      "Epoch 1 Batch 33 Loss 282.0566\n",
      "Epoch 1 Batch 34 Loss 235.7607\n",
      "Epoch 1 Batch 35 Loss 285.5890\n",
      "Epoch 1 Batch 36 Loss 216.6525\n",
      "Epoch 1 Batch 37 Loss 243.4007\n",
      "Epoch 1 Batch 38 Loss 253.1187\n",
      "Epoch 1 Batch 39 Loss 184.8378\n",
      "Epoch 1 Batch 40 Loss 168.0593\n",
      "Epoch 1 Batch 41 Loss 176.5365\n",
      "Epoch 1 Batch 42 Loss 200.3641\n",
      "Epoch 1 Batch 43 Loss 170.0655\n",
      "Epoch 1 Batch 44 Loss 167.6285\n",
      "Epoch 1 Batch 45 Loss 152.7393\n",
      "Epoch 1 Batch 46 Loss 209.8119\n",
      "Epoch 1 Batch 47 Loss 178.2139\n",
      "Epoch 1 Batch 48 Loss 141.0202\n",
      "Epoch 1 Batch 49 Loss 156.3652\n",
      "Epoch 1 Batch 50 Loss 164.2954\n",
      "Epoch 1 Batch 51 Loss 161.8215\n",
      "Epoch 1 Batch 52 Loss 132.5719\n",
      "Epoch 1 Batch 53 Loss 177.6521\n",
      "Epoch 1 Batch 54 Loss 157.0096\n",
      "Epoch 1 Batch 55 Loss 188.5558\n",
      "Epoch 1 Batch 56 Loss 134.4357\n",
      "Epoch 1 Batch 57 Loss 192.9614\n",
      "Epoch 1 Batch 58 Loss 160.9278\n",
      "Epoch 1 Batch 59 Loss 151.3531\n",
      "Training Print: Epoch 1 Batch 59 Loss 196.9637\n",
      "Validation Print: Epoch 1 Batch 59 Loss 168.2005\n",
      "Epoch 1 Batch 60 Loss 202.7110\n",
      "Epoch 1 Batch 61 Loss 128.9639\n",
      "Epoch 1 Batch 62 Loss 152.4429\n",
      "Epoch 1 Batch 63 Loss 185.8987\n",
      "Epoch 1 Batch 64 Loss 163.0376\n",
      "Epoch 1 Batch 65 Loss 145.3190\n",
      "Epoch 1 Batch 66 Loss 184.5523\n",
      "Epoch 1 Batch 67 Loss 144.1452\n",
      "Epoch 1 Batch 68 Loss 119.3670\n",
      "Epoch 1 Batch 69 Loss 181.3598\n",
      "Epoch 1 Batch 70 Loss 158.3836\n",
      "Epoch 1 Batch 71 Loss 128.6063\n",
      "Epoch 1 Batch 72 Loss 143.1871\n",
      "Epoch 1 Batch 73 Loss 119.1031\n",
      "Epoch 1 Batch 74 Loss 133.6909\n",
      "Epoch 1 Batch 75 Loss 123.5965\n",
      "Epoch 1 Batch 76 Loss 122.9118\n",
      "Epoch 1 Batch 77 Loss 170.3825\n",
      "Epoch 1 Batch 78 Loss 171.2240\n",
      "Epoch 1 Batch 79 Loss 145.7757\n",
      "Epoch 1 Batch 80 Loss 155.5005\n",
      "Epoch 1 Batch 81 Loss 131.5541\n",
      "Epoch 1 Batch 82 Loss 135.8527\n",
      "Epoch 1 Batch 83 Loss 147.2823\n",
      "Epoch 1 Batch 84 Loss 165.0205\n",
      "Epoch 1 Batch 85 Loss 136.2477\n",
      "Epoch 1 Batch 86 Loss 164.8709\n",
      "Epoch 1 Batch 87 Loss 161.8643\n",
      "Epoch 1 Batch 88 Loss 183.7643\n",
      "Epoch 1 Batch 89 Loss 125.6280\n",
      "Training Print: Epoch 1 Batch 89 Loss 151.0748\n",
      "Validation Print: Epoch 1 Batch 89 Loss 162.8662\n",
      "Epoch 1 Batch 90 Loss 145.1725\n",
      "Epoch 1 Batch 91 Loss 171.7218\n",
      "Epoch 1 Batch 92 Loss 180.4161\n",
      "Epoch 1 Batch 93 Loss 145.4357\n",
      "Epoch 1 Batch 94 Loss 145.8033\n",
      "Epoch 1 Batch 95 Loss 133.3101\n",
      "Epoch 1 Batch 96 Loss 143.2531\n",
      "Epoch 1 Batch 97 Loss 158.6070\n",
      "Epoch 1 Batch 98 Loss 140.8294\n",
      "Epoch 1 Batch 99 Loss 182.8074\n",
      "Epoch 1 Batch 100 Loss 138.7899\n",
      "Epoch 1 Batch 101 Loss 156.0790\n",
      "Epoch 1 Batch 102 Loss 157.6550\n",
      "Epoch 1 Batch 103 Loss 165.6491\n",
      "Epoch 1 Batch 104 Loss 127.8805\n",
      "Epoch 1 Batch 105 Loss 168.3626\n",
      "Epoch 1 Batch 106 Loss 148.6411\n",
      "Epoch 1 Batch 107 Loss 134.6098\n",
      "Epoch 1 Batch 108 Loss 155.6478\n",
      "Epoch 1 Batch 109 Loss 114.3230\n",
      "Epoch 1 Batch 110 Loss 150.2661\n",
      "Epoch 1 Batch 111 Loss 144.8993\n",
      "Epoch 1 Batch 112 Loss 154.4826\n",
      "Epoch 1 Batch 113 Loss 236.2604\n",
      "Epoch 1 Batch 114 Loss 119.8465\n",
      "Epoch 1 Batch 115 Loss 110.6441\n",
      "Epoch 1 Batch 116 Loss 145.7452\n",
      "Epoch 1 Batch 117 Loss 144.9625\n",
      "Epoch 1 Batch 118 Loss 132.5262\n",
      "Epoch 1 Batch 119 Loss 117.5172\n",
      "Training Print: Epoch 1 Batch 119 Loss 149.0715\n",
      "Validation Print: Epoch 1 Batch 119 Loss 159.1817\n",
      "Epoch 1 Batch 120 Loss 123.5835\n",
      "Epoch 1 Batch 121 Loss 139.8486\n",
      "Epoch 1 Batch 122 Loss 150.1677\n",
      "Epoch 1 Batch 123 Loss 152.5705\n",
      "Epoch 1 Batch 124 Loss 125.5281\n",
      "Epoch 1 Batch 125 Loss 159.4471\n",
      "Epoch 1 Batch 126 Loss 116.1818\n",
      "Epoch 1 Batch 127 Loss 134.2935\n",
      "Epoch 1 Batch 128 Loss 149.9470\n",
      "Epoch 1 Batch 129 Loss 167.6026\n",
      "Epoch 1 Batch 130 Loss 132.5268\n",
      "Epoch 1 Batch 131 Loss 135.8723\n",
      "Epoch 1 Batch 132 Loss 148.7576\n",
      "Epoch 1 Batch 133 Loss 149.3047\n",
      "Epoch 1 Batch 134 Loss 153.5839\n",
      "Epoch 1 Batch 135 Loss 134.7487\n",
      "Epoch 1 Batch 136 Loss 141.5555\n",
      "Epoch 1 Batch 137 Loss 132.3023\n",
      "Epoch 1 Batch 138 Loss 118.3299\n",
      "Epoch 1 Batch 139 Loss 131.8828\n",
      "Epoch 1 Batch 140 Loss 154.8905\n",
      "Epoch 1 Batch 141 Loss 124.1221\n",
      "Epoch 1 Batch 142 Loss 146.8274\n",
      "Epoch 1 Batch 143 Loss 124.4637\n",
      "Epoch 1 Batch 144 Loss 174.7510\n",
      "Epoch 1 Batch 145 Loss 173.2397\n",
      "Epoch 1 Batch 146 Loss 152.4309\n",
      "Epoch 1 Batch 147 Loss 147.5953\n",
      "Epoch 1 Batch 148 Loss 115.9699\n",
      "Epoch 1 Batch 149 Loss 105.5686\n",
      "Training Print: Epoch 1 Batch 149 Loss 140.5965\n",
      "Validation Print: Epoch 1 Batch 149 Loss 153.0165\n",
      "Epoch 1 Batch 150 Loss 142.5831\n",
      "Epoch 1 Batch 151 Loss 124.8660\n",
      "Epoch 1 Batch 152 Loss 117.0470\n",
      "Epoch 1 Batch 153 Loss 164.7241\n",
      "Epoch 1 Batch 154 Loss 123.8897\n",
      "Epoch 1 Batch 155 Loss 137.1920\n",
      "Epoch 1 Batch 156 Loss 160.3451\n",
      "Epoch 1 Batch 157 Loss 135.1987\n",
      "Epoch 1 Batch 158 Loss 165.0895\n",
      "Epoch 1 Batch 159 Loss 152.9079\n",
      "Epoch 1 Batch 160 Loss 144.2576\n",
      "Epoch 1 Batch 161 Loss 149.7005\n",
      "Epoch 1 Batch 162 Loss 149.9568\n",
      "Epoch 1 Batch 163 Loss 142.5282\n",
      "Epoch 1 Batch 164 Loss 139.4450\n",
      "Epoch 1 Batch 165 Loss 137.0123\n",
      "Epoch 1 Batch 166 Loss 121.5707\n",
      "Epoch 1 Batch 167 Loss 119.9490\n",
      "Epoch 1 Batch 168 Loss 162.6193\n",
      "Epoch 1 Batch 169 Loss 168.8960\n",
      "Epoch 1 Batch 170 Loss 130.1658\n",
      "Epoch 1 Batch 171 Loss 127.0855\n",
      "Epoch 1 Batch 172 Loss 127.8448\n",
      "Epoch 1 Batch 173 Loss 158.4789\n",
      "Epoch 1 Batch 174 Loss 110.9705\n",
      "Epoch 1 Batch 175 Loss 139.2338\n",
      "Epoch 1 Batch 176 Loss 114.1672\n",
      "Epoch 1 Batch 177 Loss 116.7591\n",
      "Epoch 1 Batch 178 Loss 135.5590\n",
      "Epoch 1 Batch 179 Loss 114.5404\n",
      "Training Print: Epoch 1 Batch 179 Loss 137.8194\n",
      "Validation Print: Epoch 1 Batch 179 Loss 147.9284\n",
      "Epoch 1 Batch 180 Loss 150.7201\n",
      "Epoch 1 Batch 181 Loss 155.2240\n",
      "Epoch 1 Batch 182 Loss 155.3551\n",
      "Epoch 1 Batch 183 Loss 133.2175\n",
      "Epoch 1 Batch 184 Loss 133.7881\n",
      "Epoch 1 Batch 185 Loss 143.2609\n",
      "Epoch 1 Batch 186 Loss 129.7769\n",
      "Epoch 1 Batch 187 Loss 136.9866\n",
      "Epoch 1 Batch 188 Loss 118.4534\n",
      "Epoch 1 Batch 189 Loss 128.0691\n",
      "Epoch 1 Batch 190 Loss 132.6666\n",
      "Epoch 1 Batch 191 Loss 128.4651\n",
      "Epoch 1 Batch 192 Loss 130.8045\n",
      "Epoch 1 Batch 193 Loss 120.8672\n",
      "Epoch 1 Batch 194 Loss 150.0153\n",
      "Epoch 1 Batch 195 Loss 112.2991\n",
      "Epoch 1 Batch 196 Loss 144.6375\n",
      "Epoch 1 Batch 197 Loss 140.5141\n",
      "Epoch 1 Batch 198 Loss 144.5521\n",
      "Epoch 1 Batch 199 Loss 122.3111\n",
      "Saving checkpoint for batch 199 at gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkopoints/ckpt-1\n",
      "Epoch 1 Batch 200 Loss 121.3002\n",
      "Epoch 1 Batch 201 Loss 116.3009\n",
      "Epoch 1 Batch 202 Loss 121.8889\n",
      "Epoch 1 Batch 203 Loss 136.0792\n",
      "Epoch 1 Batch 204 Loss 125.6387\n",
      "Epoch 1 Batch 205 Loss 124.7263\n",
      "Epoch 1 Batch 206 Loss 130.4964\n",
      "Epoch 1 Batch 207 Loss 136.4751\n",
      "Epoch 1 Batch 208 Loss 136.1882\n",
      "Epoch 1 Batch 209 Loss 138.0868\n",
      "Training Print: Epoch 1 Batch 209 Loss 133.3055\n",
      "Validation Print: Epoch 1 Batch 209 Loss 145.6364\n",
      "Epoch 1 Batch 210 Loss 111.1574\n",
      "Epoch 1 Batch 211 Loss 133.8054\n",
      "Epoch 1 Batch 212 Loss 106.1817\n",
      "Epoch 1 Batch 213 Loss 120.0513\n",
      "Epoch 1 Batch 214 Loss 126.1156\n",
      "Epoch 1 Batch 215 Loss 113.7374\n",
      "Epoch 1 Batch 216 Loss 117.7645\n",
      "Epoch 1 Batch 217 Loss 139.0465\n",
      "Epoch 1 Batch 218 Loss 153.6826\n",
      "Epoch 1 Batch 219 Loss 138.7593\n",
      "Epoch 1 Batch 220 Loss 133.1712\n",
      "Epoch 1 Batch 221 Loss 115.6311\n",
      "Epoch 1 Batch 222 Loss 125.3852\n",
      "Epoch 1 Batch 223 Loss 150.3053\n",
      "Epoch 1 Batch 224 Loss 117.7487\n",
      "Epoch 1 Batch 225 Loss 115.0219\n",
      "Epoch 1 Batch 226 Loss 149.0809\n",
      "Epoch 1 Batch 227 Loss 113.4585\n",
      "Epoch 1 Batch 228 Loss 125.2913\n",
      "Epoch 1 Batch 229 Loss 124.7313\n",
      "Epoch 1 Batch 230 Loss 158.0630\n",
      "Epoch 1 Batch 231 Loss 127.7854\n",
      "Epoch 1 Batch 232 Loss 144.9276\n",
      "Epoch 1 Batch 233 Loss 136.5759\n",
      "Epoch 1 Batch 234 Loss 126.5666\n",
      "Epoch 1 Batch 235 Loss 133.6077\n",
      "Epoch 1 Batch 236 Loss 130.6670\n",
      "Epoch 1 Batch 237 Loss 141.1802\n",
      "Epoch 1 Batch 238 Loss 143.9963\n",
      "Epoch 1 Batch 239 Loss 111.3533\n",
      "Training Print: Epoch 1 Batch 239 Loss 129.4950\n",
      "Validation Print: Epoch 1 Batch 239 Loss 143.6273\n",
      "Epoch 1 Batch 240 Loss 167.7642\n",
      "Epoch 1 Batch 241 Loss 121.2651\n",
      "Epoch 1 Batch 242 Loss 110.6419\n",
      "Epoch 1 Batch 243 Loss 162.1792\n",
      "Epoch 1 Batch 244 Loss 133.7988\n",
      "Epoch 1 Batch 245 Loss 124.8653\n",
      "Epoch 1 Batch 246 Loss 114.3532\n",
      "Epoch 1 Batch 247 Loss 111.3561\n",
      "Epoch 1 Batch 248 Loss 156.1907\n",
      "Epoch 1 Batch 249 Loss 125.3149\n",
      "Epoch 1 Batch 250 Loss 112.9259\n",
      "Epoch 1 Batch 251 Loss 142.3392\n",
      "Epoch 1 Batch 252 Loss 116.0731\n",
      "Epoch 1 Batch 253 Loss 126.5979\n",
      "Epoch 1 Batch 254 Loss 156.3811\n",
      "Epoch 1 Batch 255 Loss 109.8502\n",
      "Epoch 1 Batch 256 Loss 97.9262\n",
      "Epoch 1 Batch 257 Loss 137.5022\n",
      "Epoch 1 Batch 258 Loss 126.5301\n",
      "Epoch 1 Batch 259 Loss 155.9400\n",
      "Epoch 1 Batch 260 Loss 116.7102\n",
      "Epoch 1 Batch 261 Loss 145.1441\n",
      "Epoch 1 Batch 262 Loss 132.6444\n",
      "Epoch 1 Batch 263 Loss 122.0656\n",
      "Epoch 1 Batch 264 Loss 109.8223\n",
      "Epoch 1 Batch 265 Loss 113.4668\n",
      "Epoch 1 Batch 266 Loss 130.5659\n",
      "Epoch 1 Batch 267 Loss 132.7179\n",
      "Epoch 1 Batch 268 Loss 114.2309\n",
      "Epoch 1 Batch 269 Loss 148.0685\n",
      "Training Print: Epoch 1 Batch 269 Loss 129.1744\n",
      "Validation Print: Epoch 1 Batch 269 Loss 141.6561\n",
      "Epoch 1 Batch 270 Loss 115.1262\n",
      "Epoch 1 Batch 271 Loss 132.5274\n",
      "Epoch 1 Batch 272 Loss 115.2783\n",
      "Epoch 1 Batch 273 Loss 122.7569\n",
      "Epoch 1 Batch 274 Loss 142.6066\n",
      "Epoch 1 Batch 275 Loss 105.2276\n",
      "Epoch 1 Batch 276 Loss 107.9233\n",
      "Epoch 1 Batch 277 Loss 128.3147\n",
      "Epoch 1 Batch 278 Loss 126.0517\n",
      "Epoch 1 Batch 279 Loss 129.0414\n",
      "Epoch 1 Batch 280 Loss 119.1099\n",
      "Epoch 1 Batch 281 Loss 93.8833\n",
      "Epoch 1 Batch 282 Loss 134.3081\n",
      "Epoch 1 Batch 283 Loss 143.1584\n",
      "Epoch 1 Batch 284 Loss 123.8798\n",
      "Epoch 1 Batch 285 Loss 116.2705\n",
      "Epoch 1 Batch 286 Loss 123.9892\n",
      "Epoch 1 Batch 287 Loss 112.2498\n",
      "Epoch 1 Batch 288 Loss 141.3599\n",
      "Epoch 1 Batch 289 Loss 147.5910\n",
      "Epoch 1 Batch 290 Loss 125.4097\n",
      "Epoch 1 Batch 291 Loss 148.4483\n",
      "Epoch 1 Batch 292 Loss 94.2561\n",
      "Epoch 1 Batch 293 Loss 161.8561\n",
      "Epoch 1 Batch 294 Loss 101.6938\n",
      "Epoch 1 Batch 295 Loss 107.3245\n",
      "Epoch 1 Batch 296 Loss 137.4280\n",
      "Epoch 1 Batch 297 Loss 138.3407\n",
      "Epoch 1 Batch 298 Loss 128.1550\n",
      "Epoch 1 Batch 299 Loss 134.0339\n",
      "Training Print: Epoch 1 Batch 299 Loss 125.2533\n",
      "Validation Print: Epoch 1 Batch 299 Loss 144.7059\n",
      "Epoch 1 Batch 300 Loss 122.7841\n",
      "Epoch 1 Batch 301 Loss 112.1545\n",
      "Epoch 1 Batch 302 Loss 120.8603\n",
      "Epoch 1 Batch 303 Loss 112.8583\n",
      "Epoch 1 Batch 304 Loss 104.2245\n",
      "Epoch 1 Batch 305 Loss 138.1701\n",
      "Epoch 1 Batch 306 Loss 132.0405\n",
      "Epoch 1 Batch 307 Loss 111.1298\n",
      "Epoch 1 Batch 308 Loss 140.8738\n",
      "Epoch 1 Batch 309 Loss 143.7675\n",
      "Epoch 1 Batch 310 Loss 137.3519\n",
      "Epoch 1 Batch 311 Loss 136.2897\n",
      "Epoch 1 Batch 312 Loss 114.8327\n",
      "Epoch 1 Batch 313 Loss 108.2510\n",
      "Epoch 1 Batch 314 Loss 107.8603\n",
      "Epoch 1 Batch 315 Loss 120.7854\n",
      "Epoch 1 Batch 316 Loss 126.0308\n",
      "Epoch 1 Batch 317 Loss 109.3949\n",
      "Epoch 1 Batch 318 Loss 141.5551\n",
      "Epoch 1 Batch 319 Loss 99.4797\n",
      "Epoch 1 Batch 320 Loss 121.6517\n",
      "Epoch 1 Batch 321 Loss 116.7122\n",
      "Epoch 1 Batch 322 Loss 135.1553\n",
      "Epoch 1 Batch 323 Loss 143.0385\n",
      "Epoch 1 Batch 324 Loss 146.2481\n",
      "Epoch 1 Batch 325 Loss 117.1824\n",
      "Epoch 1 Batch 326 Loss 126.9574\n",
      "Epoch 1 Batch 327 Loss 123.4925\n",
      "Epoch 1 Batch 328 Loss 109.4137\n",
      "Epoch 1 Batch 329 Loss 101.9750\n",
      "Training Print: Epoch 1 Batch 329 Loss 122.7507\n",
      "Validation Print: Epoch 1 Batch 329 Loss 143.3265\n",
      "Epoch 1 Batch 330 Loss 135.6797\n",
      "Epoch 1 Batch 331 Loss 117.5893\n",
      "Epoch 1 Batch 332 Loss 131.1244\n",
      "Epoch 1 Batch 333 Loss 120.6823\n",
      "Epoch 1 Batch 334 Loss 145.1955\n",
      "Epoch 1 Batch 335 Loss 124.3688\n",
      "Epoch 1 Batch 336 Loss 125.3854\n",
      "Epoch 1 Batch 337 Loss 98.3108\n",
      "Epoch 1 Batch 338 Loss 133.3588\n",
      "Epoch 1 Batch 339 Loss 109.8241\n",
      "Epoch 1 Batch 340 Loss 152.3723\n",
      "Epoch 1 Batch 341 Loss 131.7180\n",
      "Epoch 1 Batch 342 Loss 139.9433\n",
      "Epoch 1 Batch 343 Loss 145.0439\n",
      "Epoch 1 Batch 344 Loss 109.3451\n",
      "Epoch 1 Batch 345 Loss 123.7128\n",
      "Epoch 1 Batch 346 Loss 133.7668\n",
      "Epoch 1 Batch 347 Loss 123.9160\n",
      "Epoch 1 Batch 348 Loss 130.0161\n",
      "Epoch 1 Batch 349 Loss 129.5741\n",
      "Epoch 1 Batch 350 Loss 145.3544\n",
      "Epoch 1 Batch 351 Loss 107.8069\n",
      "Epoch 1 Batch 352 Loss 118.9635\n",
      "Epoch 1 Batch 353 Loss 125.3661\n",
      "Epoch 1 Batch 354 Loss 150.7768\n",
      "Epoch 1 Batch 355 Loss 90.7107\n",
      "Epoch 1 Batch 356 Loss 135.1359\n",
      "Epoch 1 Batch 357 Loss 136.1831\n",
      "Epoch 1 Batch 358 Loss 122.9627\n",
      "Epoch 1 Batch 359 Loss 135.8265\n",
      "Training Print: Epoch 1 Batch 359 Loss 127.6671\n",
      "Validation Print: Epoch 1 Batch 359 Loss 143.4350\n",
      "Epoch 1 Batch 360 Loss 138.7655\n",
      "Epoch 1 Batch 361 Loss 105.8707\n",
      "Epoch 1 Batch 362 Loss 122.1102\n",
      "Epoch 1 Batch 363 Loss 141.4332\n",
      "Epoch 1 Batch 364 Loss 118.5785\n",
      "Epoch 1 Batch 365 Loss 130.3807\n",
      "Epoch 1 Batch 366 Loss 104.7438\n",
      "Epoch 1 Batch 367 Loss 119.3655\n",
      "Epoch 1 Batch 368 Loss 127.8614\n",
      "Epoch 1 Batch 369 Loss 114.5906\n",
      "Epoch 1 Batch 370 Loss 93.9848\n",
      "Epoch 1 Batch 371 Loss 105.0892\n",
      "Epoch 1 Batch 372 Loss 108.6782\n",
      "Epoch 1 Batch 373 Loss 117.3486\n",
      "Epoch 1 Batch 374 Loss 148.5440\n",
      "Epoch 1 Batch 375 Loss 103.6674\n",
      "Epoch 1 Batch 376 Loss 109.0087\n",
      "Epoch 1 Batch 377 Loss 115.9852\n",
      "Epoch 1 Batch 378 Loss 93.4615\n",
      "Epoch 1 Batch 379 Loss 125.0541\n",
      "Epoch 1 Batch 380 Loss 130.5651\n",
      "Epoch 1 Batch 381 Loss 132.0795\n",
      "Epoch 1 Batch 382 Loss 100.3482\n",
      "Epoch 1 Batch 383 Loss 104.7671\n",
      "Epoch 1 Batch 384 Loss 124.2921\n",
      "Epoch 1 Batch 385 Loss 128.8669\n",
      "Epoch 1 Batch 386 Loss 110.9855\n",
      "Epoch 1 Batch 387 Loss 129.8652\n",
      "Epoch 1 Batch 388 Loss 104.9996\n",
      "Epoch 1 Batch 389 Loss 139.1445\n",
      "Training Print: Epoch 1 Batch 389 Loss 118.3479\n",
      "Validation Print: Epoch 1 Batch 389 Loss 144.6143\n",
      "Epoch 1 Batch 390 Loss 133.1534\n",
      "Epoch 1 Batch 391 Loss 148.6595\n",
      "Epoch 1 Batch 392 Loss 110.1924\n",
      "Epoch 1 Batch 393 Loss 127.0087\n",
      "Epoch 1 Batch 394 Loss 149.8859\n",
      "Epoch 1 Batch 395 Loss 121.2391\n",
      "Epoch 1 Batch 396 Loss 135.8809\n",
      "Epoch 1 Batch 397 Loss 142.5021\n",
      "Epoch 1 Batch 398 Loss 141.4601\n",
      "Epoch 1 Batch 399 Loss 137.2500\n",
      "Saving checkpoint for batch 399 at gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkopoints/ckpt-2\n",
      "Epoch 1 Batch 400 Loss 156.1779\n",
      "Epoch 1 Batch 401 Loss 137.5698\n",
      "Epoch 1 Batch 402 Loss 103.7792\n",
      "Epoch 1 Batch 403 Loss 151.0253\n",
      "Epoch 1 Batch 404 Loss 112.0890\n",
      "Epoch 1 Batch 405 Loss 124.2690\n",
      "Epoch 1 Batch 406 Loss 106.7151\n",
      "Epoch 1 Batch 407 Loss 134.3579\n",
      "Epoch 1 Batch 408 Loss 144.6647\n",
      "Epoch 1 Batch 409 Loss 118.2669\n",
      "Epoch 1 Batch 410 Loss 116.2306\n",
      "Epoch 1 Batch 411 Loss 157.4448\n",
      "Epoch 1 Batch 412 Loss 134.1385\n",
      "Epoch 1 Batch 413 Loss 104.7325\n",
      "Epoch 1 Batch 414 Loss 130.7592\n",
      "Epoch 1 Batch 415 Loss 119.2796\n",
      "Epoch 1 Batch 416 Loss 119.3016\n",
      "Epoch 1 Batch 417 Loss 121.5110\n",
      "Epoch 1 Batch 418 Loss 119.4860\n",
      "Epoch 1 Batch 419 Loss 124.4310\n",
      "Training Print: Epoch 1 Batch 419 Loss 129.4487\n",
      "Validation Print: Epoch 1 Batch 419 Loss 143.8493\n",
      "Epoch 1 Batch 420 Loss 140.8996\n",
      "Epoch 1 Batch 421 Loss 139.5993\n",
      "Epoch 1 Batch 422 Loss 133.2048\n",
      "Epoch 1 Batch 423 Loss 125.4138\n",
      "Epoch 1 Batch 424 Loss 162.6378\n",
      "Epoch 1 Batch 425 Loss 135.5067\n",
      "Epoch 1 Batch 426 Loss 142.8000\n",
      "Epoch 1 Batch 427 Loss 116.3800\n",
      "Epoch 1 Batch 428 Loss 136.2510\n",
      "Epoch 1 Batch 429 Loss 133.3029\n",
      "Epoch 1 Batch 430 Loss 141.6931\n",
      "Epoch 1 Batch 431 Loss 123.9689\n",
      "Epoch 1 Batch 432 Loss 87.4979\n",
      "Epoch 1 Batch 433 Loss 117.5385\n",
      "Epoch 1 Batch 434 Loss 107.5341\n",
      "Epoch 1 Batch 435 Loss 108.0028\n",
      "Epoch 1 Batch 436 Loss 130.8558\n",
      "Epoch 1 Batch 437 Loss 122.9598\n",
      "Epoch 1 Batch 438 Loss 139.2971\n",
      "Epoch 1 Batch 439 Loss 124.4534\n",
      "Epoch 1 Batch 440 Loss 143.2593\n",
      "Epoch 1 Batch 441 Loss 149.6350\n",
      "Epoch 1 Batch 442 Loss 147.6691\n",
      "Epoch 1 Batch 443 Loss 131.0576\n",
      "Epoch 1 Batch 444 Loss 124.7051\n",
      "Epoch 1 Batch 445 Loss 140.9686\n",
      "Epoch 1 Batch 446 Loss 130.5947\n",
      "Epoch 1 Batch 447 Loss 135.1700\n",
      "Epoch 1 Batch 448 Loss 133.7975\n",
      "Epoch 1 Batch 449 Loss 110.5570\n",
      "Training Print: Epoch 1 Batch 449 Loss 130.5737\n",
      "Validation Print: Epoch 1 Batch 449 Loss 143.6329\n",
      "Epoch 1 Batch 450 Loss 132.3465\n",
      "Epoch 1 Batch 451 Loss 123.2883\n",
      "Epoch 1 Batch 452 Loss 119.7159\n",
      "Epoch 1 Batch 453 Loss 126.1186\n",
      "Epoch 1 Batch 454 Loss 126.6068\n",
      "Epoch 1 Batch 455 Loss 141.0192\n",
      "Epoch 1 Batch 456 Loss 120.3160\n",
      "Epoch 1 Batch 457 Loss 110.7328\n",
      "Epoch 1 Batch 458 Loss 137.7654\n",
      "Epoch 1 Batch 459 Loss 119.4351\n",
      "Epoch 1 Batch 460 Loss 116.1333\n",
      "Epoch 1 Batch 461 Loss 144.2298\n",
      "Epoch 1 Batch 462 Loss 113.4799\n",
      "Epoch 1 Batch 463 Loss 121.5084\n",
      "Epoch 1 Batch 464 Loss 113.1108\n",
      "Epoch 1 Batch 465 Loss 119.1767\n",
      "Epoch 1 Batch 466 Loss 149.6906\n",
      "Epoch 1 Batch 467 Loss 126.5559\n",
      "Epoch 1 Batch 468 Loss 111.6627\n",
      "Epoch 1 Batch 469 Loss 122.0856\n",
      "Epoch 1 Batch 470 Loss 125.7682\n",
      "Epoch 1 Batch 471 Loss 142.2512\n",
      "Epoch 1 Batch 472 Loss 125.3298\n",
      "Epoch 1 Batch 473 Loss 131.9070\n",
      "Epoch 1 Batch 474 Loss 132.4136\n",
      "Epoch 1 Batch 475 Loss 128.6868\n",
      "Epoch 1 Batch 476 Loss 120.3289\n",
      "Epoch 1 Batch 477 Loss 105.4043\n",
      "Epoch 1 Batch 478 Loss 130.6363\n",
      "Epoch 1 Batch 479 Loss 131.3922\n",
      "Training Print: Epoch 1 Batch 479 Loss 125.6365\n",
      "Validation Print: Epoch 1 Batch 479 Loss 144.7215\n",
      "Epoch 1 Batch 480 Loss 108.6387\n",
      "Epoch 1 Batch 481 Loss 113.8142\n",
      "Epoch 1 Batch 482 Loss 117.6000\n",
      "Epoch 1 Batch 483 Loss 122.0175\n",
      "Epoch 1 Batch 484 Loss 130.6824\n",
      "Epoch 1 Batch 485 Loss 135.7008\n",
      "Epoch 1 Batch 486 Loss 138.4346\n",
      "Epoch 1 Batch 487 Loss 116.5509\n",
      "Epoch 1 Batch 488 Loss 154.2534\n",
      "Epoch 1 Batch 489 Loss 136.0771\n",
      "Epoch 1 Batch 490 Loss 125.7744\n",
      "Epoch 1 Batch 491 Loss 120.9990\n",
      "Epoch 1 Batch 492 Loss 126.6591\n",
      "Epoch 1 Batch 493 Loss 93.8246\n",
      "Epoch 1 Batch 494 Loss 119.8103\n",
      "Epoch 1 Batch 495 Loss 117.0688\n",
      "Epoch 1 Batch 496 Loss 124.7951\n",
      "Epoch 1 Batch 497 Loss 157.6770\n",
      "Epoch 1 Batch 498 Loss 106.4869\n",
      "Epoch 1 Batch 499 Loss 134.6184\n",
      "Epoch 1 Batch 500 Loss 121.2437\n",
      "Epoch 1 Batch 501 Loss 130.1637\n",
      "Epoch 1 Batch 502 Loss 129.7299\n",
      "Epoch 1 Batch 503 Loss 109.2300\n",
      "Epoch 1 Batch 504 Loss 109.4999\n",
      "Epoch 1 Batch 505 Loss 139.7474\n",
      "Epoch 1 Batch 506 Loss 118.4668\n",
      "Epoch 1 Batch 507 Loss 115.1346\n",
      "Epoch 1 Batch 508 Loss 126.4390\n",
      "Epoch 1 Batch 509 Loss 116.6962\n",
      "Training Print: Epoch 1 Batch 509 Loss 123.9278\n",
      "Validation Print: Epoch 1 Batch 509 Loss 144.4568\n",
      "Epoch 1 Batch 510 Loss 109.0641\n",
      "Epoch 1 Batch 511 Loss 139.0905\n",
      "Epoch 1 Batch 512 Loss 119.2378\n",
      "Epoch 1 Batch 513 Loss 112.2285\n",
      "Epoch 1 Batch 514 Loss 129.7043\n",
      "Epoch 1 Batch 515 Loss 104.5540\n",
      "Epoch 1 Batch 516 Loss 171.9067\n",
      "Epoch 1 Batch 517 Loss 107.6032\n",
      "Epoch 1 Batch 518 Loss 110.7128\n",
      "Epoch 1 Batch 519 Loss 114.1090\n",
      "Epoch 1 Batch 520 Loss 142.2032\n",
      "Epoch 1 Batch 521 Loss 110.8460\n",
      "Epoch 1 Batch 522 Loss 115.5646\n",
      "Epoch 1 Batch 523 Loss 115.5421\n",
      "Epoch 1 Batch 524 Loss 129.1564\n",
      "Epoch 1 Batch 525 Loss 121.1450\n",
      "Epoch 1 Batch 526 Loss 103.8963\n",
      "Epoch 1 Batch 527 Loss 145.4796\n",
      "Epoch 1 Batch 528 Loss 120.9396\n",
      "Epoch 1 Batch 529 Loss 106.3735\n",
      "Epoch 1 Batch 530 Loss 124.1528\n",
      "Epoch 1 Batch 531 Loss 131.1678\n",
      "Epoch 1 Batch 532 Loss 130.3097\n",
      "Epoch 1 Batch 533 Loss 115.3795\n",
      "Epoch 1 Batch 534 Loss 127.3577\n",
      "Epoch 1 Batch 535 Loss 109.8182\n",
      "Epoch 1 Batch 536 Loss 131.4278\n",
      "Epoch 1 Batch 537 Loss 114.4104\n",
      "Epoch 1 Batch 538 Loss 113.3181\n",
      "Epoch 1 Batch 539 Loss 121.0191\n",
      "Training Print: Epoch 1 Batch 539 Loss 121.5906\n",
      "Validation Print: Epoch 1 Batch 539 Loss 145.9861\n",
      "Epoch 1 Batch 540 Loss 132.0563\n",
      "Epoch 1 Batch 541 Loss 130.5588\n",
      "Epoch 1 Batch 542 Loss 134.6917\n",
      "Epoch 1 Batch 543 Loss 118.5548\n",
      "Epoch 1 Batch 544 Loss 123.9043\n",
      "Epoch 1 Batch 545 Loss 100.1286\n",
      "Epoch 1 Batch 546 Loss 101.3186\n",
      "Epoch 1 Batch 547 Loss 108.6141\n",
      "Epoch 1 Batch 548 Loss 127.2051\n",
      "Epoch 1 Batch 549 Loss 126.4539\n",
      "Epoch 1 Batch 550 Loss 114.2849\n",
      "Epoch 1 Batch 551 Loss 148.6112\n",
      "Epoch 1 Batch 552 Loss 96.0831\n",
      "Epoch 1 Batch 553 Loss 111.2302\n",
      "Epoch 1 Batch 554 Loss 142.6791\n",
      "Epoch 1 Batch 555 Loss 129.5702\n",
      "Epoch 1 Batch 556 Loss 106.3225\n",
      "Epoch 1 Batch 557 Loss 136.4047\n",
      "Epoch 1 Batch 558 Loss 110.7357\n",
      "Epoch 1 Batch 559 Loss 108.2917\n",
      "Epoch 1 Batch 560 Loss 110.4153\n",
      "Epoch 1 Batch 561 Loss 128.7715\n",
      "Epoch 1 Batch 562 Loss 107.5969\n",
      "Epoch 1 Batch 563 Loss 112.1873\n",
      "Epoch 1 Batch 564 Loss 104.8368\n",
      "Epoch 1 Batch 565 Loss 109.9647\n",
      "Epoch 1 Batch 566 Loss 152.7189\n",
      "Epoch 1 Batch 567 Loss 128.7077\n",
      "Epoch 1 Batch 568 Loss 131.8950\n",
      "Epoch 1 Batch 569 Loss 132.0512\n",
      "Training Print: Epoch 1 Batch 569 Loss 120.8948\n",
      "Validation Print: Epoch 1 Batch 569 Loss 144.6967\n",
      "Epoch 1 Batch 570 Loss 128.2429\n",
      "Epoch 1 Batch 571 Loss 153.0540\n",
      "Epoch 1 Batch 572 Loss 136.7538\n",
      "Epoch 1 Batch 573 Loss 130.4603\n",
      "Epoch 1 Batch 574 Loss 119.9568\n",
      "Epoch 1 Batch 575 Loss 118.3631\n",
      "Epoch 1 Batch 576 Loss 134.7502\n",
      "Epoch 1 Batch 577 Loss 132.8551\n",
      "Epoch 1 Batch 578 Loss 120.3591\n",
      "Epoch 1 Batch 579 Loss 121.5140\n",
      "Epoch 1 Batch 580 Loss 110.2694\n",
      "Epoch 1 Batch 581 Loss 112.7452\n",
      "Epoch 1 Batch 582 Loss 110.4800\n",
      "Epoch 1 Batch 583 Loss 108.5954\n",
      "Epoch 1 Batch 584 Loss 111.8255\n",
      "Epoch 1 Batch 585 Loss 104.1836\n",
      "Epoch 1 Batch 586 Loss 121.7640\n",
      "Epoch 1 Batch 587 Loss 127.9050\n",
      "Epoch 1 Batch 588 Loss 142.6108\n",
      "Epoch 1 Batch 589 Loss 101.5812\n",
      "Epoch 1 Batch 590 Loss 129.4182\n",
      "Epoch 1 Batch 591 Loss 127.5422\n",
      "Epoch 1 Batch 592 Loss 155.4818\n",
      "Epoch 1 Batch 593 Loss 132.5005\n",
      "Epoch 1 Batch 594 Loss 120.0928\n",
      "Epoch 1 Batch 595 Loss 147.5120\n",
      "Epoch 1 Batch 596 Loss 126.1642\n",
      "Epoch 1 Batch 597 Loss 135.1208\n",
      "Epoch 1 Batch 598 Loss 117.5987\n",
      "Epoch 1 Batch 599 Loss 141.1483\n",
      "Training Print: Epoch 1 Batch 599 Loss 126.0283\n",
      "Saving checkpoint for batch 599 at gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkopoints/ckpt-3\n",
      "Validation Print: Epoch 1 Batch 599 Loss 144.5931\n",
      "Epoch 1 Batch 600 Loss 120.7659\n",
      "Epoch 1 Batch 601 Loss 104.9835\n",
      "Epoch 1 Batch 602 Loss 108.9149\n",
      "Epoch 1 Batch 603 Loss 100.9995\n",
      "Epoch 1 Batch 604 Loss 123.4624\n",
      "Epoch 1 Batch 605 Loss 121.2955\n",
      "Epoch 1 Batch 606 Loss 150.5190\n",
      "Epoch 1 Batch 607 Loss 106.2075\n",
      "Epoch 1 Batch 608 Loss 108.8712\n",
      "Epoch 1 Batch 609 Loss 118.3049\n",
      "Epoch 1 Batch 610 Loss 117.2172\n",
      "Epoch 1 Batch 611 Loss 127.2958\n",
      "Epoch 1 Batch 612 Loss 126.1776\n",
      "Epoch 1 Batch 613 Loss 122.7100\n",
      "Epoch 1 Batch 614 Loss 153.9644\n",
      "Epoch 1 Batch 615 Loss 113.4129\n",
      "Epoch 1 Batch 616 Loss 143.0289\n",
      "Epoch 1 Batch 617 Loss 125.0970\n",
      "Epoch 1 Batch 618 Loss 175.0622\n",
      "Epoch 1 Batch 619 Loss 120.2006\n",
      "Epoch 1 Batch 620 Loss 124.7095\n",
      "Epoch 1 Batch 621 Loss 124.9026\n",
      "Epoch 1 Batch 622 Loss 136.6558\n",
      "Epoch 1 Batch 623 Loss 121.5477\n",
      "Epoch 1 Batch 624 Loss 110.9940\n",
      "Epoch 1 Batch 625 Loss 144.7222\n",
      "Epoch 1 Batch 626 Loss 138.9127\n",
      "Epoch 1 Batch 627 Loss 151.1131\n",
      "Epoch 1 Batch 628 Loss 129.2099\n",
      "Epoch 1 Batch 629 Loss 102.7380\n",
      "Training Print: Epoch 1 Batch 629 Loss 125.7999\n",
      "Validation Print: Epoch 1 Batch 629 Loss 147.0790\n",
      "Epoch 1 Batch 630 Loss 140.1127\n",
      "Epoch 1 Batch 631 Loss 116.9139\n",
      "Epoch 1 Batch 632 Loss 103.3351\n",
      "Epoch 1 Batch 633 Loss 137.4478\n",
      "Epoch 1 Batch 634 Loss 115.4723\n",
      "Epoch 1 Batch 635 Loss 106.7475\n",
      "Epoch 1 Batch 636 Loss 129.7558\n",
      "Epoch 1 Batch 637 Loss 110.6696\n",
      "Epoch 1 Batch 638 Loss 135.4385\n",
      "Epoch 1 Batch 639 Loss 116.9853\n",
      "Epoch 1 Batch 640 Loss 138.7414\n",
      "Epoch 1 Batch 641 Loss 112.9132\n",
      "Epoch 1 Batch 642 Loss 100.6230\n",
      "Epoch 1 Batch 643 Loss 125.1399\n",
      "Epoch 1 Batch 644 Loss 107.3116\n",
      "Epoch 1 Batch 645 Loss 132.0280\n",
      "Epoch 1 Batch 646 Loss 176.8351\n",
      "Epoch 1 Batch 647 Loss 134.6659\n",
      "Epoch 1 Batch 648 Loss 143.2432\n",
      "Epoch 1 Batch 649 Loss 111.6812\n",
      "Epoch 1 Batch 650 Loss 110.0406\n",
      "Epoch 1 Batch 651 Loss 116.6427\n",
      "Epoch 1 Batch 652 Loss 151.3732\n",
      "Epoch 1 Batch 653 Loss 137.3774\n",
      "Epoch 1 Batch 654 Loss 145.5666\n",
      "Epoch 1 Batch 655 Loss 131.6013\n",
      "Epoch 1 Batch 656 Loss 133.6570\n",
      "Epoch 1 Batch 657 Loss 122.0459\n",
      "Epoch 1 Batch 658 Loss 116.9537\n",
      "Epoch 1 Batch 659 Loss 133.9620\n",
      "Training Print: Epoch 1 Batch 659 Loss 126.5094\n",
      "Validation Print: Epoch 1 Batch 659 Loss 147.0878\n",
      "Epoch 1 Batch 660 Loss 121.0722\n",
      "Epoch 1 Batch 661 Loss 115.3176\n",
      "Epoch 1 Batch 662 Loss 112.6307\n",
      "Epoch 1 Batch 663 Loss 129.7147\n",
      "Epoch 1 Batch 664 Loss 126.5690\n",
      "Epoch 1 Batch 665 Loss 113.8867\n",
      "Epoch 1 Batch 666 Loss 121.2577\n",
      "Epoch 1 Batch 667 Loss 103.2468\n",
      "Epoch 1 Batch 668 Loss 138.6520\n",
      "Epoch 1 Batch 669 Loss 140.4900\n",
      "Epoch 1 Batch 670 Loss 127.7168\n",
      "Epoch 1 Batch 671 Loss 155.7428\n",
      "Epoch 1 Batch 672 Loss 147.1832\n",
      "Epoch 1 Batch 673 Loss 118.6928\n",
      "Epoch 1 Batch 674 Loss 123.1011\n",
      "Epoch 1 Batch 675 Loss 133.2232\n",
      "Epoch 1 Batch 676 Loss 112.7500\n",
      "Epoch 1 Batch 677 Loss 134.3317\n",
      "Epoch 1 Batch 678 Loss 128.5153\n",
      "Epoch 1 Batch 679 Loss 145.7258\n",
      "Epoch 1 Batch 680 Loss 136.5204\n",
      "Epoch 1 Batch 681 Loss 129.0011\n",
      "Epoch 1 Batch 682 Loss 122.6032\n",
      "Epoch 1 Batch 683 Loss 125.7703\n",
      "Epoch 1 Batch 684 Loss 102.0320\n",
      "Epoch 1 Batch 685 Loss 113.1032\n",
      "Epoch 1 Batch 686 Loss 152.7863\n",
      "Epoch 1 Batch 687 Loss 118.9263\n",
      "Epoch 1 Batch 688 Loss 103.9129\n",
      "Epoch 1 Batch 689 Loss 124.1608\n",
      "Training Print: Epoch 1 Batch 689 Loss 125.9546\n",
      "Validation Print: Epoch 1 Batch 689 Loss 147.4685\n",
      "Epoch 1 Batch 690 Loss 112.0290\n",
      "Epoch 1 Batch 691 Loss 111.5636\n",
      "Epoch 1 Batch 692 Loss 112.3371\n",
      "Epoch 1 Batch 693 Loss 114.5574\n",
      "Epoch 1 Batch 694 Loss 153.3863\n",
      "Epoch 1 Batch 695 Loss 102.3602\n",
      "Epoch 1 Batch 696 Loss 123.7817\n",
      "Epoch 1 Batch 697 Loss 133.4995\n",
      "Epoch 1 Batch 698 Loss 131.5284\n",
      "Epoch 1 Batch 699 Loss 97.4018\n",
      "Epoch 1 Batch 700 Loss 123.3199\n",
      "Epoch 1 Batch 701 Loss 100.1671\n",
      "Epoch 1 Batch 702 Loss 137.4945\n",
      "Epoch 1 Batch 703 Loss 124.3314\n",
      "Epoch 1 Batch 704 Loss 118.2170\n",
      "Epoch 1 Batch 705 Loss 144.9263\n",
      "Epoch 1 Batch 706 Loss 113.2628\n",
      "Epoch 1 Batch 707 Loss 114.6704\n",
      "Epoch 1 Batch 708 Loss 140.1246\n",
      "Epoch 1 Batch 709 Loss 160.7089\n",
      "Epoch 1 Batch 710 Loss 121.1185\n",
      "Epoch 1 Batch 711 Loss 125.2262\n",
      "Epoch 1 Batch 712 Loss 126.3991\n",
      "Epoch 1 Batch 713 Loss 125.5863\n",
      "Epoch 1 Batch 714 Loss 138.7995\n",
      "Epoch 1 Batch 715 Loss 144.5472\n",
      "Epoch 1 Batch 716 Loss 123.5522\n",
      "Epoch 1 Batch 717 Loss 105.5558\n",
      "Epoch 1 Batch 718 Loss 111.9517\n",
      "Epoch 1 Batch 719 Loss 105.6102\n",
      "Training Print: Epoch 1 Batch 719 Loss 123.2672\n",
      "Validation Print: Epoch 1 Batch 719 Loss 146.4846\n",
      "Epoch 1 Batch 720 Loss 99.3077\n",
      "Epoch 1 Batch 721 Loss 142.5792\n",
      "Epoch 1 Batch 722 Loss 108.2463\n",
      "Epoch 1 Batch 723 Loss 126.5625\n",
      "Epoch 1 Batch 724 Loss 138.7388\n",
      "Epoch 1 Batch 725 Loss 118.8833\n",
      "Epoch 1 Batch 726 Loss 135.7119\n",
      "Epoch 1 Batch 727 Loss 164.9335\n",
      "Epoch 1 Batch 728 Loss 131.4379\n",
      "Epoch 1 Batch 729 Loss 117.2423\n",
      "Epoch 1 Batch 730 Loss 128.0491\n",
      "Epoch 1 Batch 731 Loss 125.7634\n",
      "Epoch 1 Batch 732 Loss 132.2953\n",
      "Epoch 1 Batch 733 Loss 156.8341\n",
      "Epoch 1 Batch 734 Loss 135.6613\n",
      "Epoch 1 Batch 735 Loss 132.1377\n",
      "Epoch 1 Batch 736 Loss 124.4869\n",
      "Epoch 1 Batch 737 Loss 98.5840\n",
      "Epoch 1 Batch 738 Loss 103.5823\n",
      "Epoch 1 Batch 739 Loss 112.2063\n",
      "Epoch 1 Batch 740 Loss 138.2793\n",
      "Epoch 1 Batch 741 Loss 115.5284\n",
      "Epoch 1 Batch 742 Loss 119.1736\n",
      "Epoch 1 Batch 743 Loss 121.8092\n",
      "Epoch 1 Batch 744 Loss 119.8512\n",
      "Epoch 1 Batch 745 Loss 118.6365\n",
      "Epoch 1 Batch 746 Loss 112.5749\n",
      "Epoch 1 Batch 747 Loss 123.2813\n",
      "Epoch 1 Batch 748 Loss 111.8069\n",
      "Epoch 1 Batch 749 Loss 121.5088\n",
      "Training Print: Epoch 1 Batch 749 Loss 124.5231\n",
      "Validation Print: Epoch 1 Batch 749 Loss 149.5200\n",
      "Epoch 1 Batch 750 Loss 115.5616\n",
      "Epoch 1 Batch 751 Loss 138.3735\n",
      "Epoch 1 Batch 752 Loss 125.9781\n",
      "Epoch 1 Batch 753 Loss 118.7377\n",
      "Epoch 1 Batch 754 Loss 128.9719\n",
      "Epoch 1 Batch 755 Loss 96.2812\n",
      "Epoch 1 Batch 756 Loss 120.3652\n",
      "Epoch 1 Batch 757 Loss 102.8589\n",
      "Epoch 1 Batch 758 Loss 141.6263\n",
      "Epoch 1 Batch 759 Loss 116.2639\n",
      "Epoch 1 Batch 760 Loss 104.6622\n",
      "Epoch 1 Batch 761 Loss 146.7990\n",
      "Epoch 1 Batch 762 Loss 128.6133\n",
      "Epoch 1 Batch 763 Loss 115.5495\n",
      "Epoch 1 Batch 764 Loss 134.6224\n",
      "Epoch 1 Batch 765 Loss 116.8135\n",
      "Epoch 1 Batch 766 Loss 111.8056\n",
      "Epoch 1 Batch 767 Loss 105.7772\n",
      "Epoch 1 Batch 768 Loss 143.3579\n",
      "Epoch 1 Batch 769 Loss 130.7269\n",
      "Epoch 1 Batch 770 Loss 163.2250\n",
      "Epoch 1 Batch 771 Loss 106.3522\n",
      "Epoch 1 Batch 772 Loss 134.4786\n",
      "Epoch 1 Batch 773 Loss 127.4447\n",
      "Epoch 1 Batch 774 Loss 126.2113\n",
      "Epoch 1 Batch 775 Loss 98.5346\n",
      "Epoch 1 Batch 776 Loss 130.1491\n",
      "Epoch 1 Batch 777 Loss 112.0811\n",
      "Epoch 1 Batch 778 Loss 134.8634\n",
      "Epoch 1 Batch 779 Loss 110.5072\n",
      "Training Print: Epoch 1 Batch 779 Loss 122.9198\n",
      "Validation Print: Epoch 1 Batch 779 Loss 147.1329\n",
      "Epoch 1 Batch 780 Loss 129.0794\n",
      "Epoch 1 Batch 781 Loss 140.3279\n",
      "Epoch 1 Batch 782 Loss 105.2692\n",
      "Epoch 1 Batch 783 Loss 110.6223\n",
      "Epoch 1 Batch 784 Loss 105.9487\n",
      "Epoch 1 Batch 785 Loss 126.6669\n",
      "Epoch 1 Batch 786 Loss 135.8608\n",
      "Epoch 1 Batch 787 Loss 112.1415\n",
      "Epoch 1 Batch 788 Loss 123.6278\n",
      "Epoch 1 Batch 789 Loss 114.4389\n",
      "Epoch 1 Batch 790 Loss 142.2683\n",
      "Epoch 1 Batch 791 Loss 133.9441\n",
      "Epoch 1 Batch 792 Loss 128.7417\n",
      "Epoch 1 Batch 793 Loss 124.1177\n",
      "Epoch 1 Batch 794 Loss 136.8734\n",
      "Epoch 1 Batch 795 Loss 103.4405\n",
      "Epoch 1 Batch 796 Loss 151.7230\n",
      "Epoch 1 Batch 797 Loss 88.5745\n",
      "Epoch 1 Batch 798 Loss 112.4033\n",
      "Epoch 1 Batch 799 Loss 145.6160\n",
      "Saving checkpoint for batch 799 at gs://general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkopoints/ckpt-4\n",
      "Epoch 1 Batch 800 Loss 123.4924\n",
      "Epoch 1 Batch 801 Loss 134.6106\n",
      "Epoch 1 Batch 802 Loss 138.9503\n",
      "Epoch 1 Batch 803 Loss 121.7608\n",
      "Epoch 1 Batch 804 Loss 123.1651\n",
      "Epoch 1 Batch 805 Loss 112.8719\n",
      "Epoch 1 Batch 806 Loss 130.5788\n",
      "Epoch 1 Batch 807 Loss 122.5804\n",
      "Epoch 1 Batch 808 Loss 137.3123\n",
      "Epoch 1 Batch 809 Loss 113.9785\n",
      "Training Print: Epoch 1 Batch 809 Loss 124.3662\n",
      "Validation Print: Epoch 1 Batch 809 Loss 145.8227\n",
      "Epoch 1 Batch 810 Loss 120.4795\n",
      "Epoch 1 Batch 811 Loss 126.2656\n",
      "Epoch 1 Batch 812 Loss 106.1163\n",
      "Epoch 1 Batch 813 Loss 116.7123\n",
      "Epoch 1 Batch 814 Loss 127.8422\n",
      "Epoch 1 Batch 815 Loss 102.9259\n",
      "Epoch 1 Batch 816 Loss 130.1166\n",
      "Epoch 1 Batch 817 Loss 115.5998\n",
      "Epoch 1 Batch 818 Loss 104.1701\n",
      "Epoch 1 Batch 819 Loss 107.7503\n",
      "Epoch 1 Batch 820 Loss 135.9521\n",
      "Epoch 1 Batch 821 Loss 117.5862\n",
      "Epoch 1 Batch 822 Loss 109.9102\n",
      "Epoch 1 Batch 823 Loss 131.1162\n",
      "Epoch 1 Batch 824 Loss 118.9657\n",
      "Epoch 1 Batch 825 Loss 134.0848\n",
      "Epoch 1 Batch 826 Loss 103.6361\n",
      "Epoch 1 Batch 827 Loss 117.4037\n",
      "Epoch 1 Batch 828 Loss 137.0924\n",
      "Epoch 1 Batch 829 Loss 123.8539\n",
      "Epoch 1 Batch 830 Loss 135.0894\n",
      "Epoch 1 Batch 831 Loss 100.0630\n",
      "Epoch 1 Batch 832 Loss 127.2940\n",
      "Epoch 1 Batch 833 Loss 130.7570\n",
      "Epoch 1 Batch 834 Loss 133.3309\n",
      "Epoch 1 Batch 835 Loss 128.1981\n",
      "Epoch 1 Batch 836 Loss 95.5238\n",
      "Epoch 1 Batch 837 Loss 142.3300\n",
      "Epoch 1 Batch 838 Loss 115.3498\n",
      "Epoch 1 Batch 839 Loss 121.1704\n",
      "Training Print: Epoch 1 Batch 839 Loss 120.5562\n",
      "Validation Print: Epoch 1 Batch 839 Loss 146.2654\n",
      "Epoch 1 Batch 840 Loss 137.9171\n",
      "Epoch 1 Batch 841 Loss 103.0330\n",
      "Epoch 1 Batch 842 Loss 136.7824\n",
      "Epoch 1 Batch 843 Loss 133.9966\n",
      "Epoch 1 Batch 844 Loss 106.2439\n",
      "Epoch 1 Batch 845 Loss 126.4681\n",
      "Epoch 1 Batch 846 Loss 116.2768\n",
      "Epoch 1 Batch 847 Loss 112.3773\n",
      "Epoch 1 Batch 848 Loss 100.6507\n",
      "Epoch 1 Batch 849 Loss 130.8034\n",
      "Epoch 1 Batch 850 Loss 134.4974\n",
      "Epoch 1 Batch 851 Loss 113.5366\n",
      "Epoch 1 Batch 852 Loss 101.6864\n",
      "Epoch 1 Batch 853 Loss 149.5394\n",
      "Epoch 1 Batch 854 Loss 130.8250\n",
      "Epoch 1 Batch 855 Loss 115.3112\n",
      "Epoch 1 Batch 856 Loss 133.9125\n",
      "Epoch 1 Batch 857 Loss 105.2288\n",
      "Epoch 1 Batch 858 Loss 125.7108\n",
      "Epoch 1 Batch 859 Loss 83.2731\n",
      "Epoch 1 Batch 860 Loss 124.8135\n",
      "Epoch 1 Batch 861 Loss 120.5658\n",
      "Epoch 1 Batch 862 Loss 125.6294\n",
      "Epoch 1 Batch 863 Loss 124.7540\n",
      "Epoch 1 Batch 864 Loss 100.2551\n",
      "Epoch 1 Batch 865 Loss 116.9528\n",
      "Epoch 1 Batch 866 Loss 126.6982\n",
      "Epoch 1 Batch 867 Loss 113.5940\n",
      "Epoch 1 Batch 868 Loss 114.5991\n",
      "Epoch 1 Batch 869 Loss 114.0358\n",
      "Training Print: Epoch 1 Batch 869 Loss 119.3323\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  validation_loss.reset_states()\n",
    "  instant_loss.reset_states()\n",
    "\n",
    "  jsut_dataset.take(5)\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, data) in enumerate(jvs_dataset):\n",
    "    #print(batch)\n",
    "    #if batch == 9:\n",
    "    #  print(tokenizer.detokenize(data['text']).numpy()[0].decode('utf-8'))\n",
    "    #  print(data['text'].numpy()[0])\n",
    "    #  continue\n",
    "\n",
    "    train_step(data['speech'], data['text'], data['speech_lengths'], data['text_lengths'])\n",
    "\n",
    "\n",
    "    if (batch+1) % print_period == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {instant_loss.result():.4f}')\n",
    "      instant_loss.reset_states()\n",
    "\n",
    "    if (batch+1) % train_period == 0:\n",
    "      with file_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=batch)\n",
    "      print(f'Training Print: Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
    "      train_loss.reset_states()\n",
    "\n",
    "    if (batch + 1) % save_period == 0:\n",
    "      ckpt_save_path = ckpt_manager.save()\n",
    "      print (f'Saving checkpoint for batch {batch} at {ckpt_save_path}')\n",
    "    if (batch + 1) % validate_period == 0:\n",
    "      for (val_batch, val_data) in enumerate(jsut_dataset.take(5)):\n",
    "        validation_step(val_data['speech'], val_data['text'], val_data['speech_lengths'], val_data['text_lengths'])\n",
    "      with file_writer.as_default():\n",
    "        tf.summary.scalar(\"validation_loss\", validation_loss.result(), step=batch)\n",
    "      print(f'Validation Print: Epoch {epoch + 1} Batch {batch} Loss {validation_loss.result():.4f}')\n",
    "      validation_loss.reset_states()\n",
    "\n",
    "  break\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOA7hItRagYh95XqJUwn/LF",
   "include_colab_link": true,
   "name": "Untitled22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
