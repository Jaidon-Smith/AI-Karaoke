{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "##### <a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/STT%20JVS%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkRMEbJrdWhE",
    "outputId": "742dcc75-c240-424b-f8fc-3c3da87acce5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e5b0420cd036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0S4eBN73j0B4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f759c1655bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo fusermount -u ~/general-304503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503’: File exists\n",
      "Using mount point: /home/jsjsrobert500/general-304503\n",
      "2021/03/16 00:59:55.357644 Opening GCS connection...\n",
      "2021/03/16 00:59:55.510511 Mounting file system...\n",
      "2021/03/16 00:59:55.528274 File system has been successfully mounted.\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/general-304503\n",
    "!gcsfuse general-304503 ~/general-304503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnmItTWZe0ay",
    "outputId": "7a27ca64-ee73-4f15-d8f6-f6ae8e3689eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tfds-nightly in /home/jsjsrobert500/.local/lib/python3.7/site-packages (4.2.0.dev202103150106)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (5.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.10.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.15.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.3.1.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.27.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (0.18.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (20.3.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (4.58.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (3.7.4.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (2.25.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tfds-nightly) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.4 in /opt/conda/lib/python3.7/site-packages (from importlib-resources->tfds-nightly) (3.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tfds-nightly) (1.52.0)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.7/site-packages (0.25.1)\n",
      "Requirement already satisfied: tensorflow_io in /opt/conda/lib/python3.7/site-packages (0.17.0)\n",
      "Requirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_io) (2.4.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.19.5)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.15.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.36.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/jsjsrobert500/.local/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.12)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.27.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (54.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow_io) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user tfds-nightly\n",
    "!pip install --user pydub\n",
    "!pip install --user tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CaxFPa1ne2Xm"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpLtjhXMepL-",
    "outputId": "0c698c66-b28c-48bc-c73d-1b6ad75e3900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'public_datasets' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Jaidon-Smith/public_datasets.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8OZoOWZCXvm"
   },
   "source": [
    "# Setting Up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AbmSdYojCgDz"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WZrsymk7E5DH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/colab_logs/’: File exists\n",
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/colab_logs/jvs_transformer_stt_12_3_21’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Initialise Logs\n",
    "\n",
    "!rm -r ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/logs\n",
    "\n",
    "%mkdir ~/general-304503/notebook_logs/\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SdZusIv6_LiI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/colab_logs/’: File exists\n",
      "mkdir: cannot create directory ‘/home/jsjsrobert500/general-304503/colab_logs/jvs_transformer_stt_12_3_21’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Initialise Checkpoints\n",
    "!rm -r ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkpoints\n",
    "\n",
    "%mkdir ~/general-304503/notebook_logs/\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21\n",
    "%mkdir ~/general-304503/notebook_logs/jvs_transformer_stt_12_3_21/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LP5tugbE-vbI"
   },
   "outputs": [],
   "source": [
    "logdir = 'gs://general-304503/colab_logs/jvs_transformer_stt_12_3_21/logs'\n",
    "checkpoint_path = 'gs://general-304503/colab_logs/jvs_transformer_stt_12_3_21/checkopoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VimJg7KRCb3G"
   },
   "outputs": [],
   "source": [
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_tensorboard\n",
      "  Downloading jupyter_tensorboard-0.2.0.tar.gz (15 kB)\n",
      "Requirement already satisfied: notebook>=5.0 in /opt/conda/lib/python3.7/site-packages (from jupyter_tensorboard) (6.2.0)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (5.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (2.11.3)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (6.1.11)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (20.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (4.7.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (5.5.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (0.2.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (22.0.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (6.0.7)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (0.9.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (0.9.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (6.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (5.0.5)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=5.0->jupyter_tensorboard) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=5.3.4->notebook>=5.0->jupyter_tensorboard) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook>=5.0->jupyter_tensorboard) (1.15.0)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.7/site-packages (from terminado>=0.8.3->notebook>=5.0->jupyter_tensorboard) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=5.0->jupyter_tensorboard) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.0->jupyter_tensorboard) (2.20)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel->notebook>=5.0->jupyter_tensorboard) (7.21.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (54.0.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (3.0.16)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (2.8.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (0.18.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (0.8.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=5.0->jupyter_tensorboard) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=5.0->jupyter_tensorboard) (1.1.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (3.3.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.6.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.4.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.5.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=5.0->jupyter_tensorboard) (1.4.2)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.0->jupyter_tensorboard) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.0->jupyter_tensorboard) (1.4.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat->notebook>=5.0->jupyter_tensorboard) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter_tensorboard) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter_tensorboard) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter_tensorboard) (0.17.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=5.0->jupyter_tensorboard) (20.9)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=5.0->jupyter_tensorboard) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter_tensorboard) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.0->jupyter_tensorboard) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=5.0->jupyter_tensorboard) (2.4.7)\n",
      "Building wheels for collected packages: jupyter-tensorboard\n",
      "  Building wheel for jupyter-tensorboard (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter-tensorboard: filename=jupyter_tensorboard-0.2.0-py2.py3-none-any.whl size=15258 sha256=d414aad519e069fa2234cd9e83c4932823bc93a53bf17965f5379050e3bf0229\n",
      "  Stored in directory: /home/jsjsrobert500/.cache/pip/wheels/59/14/ab/6d0bce449039ebdcbf45c1aff8c19153a68bf3f0492a32620e\n",
      "Successfully built jupyter-tensorboard\n",
      "Installing collected packages: jupyter-tensorboard\n",
      "Successfully installed jupyter-tensorboard-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "i5Pm3pOYdXBf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 16310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-256509ee94bbeff1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-256509ee94bbeff1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ~/general-304503/colab_logs/jvs_transformer_stt_12_3_21/logs --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkMA6i1TVy1S"
   },
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-16 01:02:34--  https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq\n",
      "Resolving docs.google.com (docs.google.com)... 108.177.111.102, 108.177.111.113, 108.177.111.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|108.177.111.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l7cmbot1cgsqme1b2lukn2i8me27qn5v/1615856550000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-03-16 01:02:35--  https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l7cmbot1cgsqme1b2lukn2i8me27qn5v/1615856550000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download\n",
      "Resolving doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)... 173.194.195.132, 2607:f8b0:4001:c11::84\n",
      "Connecting to doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)|173.194.195.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 276245 (270K) [application/octet-stream]\n",
      "Saving to: ‘hiragana_jsut.model’\n",
      "\n",
      "hiragana_jsut.model 100%[===================>] 269.77K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-03-16 01:02:35 (100 MB/s) - ‘hiragana_jsut.model’ saved [276245/276245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq' -O hiragana_jsut.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBFpGwTo9kUs",
    "outputId": "cb5cf1d0-af18-4d6d-c9ae-6dccf80c8347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet tensorflow-text\n",
    "\n",
    "import tensorflow_text as text\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "model_file = 'hiragana_jsut.model'\n",
    "model = gfile.GFile(model_file, 'rb').read()\n",
    "\n",
    "tokenizer = text.SentencepieceTokenizer(model=model)\n",
    "\n",
    "input_vocab_size = tokenizer.vocab_size().numpy()\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyeaq-FTCB7X",
    "outputId": "97ce5020-8122-4856-b85d-e28483b8843e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykakasi\n",
      "  Using cached pykakasi-2.0.6-py3-none-any.whl (2.4 MB)\n",
      "Collecting klepto\n",
      "  Using cached klepto-0.2.0-py2.py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from pykakasi) (3.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->pykakasi) (3.7.4.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /opt/conda/lib/python3.7/site-packages (from klepto->pykakasi) (0.2.9)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/lib/python3.7/site-packages (from klepto->pykakasi) (0.3.3)\n",
      "Installing collected packages: klepto, pykakasi\n",
      "Successfully installed klepto-0.2.0 pykakasi-2.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pykakasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rWcfthvAEbZI"
   },
   "outputs": [],
   "source": [
    "import pykakasi\n",
    "converter = pykakasi.kakasi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "154oNA4E6hEg"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(old_text):\n",
    "  text = old_text\n",
    "  text = tokenizer.tokenize(text)\n",
    "  text = tf.pad(text, paddings=[[0 , 200 - tf.shape(text)[0]]])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LiqQD4F8j8YJ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text_no_pad(old_text):\n",
    "  text = old_text\n",
    "  text = tokenizer.tokenize(text)\n",
    "  text = tf.shape(text)[0]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VxIOOV5_pS58"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio(old_audio, original_sample_rate):\n",
    "  audio = old_audio/tf.int16.max\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
    "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
    "  audio = tf.pad(audio, paddings=[[0 , 500 - tf.shape(audio)[0]], [0,0]])\n",
    "  return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nCRZuRVb4H_d"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio_no_pad(old_audio, original_sample_rate):\n",
    "  audio = old_audio/tf.int16.max\n",
    "  audio = tf.cast(audio, tf.float32)\n",
    "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
    "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
    "  audio = tf.shape(audio)[0]\n",
    "  return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "SSaiGkuUZYFL"
   },
   "outputs": [],
   "source": [
    "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
    "\n",
    "jsut_dataset, info = tfds.load(\n",
    "                    \"jsut\",\n",
    "                    split=\"train\",\n",
    "                    data_dir='gs://general-304503/public_datasets',\n",
    "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
    "                    with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "p4dWKtfoEyKy"
   },
   "outputs": [],
   "source": [
    "def convert_to_kana(text):\n",
    "  #kana = converter.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')\n",
    "  new_text = text.numpy().decode('utf-8')\n",
    "  kana = converter.convert(new_text)\n",
    "  return ''.join([i['hira'] for i in kana])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dsITXisWE81g"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": x[\"speech\"],\n",
    "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5TEZzObuMtUW"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
    "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
    "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
    "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "htZ7O_L6ZPMm"
   },
   "outputs": [],
   "source": [
    "jsut_dataset = jsut_dataset.batch(8)\n",
    "jsut_dataset = jsut_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lm95V3vntL6"
   },
   "source": [
    "## jvs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "u1GQRngMnrUx"
   },
   "outputs": [],
   "source": [
    "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
    "\n",
    "jvs_dataset, info = tfds.load(\n",
    "                    \"jvs\",\n",
    "                    split=\"train\",\n",
    "                    data_dir='gs://general-304503/public_datasets',\n",
    "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
    "                    with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Yc2xw1UvnrUx"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": x[\"speech\"],\n",
    "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pOMQs6x_nrUx"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
    "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
    "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
    "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
    "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Iv1ibtCpnrUy"
   },
   "outputs": [],
   "source": [
    "jvs_dataset = jvs_dataset.batch(8)\n",
    "jvs_dataset = jvs_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5WxMKAI-CF3"
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "ds = jvs_dataset.take(1)\n",
    "\n",
    "for i in ds:\n",
    "  data = i\n",
    "  break\n",
    "\n",
    "speech = data['speech']\n",
    "text = data['text']\n",
    "id = data['id']\n",
    "speech_lengths = data['speech_lengths']\n",
    "text_lengths = data['text_lengths']\n",
    "\n",
    "tokenizer.detokenize(text.numpy()[1]).numpy().decode('utf-8')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-EgkkRmnrUy",
    "outputId": "9638a475-079e-4dbc-c36d-236d49bbb039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2000, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = tokenizer.vocab_size()\n",
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cjiiyX82XU5"
   },
   "source": [
    "# Using Transformer Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xCSRf4_0wZHZ"
   },
   "outputs": [],
   "source": [
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CJ1HYBRd6NQu"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "_fHkHLsK22m5"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "wWLWmM3MPr5V"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "eR9vt3slYa6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "JpqOGPpKYfuf"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "F2lvYoMO4j1w"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.embedding = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training=True):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Makes input larger by a constant\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rIkO24ZTsYng"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "kFvfDRy_sgBc"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "MaKn44ztspmJ"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "instant_loss = tf.keras.metrics.Mean(name='instant_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "GVPnnLizwA3m"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           maximum_position_encoding)\n",
    "    self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "  def call(self, inp, training=True):\n",
    "    enc_output = self.encoder(inp, training)  # (batch_size, inp_seq_len, d_model)\n",
    "    final_output = self.final_layer(enc_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    return final_output\n",
    "\n",
    "transformer = Transformer(num_layers=6,\n",
    "          d_model=d_model,\n",
    "          num_heads=8,\n",
    "          dff=2048, \n",
    "          maximum_position_encoding=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "1Y1kDSm9st_Q"
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "hXw3bhf7tGfJ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(speech, text, speech_lengths, text_lengths):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = transformer(speech)\n",
    "\n",
    "    labels = text\n",
    "    label_length = text_lengths\n",
    "    logit_length = speech_lengths\n",
    "    unique = tf.nn.ctc_unique_labels(text)\n",
    "\n",
    "\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
    "    )\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  instant_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "VgTWTimuzAtQ"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(speech, text, speech_lengths, text_lengths):\n",
    "  logits = transformer(speech)\n",
    "\n",
    "  labels = text\n",
    "  label_length = text_lengths\n",
    "  logit_length = speech_lengths\n",
    "  unique = tf.nn.ctc_unique_labels(text)\n",
    "\n",
    "\n",
    "  loss = tf.nn.ctc_loss(\n",
    "      labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
    "  )\n",
    "\n",
    "\n",
    "  validation_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "XsdWdYE11Kea"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "validate_period = 30\n",
    "save_period = 200\n",
    "print_period = 1\n",
    "train_period = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Xn0oW1kyp0qr"
   },
   "outputs": [],
   "source": [
    "dataset = jvs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AhEzjP3ztJYz",
    "outputId": "ed3e04ec-8c39-4d7b-ca5d-c21bf516bf59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1196: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1196: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 544.6199\n",
      "Epoch 1 Batch 1 Loss 527.0213\n",
      "Epoch 1 Batch 2 Loss 542.9796\n",
      "Epoch 1 Batch 3 Loss 524.2440\n",
      "Epoch 1 Batch 4 Loss 560.5367\n",
      "Epoch 1 Batch 5 Loss 708.1067\n",
      "Epoch 1 Batch 6 Loss 421.4478\n",
      "Epoch 1 Batch 7 Loss 473.7731\n",
      "Epoch 1 Batch 8 Loss 621.6588\n",
      "Epoch 1 Batch 9 Loss 489.9309\n",
      "Epoch 1 Batch 10 Loss 450.9550\n",
      "Epoch 1 Batch 11 Loss 455.1725\n",
      "Epoch 1 Batch 12 Loss 598.8470\n",
      "Epoch 1 Batch 13 Loss 449.5757\n",
      "Epoch 1 Batch 14 Loss 629.1925\n",
      "Epoch 1 Batch 15 Loss 430.8074\n",
      "Epoch 1 Batch 16 Loss 535.7236\n",
      "Epoch 1 Batch 17 Loss 447.5312\n",
      "Epoch 1 Batch 18 Loss 503.7372\n",
      "Epoch 1 Batch 19 Loss 401.9178\n",
      "Epoch 1 Batch 20 Loss 359.5510\n",
      "Epoch 1 Batch 21 Loss 553.8704\n",
      "Epoch 1 Batch 22 Loss 348.0706\n",
      "Epoch 1 Batch 23 Loss 442.0386\n",
      "Epoch 1 Batch 24 Loss 458.1676\n",
      "Epoch 1 Batch 25 Loss 419.2437\n",
      "Epoch 1 Batch 26 Loss 373.2211\n",
      "Epoch 1 Batch 27 Loss 396.9250\n",
      "Epoch 1 Batch 28 Loss 330.9875\n",
      "Epoch 1 Batch 29 Loss 320.4328\n",
      "Training Print: Epoch 1 Batch 29 Loss 477.3429\n",
      "Validation Print: Epoch 1 Batch 29 Loss 434.7172\n",
      "Epoch 1 Batch 30 Loss 265.4809\n",
      "Epoch 1 Batch 31 Loss 334.4113\n",
      "Epoch 1 Batch 32 Loss 260.4206\n",
      "Epoch 1 Batch 33 Loss 273.3125\n",
      "Epoch 1 Batch 34 Loss 247.5757\n",
      "Epoch 1 Batch 35 Loss 285.2469\n",
      "Epoch 1 Batch 36 Loss 223.8231\n",
      "Epoch 1 Batch 37 Loss 237.3765\n",
      "Epoch 1 Batch 38 Loss 247.3849\n",
      "Epoch 1 Batch 39 Loss 181.8366\n",
      "Epoch 1 Batch 40 Loss 170.3355\n",
      "Epoch 1 Batch 41 Loss 174.5147\n",
      "Epoch 1 Batch 42 Loss 202.3770\n",
      "Epoch 1 Batch 43 Loss 169.7682\n",
      "Epoch 1 Batch 44 Loss 169.7959\n",
      "Epoch 1 Batch 45 Loss 151.6081\n",
      "Epoch 1 Batch 46 Loss 206.2248\n",
      "Epoch 1 Batch 47 Loss 178.3771\n",
      "Epoch 1 Batch 48 Loss 139.7761\n",
      "Epoch 1 Batch 49 Loss 156.2229\n",
      "Epoch 1 Batch 50 Loss 164.7515\n",
      "Epoch 1 Batch 51 Loss 161.9043\n",
      "Epoch 1 Batch 52 Loss 130.7981\n",
      "Epoch 1 Batch 53 Loss 177.4450\n",
      "Epoch 1 Batch 54 Loss 155.0013\n",
      "Epoch 1 Batch 55 Loss 188.6313\n",
      "Epoch 1 Batch 56 Loss 133.6737\n",
      "Epoch 1 Batch 57 Loss 192.4672\n",
      "Epoch 1 Batch 58 Loss 161.0795\n",
      "Epoch 1 Batch 59 Loss 152.5511\n",
      "Training Print: Epoch 1 Batch 59 Loss 196.4724\n",
      "Validation Print: Epoch 1 Batch 59 Loss 169.2472\n",
      "Epoch 1 Batch 60 Loss 201.7928\n",
      "Epoch 1 Batch 61 Loss 131.0277\n",
      "Epoch 1 Batch 62 Loss 151.7887\n",
      "Epoch 1 Batch 63 Loss 187.9516\n",
      "Epoch 1 Batch 64 Loss 162.9637\n",
      "Epoch 1 Batch 65 Loss 146.0240\n",
      "Epoch 1 Batch 66 Loss 184.6135\n",
      "Epoch 1 Batch 67 Loss 143.4048\n",
      "Epoch 1 Batch 68 Loss 119.4163\n",
      "Epoch 1 Batch 69 Loss 182.6142\n",
      "Epoch 1 Batch 70 Loss 159.1361\n",
      "Epoch 1 Batch 71 Loss 127.8115\n",
      "Epoch 1 Batch 72 Loss 145.0027\n",
      "Epoch 1 Batch 73 Loss 118.5919\n",
      "Epoch 1 Batch 74 Loss 135.4272\n",
      "Epoch 1 Batch 75 Loss 124.0797\n",
      "Epoch 1 Batch 76 Loss 122.0807\n",
      "Epoch 1 Batch 77 Loss 170.9930\n",
      "Epoch 1 Batch 78 Loss 172.3047\n",
      "Epoch 1 Batch 79 Loss 146.4630\n",
      "Epoch 1 Batch 80 Loss 154.7830\n",
      "Epoch 1 Batch 81 Loss 131.1809\n",
      "Epoch 1 Batch 82 Loss 133.5516\n",
      "Epoch 1 Batch 83 Loss 148.6805\n",
      "Epoch 1 Batch 84 Loss 166.9028\n",
      "Epoch 1 Batch 85 Loss 135.0892\n",
      "Epoch 1 Batch 86 Loss 165.4987\n",
      "Epoch 1 Batch 87 Loss 164.2725\n",
      "Epoch 1 Batch 88 Loss 182.3541\n",
      "Epoch 1 Batch 89 Loss 126.3492\n",
      "Training Print: Epoch 1 Batch 89 Loss 151.4050\n",
      "Validation Print: Epoch 1 Batch 89 Loss 165.8863\n",
      "Epoch 1 Batch 90 Loss 142.7208\n",
      "Epoch 1 Batch 91 Loss 172.3160\n",
      "Epoch 1 Batch 92 Loss 180.5627\n",
      "Epoch 1 Batch 93 Loss 145.2024\n",
      "Epoch 1 Batch 94 Loss 146.6648\n",
      "Epoch 1 Batch 95 Loss 133.9025\n",
      "Epoch 1 Batch 96 Loss 141.3363\n",
      "Epoch 1 Batch 97 Loss 159.1575\n",
      "Epoch 1 Batch 98 Loss 141.6772\n",
      "Epoch 1 Batch 99 Loss 182.2280\n",
      "Epoch 1 Batch 100 Loss 138.7147\n",
      "Epoch 1 Batch 101 Loss 156.3655\n",
      "Epoch 1 Batch 102 Loss 157.6794\n",
      "Epoch 1 Batch 103 Loss 166.0261\n",
      "Epoch 1 Batch 104 Loss 128.4529\n",
      "Epoch 1 Batch 105 Loss 167.9684\n",
      "Epoch 1 Batch 106 Loss 148.4659\n",
      "Epoch 1 Batch 107 Loss 136.1927\n",
      "Epoch 1 Batch 108 Loss 156.0470\n",
      "Epoch 1 Batch 109 Loss 114.1249\n",
      "Epoch 1 Batch 110 Loss 148.6107\n",
      "Epoch 1 Batch 111 Loss 142.2083\n",
      "Epoch 1 Batch 112 Loss 155.8072\n",
      "Epoch 1 Batch 113 Loss 234.6672\n",
      "Epoch 1 Batch 114 Loss 119.1915\n",
      "Epoch 1 Batch 115 Loss 109.2426\n",
      "Epoch 1 Batch 116 Loss 149.2708\n",
      "Epoch 1 Batch 117 Loss 146.1509\n",
      "Epoch 1 Batch 118 Loss 133.2184\n",
      "Epoch 1 Batch 119 Loss 116.9313\n",
      "Training Print: Epoch 1 Batch 119 Loss 149.0368\n",
      "Validation Print: Epoch 1 Batch 119 Loss 160.6981\n",
      "Epoch 1 Batch 120 Loss 122.2879\n",
      "Epoch 1 Batch 121 Loss 139.8824\n",
      "Epoch 1 Batch 122 Loss 150.2042\n",
      "Epoch 1 Batch 123 Loss 154.8672\n",
      "Epoch 1 Batch 124 Loss 125.3884\n",
      "Epoch 1 Batch 125 Loss 158.2272\n",
      "Epoch 1 Batch 126 Loss 117.6602\n",
      "Epoch 1 Batch 127 Loss 133.2682\n",
      "Epoch 1 Batch 128 Loss 148.9811\n",
      "Epoch 1 Batch 129 Loss 167.8827\n",
      "Epoch 1 Batch 130 Loss 134.6478\n",
      "Epoch 1 Batch 131 Loss 137.1575\n",
      "Epoch 1 Batch 132 Loss 149.4732\n",
      "Epoch 1 Batch 133 Loss 149.3065\n",
      "Epoch 1 Batch 134 Loss 154.5382\n",
      "Epoch 1 Batch 135 Loss 133.9482\n",
      "Epoch 1 Batch 136 Loss 142.1836\n",
      "Epoch 1 Batch 137 Loss 133.4913\n",
      "Epoch 1 Batch 138 Loss 118.6012\n",
      "Epoch 1 Batch 139 Loss 131.4942\n",
      "Epoch 1 Batch 140 Loss 154.9744\n",
      "Epoch 1 Batch 141 Loss 123.8577\n",
      "Epoch 1 Batch 142 Loss 146.0165\n",
      "Epoch 1 Batch 143 Loss 125.0510\n",
      "Epoch 1 Batch 144 Loss 174.6149\n",
      "Epoch 1 Batch 145 Loss 173.5961\n",
      "Epoch 1 Batch 146 Loss 152.8784\n",
      "Epoch 1 Batch 147 Loss 146.8273\n",
      "Epoch 1 Batch 148 Loss 117.1015\n",
      "Epoch 1 Batch 149 Loss 106.0924\n",
      "Training Print: Epoch 1 Batch 149 Loss 140.8167\n",
      "Validation Print: Epoch 1 Batch 149 Loss 154.8717\n",
      "Epoch 1 Batch 150 Loss 142.3949\n",
      "Epoch 1 Batch 151 Loss 126.0389\n",
      "Epoch 1 Batch 152 Loss 117.4977\n",
      "Epoch 1 Batch 153 Loss 166.7182\n",
      "Epoch 1 Batch 154 Loss 122.8641\n",
      "Epoch 1 Batch 155 Loss 138.6397\n",
      "Epoch 1 Batch 156 Loss 159.2300\n",
      "Epoch 1 Batch 157 Loss 135.5784\n",
      "Epoch 1 Batch 158 Loss 167.0155\n",
      "Epoch 1 Batch 159 Loss 153.6001\n",
      "Epoch 1 Batch 160 Loss 143.8705\n",
      "Epoch 1 Batch 161 Loss 149.0382\n",
      "Epoch 1 Batch 162 Loss 149.5205\n",
      "Epoch 1 Batch 163 Loss 142.8405\n",
      "Epoch 1 Batch 164 Loss 141.8245\n",
      "Epoch 1 Batch 165 Loss 136.7314\n",
      "Epoch 1 Batch 166 Loss 121.6931\n",
      "Epoch 1 Batch 167 Loss 121.3450\n",
      "Epoch 1 Batch 168 Loss 163.0668\n",
      "Epoch 1 Batch 169 Loss 170.1468\n",
      "Epoch 1 Batch 170 Loss 129.2799\n",
      "Epoch 1 Batch 171 Loss 126.0247\n",
      "Epoch 1 Batch 172 Loss 126.9103\n",
      "Epoch 1 Batch 173 Loss 158.8278\n",
      "Epoch 1 Batch 174 Loss 112.0958\n",
      "Epoch 1 Batch 175 Loss 139.6034\n",
      "Epoch 1 Batch 176 Loss 114.2538\n",
      "Epoch 1 Batch 177 Loss 116.2874\n",
      "Epoch 1 Batch 178 Loss 136.0128\n",
      "Epoch 1 Batch 179 Loss 113.6378\n",
      "Training Print: Epoch 1 Batch 179 Loss 138.0863\n",
      "Validation Print: Epoch 1 Batch 179 Loss 148.7276\n",
      "Epoch 1 Batch 180 Loss 150.8270\n",
      "Epoch 1 Batch 181 Loss 153.9765\n",
      "Epoch 1 Batch 182 Loss 157.4631\n",
      "Epoch 1 Batch 183 Loss 132.8654\n",
      "Epoch 1 Batch 184 Loss 134.5743\n",
      "Epoch 1 Batch 185 Loss 143.4772\n",
      "Epoch 1 Batch 186 Loss 131.5711\n",
      "Epoch 1 Batch 187 Loss 137.2774\n",
      "Epoch 1 Batch 188 Loss 118.8619\n",
      "Epoch 1 Batch 189 Loss 127.0369\n",
      "Epoch 1 Batch 190 Loss 133.5663\n",
      "Epoch 1 Batch 191 Loss 130.4302\n",
      "Epoch 1 Batch 192 Loss 132.0304\n",
      "Epoch 1 Batch 193 Loss 121.3271\n",
      "Epoch 1 Batch 194 Loss 149.9682\n",
      "Epoch 1 Batch 195 Loss 111.7896\n",
      "Epoch 1 Batch 196 Loss 145.1421\n",
      "Epoch 1 Batch 197 Loss 141.1809\n",
      "Epoch 1 Batch 198 Loss 145.9102\n",
      "Epoch 1 Batch 199 Loss 121.7725\n",
      "Saving checkpoint for batch 199 at ~/general-304503/colab_logs/jvs_transformer_stt_12_3_21/checkpoints/ckpt-1\n",
      "Epoch 1 Batch 200 Loss 122.0089\n",
      "Epoch 1 Batch 201 Loss 117.8036\n",
      "Epoch 1 Batch 202 Loss 120.2221\n",
      "Epoch 1 Batch 203 Loss 136.7588\n",
      "Epoch 1 Batch 204 Loss 126.5479\n",
      "Epoch 1 Batch 205 Loss 123.8313\n",
      "Epoch 1 Batch 206 Loss 131.3173\n",
      "Epoch 1 Batch 207 Loss 137.8518\n",
      "Epoch 1 Batch 208 Loss 137.1363\n",
      "Epoch 1 Batch 209 Loss 138.0121\n",
      "Training Print: Epoch 1 Batch 209 Loss 133.7513\n",
      "Validation Print: Epoch 1 Batch 209 Loss 146.4644\n",
      "Epoch 1 Batch 210 Loss 111.6968\n",
      "Epoch 1 Batch 211 Loss 134.7134\n",
      "Epoch 1 Batch 212 Loss 106.2113\n",
      "Epoch 1 Batch 213 Loss 121.0662\n",
      "Epoch 1 Batch 214 Loss 126.0273\n",
      "Epoch 1 Batch 215 Loss 114.5119\n",
      "Epoch 1 Batch 216 Loss 117.3719\n",
      "Epoch 1 Batch 217 Loss 139.5927\n",
      "Epoch 1 Batch 218 Loss 153.1128\n",
      "Epoch 1 Batch 219 Loss 138.7506\n",
      "Epoch 1 Batch 220 Loss 134.0215\n",
      "Epoch 1 Batch 221 Loss 115.6680\n",
      "Epoch 1 Batch 222 Loss 126.1982\n",
      "Epoch 1 Batch 223 Loss 150.4113\n",
      "Epoch 1 Batch 224 Loss 118.2577\n",
      "Epoch 1 Batch 225 Loss 113.5748\n",
      "Epoch 1 Batch 226 Loss 149.1444\n",
      "Epoch 1 Batch 227 Loss 113.2748\n",
      "Epoch 1 Batch 228 Loss 124.3313\n",
      "Epoch 1 Batch 229 Loss 125.5501\n",
      "Epoch 1 Batch 230 Loss 158.2810\n",
      "Epoch 1 Batch 231 Loss 126.8565\n",
      "Epoch 1 Batch 232 Loss 146.6405\n",
      "Epoch 1 Batch 233 Loss 137.6768\n",
      "Epoch 1 Batch 234 Loss 127.0518\n",
      "Epoch 1 Batch 235 Loss 133.6177\n",
      "Epoch 1 Batch 236 Loss 130.5857\n",
      "Epoch 1 Batch 237 Loss 141.2985\n",
      "Epoch 1 Batch 238 Loss 142.8219\n",
      "Epoch 1 Batch 239 Loss 110.1938\n",
      "Training Print: Epoch 1 Batch 239 Loss 129.6170\n",
      "Validation Print: Epoch 1 Batch 239 Loss 143.7220\n",
      "Epoch 1 Batch 240 Loss 167.2636\n",
      "Epoch 1 Batch 241 Loss 121.1644\n",
      "Epoch 1 Batch 242 Loss 110.5291\n",
      "Epoch 1 Batch 243 Loss 163.0848\n",
      "Epoch 1 Batch 244 Loss 132.5764\n",
      "Epoch 1 Batch 245 Loss 123.3965\n",
      "Epoch 1 Batch 246 Loss 113.9068\n",
      "Epoch 1 Batch 247 Loss 112.6023\n",
      "Epoch 1 Batch 248 Loss 155.7509\n",
      "Epoch 1 Batch 249 Loss 125.0106\n",
      "Epoch 1 Batch 250 Loss 112.5433\n",
      "Epoch 1 Batch 251 Loss 142.5100\n",
      "Epoch 1 Batch 252 Loss 116.0871\n",
      "Epoch 1 Batch 253 Loss 126.0088\n",
      "Epoch 1 Batch 254 Loss 157.3996\n",
      "Epoch 1 Batch 255 Loss 110.1697\n",
      "Epoch 1 Batch 256 Loss 97.1895\n",
      "Epoch 1 Batch 257 Loss 137.8510\n",
      "Epoch 1 Batch 258 Loss 127.7391\n",
      "Epoch 1 Batch 259 Loss 155.6630\n",
      "Epoch 1 Batch 260 Loss 116.9812\n",
      "Epoch 1 Batch 261 Loss 145.6329\n",
      "Epoch 1 Batch 262 Loss 131.2694\n",
      "Epoch 1 Batch 263 Loss 121.7852\n",
      "Epoch 1 Batch 264 Loss 110.0836\n",
      "Epoch 1 Batch 265 Loss 113.9864\n",
      "Epoch 1 Batch 266 Loss 131.0756\n",
      "Epoch 1 Batch 267 Loss 134.3203\n",
      "Epoch 1 Batch 268 Loss 114.2688\n",
      "Epoch 1 Batch 269 Loss 148.6645\n",
      "Training Print: Epoch 1 Batch 269 Loss 129.2171\n",
      "Validation Print: Epoch 1 Batch 269 Loss 141.9734\n",
      "Epoch 1 Batch 270 Loss 114.7074\n",
      "Epoch 1 Batch 271 Loss 132.3270\n",
      "Epoch 1 Batch 272 Loss 115.1377\n",
      "Epoch 1 Batch 273 Loss 123.3050\n",
      "Epoch 1 Batch 274 Loss 142.9489\n",
      "Epoch 1 Batch 275 Loss 105.4906\n",
      "Epoch 1 Batch 276 Loss 107.9874\n",
      "Epoch 1 Batch 277 Loss 129.4439\n",
      "Epoch 1 Batch 278 Loss 125.9796\n",
      "Epoch 1 Batch 279 Loss 128.4777\n",
      "Epoch 1 Batch 280 Loss 118.9615\n",
      "Epoch 1 Batch 281 Loss 94.0134\n",
      "Epoch 1 Batch 282 Loss 134.1926\n",
      "Epoch 1 Batch 283 Loss 142.8945\n",
      "Epoch 1 Batch 284 Loss 123.3762\n",
      "Epoch 1 Batch 285 Loss 116.5602\n",
      "Epoch 1 Batch 286 Loss 123.3282\n",
      "Epoch 1 Batch 287 Loss 111.6190\n",
      "Epoch 1 Batch 288 Loss 141.4290\n",
      "Epoch 1 Batch 289 Loss 147.0107\n",
      "Epoch 1 Batch 290 Loss 125.1931\n",
      "Epoch 1 Batch 291 Loss 147.7054\n",
      "Epoch 1 Batch 292 Loss 94.5834\n",
      "Epoch 1 Batch 293 Loss 161.6402\n",
      "Epoch 1 Batch 294 Loss 101.3761\n",
      "Epoch 1 Batch 295 Loss 108.0882\n",
      "Epoch 1 Batch 296 Loss 137.6759\n",
      "Epoch 1 Batch 297 Loss 138.4409\n",
      "Epoch 1 Batch 298 Loss 128.9088\n",
      "Epoch 1 Batch 299 Loss 133.9701\n",
      "Training Print: Epoch 1 Batch 299 Loss 125.2257\n",
      "Validation Print: Epoch 1 Batch 299 Loss 144.6457\n",
      "Epoch 1 Batch 300 Loss 121.5652\n",
      "Epoch 1 Batch 301 Loss 111.6362\n",
      "Epoch 1 Batch 302 Loss 120.7224\n",
      "Epoch 1 Batch 303 Loss 112.8551\n",
      "Epoch 1 Batch 304 Loss 103.9086\n",
      "Epoch 1 Batch 305 Loss 138.4720\n",
      "Epoch 1 Batch 306 Loss 133.6201\n",
      "Epoch 1 Batch 307 Loss 111.4068\n",
      "Epoch 1 Batch 308 Loss 140.0150\n",
      "Epoch 1 Batch 309 Loss 144.8071\n",
      "Epoch 1 Batch 310 Loss 137.5354\n",
      "Epoch 1 Batch 311 Loss 136.8214\n",
      "Epoch 1 Batch 312 Loss 115.5142\n",
      "Epoch 1 Batch 313 Loss 109.1090\n",
      "Epoch 1 Batch 314 Loss 107.9951\n",
      "Epoch 1 Batch 315 Loss 120.9222\n",
      "Epoch 1 Batch 316 Loss 125.1971\n",
      "Epoch 1 Batch 317 Loss 108.8408\n",
      "Epoch 1 Batch 318 Loss 140.9525\n",
      "Epoch 1 Batch 319 Loss 98.8001\n",
      "Epoch 1 Batch 320 Loss 121.5256\n",
      "Epoch 1 Batch 321 Loss 117.1318\n",
      "Epoch 1 Batch 322 Loss 135.5978\n",
      "Epoch 1 Batch 323 Loss 142.7344\n",
      "Epoch 1 Batch 324 Loss 146.3272\n",
      "Epoch 1 Batch 325 Loss 116.1979\n",
      "Epoch 1 Batch 326 Loss 127.7976\n",
      "Epoch 1 Batch 327 Loss 123.2641\n",
      "Epoch 1 Batch 328 Loss 109.3782\n",
      "Epoch 1 Batch 329 Loss 101.5064\n",
      "Training Print: Epoch 1 Batch 329 Loss 122.7386\n",
      "Validation Print: Epoch 1 Batch 329 Loss 143.7130\n",
      "Epoch 1 Batch 330 Loss 135.4711\n",
      "Epoch 1 Batch 331 Loss 117.6739\n",
      "Epoch 1 Batch 332 Loss 130.9006\n",
      "Epoch 1 Batch 333 Loss 120.9640\n",
      "Epoch 1 Batch 334 Loss 145.0960\n",
      "Epoch 1 Batch 335 Loss 123.9618\n",
      "Epoch 1 Batch 336 Loss 125.5313\n",
      "Epoch 1 Batch 337 Loss 97.4815\n",
      "Epoch 1 Batch 338 Loss 133.6232\n",
      "Epoch 1 Batch 339 Loss 110.4527\n",
      "Epoch 1 Batch 340 Loss 152.5959\n",
      "Epoch 1 Batch 341 Loss 132.1664\n",
      "Epoch 1 Batch 342 Loss 140.2892\n",
      "Epoch 1 Batch 343 Loss 145.1343\n",
      "Epoch 1 Batch 344 Loss 109.3696\n",
      "Epoch 1 Batch 345 Loss 123.1610\n",
      "Epoch 1 Batch 346 Loss 134.0090\n",
      "Epoch 1 Batch 347 Loss 123.5613\n",
      "Epoch 1 Batch 348 Loss 129.9419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-072d21b50571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#  continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  validation_loss.reset_states()\n",
    "  instant_loss.reset_states()\n",
    "\n",
    "  jsut_dataset.take(5)\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, data) in enumerate(jvs_dataset):\n",
    "    #print(batch)\n",
    "    #if batch == 9:\n",
    "    #  print(tokenizer.detokenize(data['text']).numpy()[0].decode('utf-8'))\n",
    "    #  print(data['text'].numpy()[0])\n",
    "    #  continue\n",
    "\n",
    "    train_step(data['speech'], data['text'], data['speech_lengths'], data['text_lengths'])\n",
    "\n",
    "\n",
    "    if (batch+1) % print_period == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {instant_loss.result():.4f}')\n",
    "      instant_loss.reset_states()\n",
    "\n",
    "    if (batch+1) % train_period == 0:\n",
    "      with file_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=batch)\n",
    "      print(f'Training Print: Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
    "      train_loss.reset_states()\n",
    "\n",
    "    if (batch + 1) % save_period == 0:\n",
    "      ckpt_save_path = ckpt_manager.save()\n",
    "      print (f'Saving checkpoint for batch {batch} at {ckpt_save_path}')\n",
    "    if (batch + 1) % validate_period == 0:\n",
    "      for (val_batch, val_data) in enumerate(jsut_dataset.take(5)):\n",
    "        validation_step(val_data['speech'], val_data['text'], val_data['speech_lengths'], val_data['text_lengths'])\n",
    "      with file_writer.as_default():\n",
    "        tf.summary.scalar(\"validation_loss\", validation_loss.result(), step=batch)\n",
    "      print(f'Validation Print: Epoch {epoch + 1} Batch {batch} Loss {validation_loss.result():.4f}')\n",
    "      validation_loss.reset_states()\n",
    "\n",
    "  break\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOA7hItRagYh95XqJUwn/LF",
   "include_colab_link": true,
   "name": "Untitled22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
