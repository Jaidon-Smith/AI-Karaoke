{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSnYKpDlZsu5rQqVCO5VNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/Japanese%20STT%20Version%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvMVV72-thw"
      },
      "source": [
        "# Exploration of Tensorflow CTC Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCcNyRFYAGQb"
      },
      "source": [
        "# Sentence piece for grapheme based BPE https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4ir3AiAwZf"
      },
      "source": [
        "Upon reading the github docs, there appears to be some tensorflow integration if you search 'Sentencepiece'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTgKetySp8z"
      },
      "source": [
        "# Exploring pretained tokenisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eyNrmEY8bbs6"
      },
      "source": [
        "#@title Install dependencies\n",
        "!pip install --quiet tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_gEuvjnXjfMG"
      },
      "source": [
        "#@title Import dependencies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NnAXoCtiSude"
      },
      "source": [
        "#@title Original hub code\n",
        "!pip install tensorflow-text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "n_layer = 12\n",
        "d_model = 768\n",
        "max_gen_len = 128\n",
        "\n",
        "def generate(module, inputs, mems):\n",
        "  \"\"\"Generate text.\"\"\"\n",
        "  inputs = tf.dtypes.cast(inputs, tf.int64)\n",
        "  generation_input_dict = dict(input_tokens=inputs)\n",
        "  mems_dict = {}\n",
        "  for i in range(n_layer):\n",
        "    mems_dict[\"mem_{}\".format(i)] = mems[i]\n",
        "  generation_input_dict.update(mems_dict)\n",
        "\n",
        "  generation_outputs = module(generation_input_dict, signature=\"prediction\",\n",
        "                              as_dict=True)\n",
        "  probs = generation_outputs[\"probs\"]\n",
        "\n",
        "  new_mems = []\n",
        "  for i in range(n_layer):\n",
        "    new_mems.append(generation_outputs[\"new_mem_{}\".format(i)])\n",
        "\n",
        "  return probs, new_mems\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  text = [\"\\n_START_ARTICLE_\\nしのぶ・まさみshow'05 恋してラララ\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\"]\n",
        "\n",
        "  # Word embeddings.\n",
        "  embeddings = module(dict(text=text), signature=\"word_embeddings\",\n",
        "                      as_dict=True)\n",
        "  embeddings = embeddings[\"word_embeddings\"]\n",
        "\n",
        "  # Activations at each layer.\n",
        "  activations = module(dict(text=text),signature=\"activations\", as_dict=True)\n",
        "  activations = activations[\"activations\"]\n",
        "\n",
        "  # Negative log likelihood of the text, and perplexity.\n",
        "  neg_log_likelihood = module(dict(text=text), signature=\"neg_log_likelihood\",\n",
        "                              as_dict=True)\n",
        "  neg_log_likelihood = neg_log_likelihood[\"neg_log_likelihood\"]\n",
        "  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))\n",
        "\n",
        "  # Tokenization and detokenization with the sentencepiece model.\n",
        "  token_ids = module(dict(text=text), signature=\"tokenization\", as_dict=True)\n",
        "  token_ids = token_ids[\"token_ids\"]\n",
        "\n",
        "  detoken_text = module(dict(token_ids=token_ids), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "  # Generation\n",
        "  mems_np = [np.zeros([1, 0, d_model], dtype=np.float32) for _ in range(n_layer)]\n",
        "  inputs_np = token_ids\n",
        "  sampled_ids = []\n",
        "  for step in range(max_gen_len):\n",
        "    probs, mems_np = generate(module, inputs_np, mems_np)\n",
        "    sampled_id = tf.random.categorical(tf.math.log(probs[0]), num_samples=1, dtype=tf.int32)\n",
        "    sampled_id = tf.squeeze(sampled_id)\n",
        "\n",
        "    sampled_ids.append(sampled_id)\n",
        "    inputs_np = tf.reshape(sampled_id, [1, 1])\n",
        "\n",
        "  sampled_ids = tf.expand_dims(sampled_ids, axis=0)\n",
        "  generated_text = module(dict(token_ids=sampled_ids),\n",
        "                          signature=\"detokenization\", as_dict=True)\n",
        "  generated_text = generated_text[\"text\"]\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text = session.run([\n",
        "    embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Itn-XLBvjn6S"
      },
      "source": [
        "#@title Original hub code without generation\n",
        "!pip install tensorflow-text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "n_layer = 12\n",
        "d_model = 768\n",
        "max_gen_len = 128\n",
        "\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  text = [\"\\n_START_ARTICLE_\\nしのぶ・まさみshow'05 恋してラララ\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\"]\n",
        "\n",
        "  # Word embeddings.\n",
        "  embeddings = module(dict(text=text), signature=\"word_embeddings\",\n",
        "                      as_dict=True)\n",
        "  embeddings = embeddings[\"word_embeddings\"]\n",
        "\n",
        "  # Activations at each layer.\n",
        "  activations = module(dict(text=text),signature=\"activations\", as_dict=True)\n",
        "  activations = activations[\"activations\"]\n",
        "\n",
        "  # Negative log likelihood of the text, and perplexity.\n",
        "  neg_log_likelihood = module(dict(text=text), signature=\"neg_log_likelihood\",\n",
        "                              as_dict=True)\n",
        "  neg_log_likelihood = neg_log_likelihood[\"neg_log_likelihood\"]\n",
        "  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))\n",
        "\n",
        "  # Tokenization and detokenization with the sentencepiece model.\n",
        "  token_ids = module(dict(text=text), signature=\"tokenization\", as_dict=True)\n",
        "  token_ids = token_ids[\"token_ids\"]\n",
        "\n",
        "  detoken_text = module(dict(token_ids=token_ids), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text = session.run([\n",
        "    embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-skOhvXxkchi",
        "outputId": "de20679a-bf38-49dc-ed48-a6c4d1ef466b"
      },
      "source": [
        "token_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   13,     3,    13,    32,     7,  1060,    12,  6708,   198,\n",
              "         4888,  6824,   577,  8469,    13,  1824,    65,   125, 12974,\n",
              "           13,     4,    13,    54,    13,     5,    13,    33,  3322,\n",
              "         9505, 20236,    35,     7, 16745,   219,  3174,  6761, 11421,\n",
              "           19,     8, 10110,  3549,     7,    53,  1202,    32,     7,\n",
              "         1060,    20,   546,    84,  1967,   315,    15,  4871, 13775,\n",
              "         2624,    19,  2702,    27,   296,   736, 14103,   175,     9,\n",
              "         2819,    10,   542]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VH65JJJtkgup",
        "outputId": "ab9fa654-604a-44b2-9f47-5d9004c6d42b"
      },
      "source": [
        "detoken_text[0].decode()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"_START_ARTICLE_ しのぶ・まさみshow'05 恋してラララ _START_SECTION_ 概要 _START_PARAGRAPH_ 『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaD8O8jetED7",
        "outputId": "c7752f15-80af-4f1b-8763-3cca905d9cf2"
      },
      "source": [
        "token_ids.shape\n",
        "num_tokens = 10000\n",
        "token_explorer = list(range(num_tokens))\n",
        "for i in range(num_tokens - 1):\n",
        "  token_explorer.insert(num_tokens - 1 - i, 0)\n",
        "token_explorer = np.array([token_explorer])\n",
        "token_explorer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    1, ..., 9998,    0, 9999]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6aAXqlt0Wa",
        "outputId": "4c9252f0-0092-42d9-b4d1-ac022c18992e"
      },
      "source": [
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  detoken_text = module(dict(token_ids=token_explorer), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  detoken_text = session.run([\n",
        "    detoken_text])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAPZs7ahvrm-",
        "outputId": "aa3f7306-a4d9-4fff-c6b0-9857579da0e3"
      },
      "source": [
        "detoken_text[0][0].decode().split('⁇')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '  ',\n",
              " '  ',\n",
              " '  ',\n",
              " ' _START_ARTICLE_ ',\n",
              " ' _START_SECTION_ ',\n",
              " ' _START_PARAGRAPH_ ',\n",
              " ' _NEWLINE_ ',\n",
              " ' の ',\n",
              " ' 、 ',\n",
              " ' 。 ',\n",
              " ' は ',\n",
              " ' 年 ',\n",
              " ' ・ ',\n",
              " '   ',\n",
              " ' ) ',\n",
              " ' が ',\n",
              " ' ( ',\n",
              " ' に ',\n",
              " ' を ',\n",
              " ' で ',\n",
              " ' と ',\n",
              " ' 月 ',\n",
              " ' 」 ',\n",
              " ' 「 ',\n",
              " ' 2 ',\n",
              " ' 1 ',\n",
              " ' から ',\n",
              " ' や ',\n",
              " ' 3 ',\n",
              " ' 日 ',\n",
              " ' である ',\n",
              " ' した ',\n",
              " ' し ',\n",
              " ' 『 ',\n",
              " ' も ',\n",
              " ' 』 ',\n",
              " ' として ',\n",
              " ' 4 ',\n",
              " ' する ',\n",
              " ' 年に ',\n",
              " ' では ',\n",
              " ' 5 ',\n",
              " ' た ',\n",
              " ' 6 ',\n",
              " ' には ',\n",
              " ' また ',\n",
              " ' 7 ',\n",
              " ' 10 ',\n",
              " ' など ',\n",
              " ' 第 ',\n",
              " ' 8 ',\n",
              " ' 9 ',\n",
              " ' この ',\n",
              " ' 大 ',\n",
              " ' 概要 ',\n",
              " ' る ',\n",
              " ' 12 ',\n",
              " ' という ',\n",
              " ' ス ',\n",
              " '  ( ',\n",
              " ' された ',\n",
              " ' その ',\n",
              " ' て ',\n",
              " ' - ',\n",
              " ' 日に ',\n",
              " ' して ',\n",
              " ' している ',\n",
              " ' 11 ',\n",
              " ' 人 ',\n",
              " ' となった ',\n",
              " ' な ',\n",
              " ' 市 ',\n",
              " '  - ',\n",
              " ' ている ',\n",
              " ' 日本 ',\n",
              " ' : ',\n",
              " ' 中 ',\n",
              " ' 山 ',\n",
              " ' 町 ',\n",
              " ' 一 ',\n",
              " ' により ',\n",
              " ' 回 ',\n",
              " ' であった ',\n",
              " ' による ',\n",
              " ' 本 ',\n",
              " ' 昭和 ',\n",
              " ' . ',\n",
              " ' され ',\n",
              " ' その後 ',\n",
              " ' となる ',\n",
              " ' によって ',\n",
              " ' 後 ',\n",
              " ' ア ',\n",
              " ' であり ',\n",
              " ' 子 ',\n",
              " ' , ',\n",
              " ' 月に ',\n",
              " ' 15 ',\n",
              " ' 長 ',\n",
              " ' より ',\n",
              " ' ト ',\n",
              " ' 17 ',\n",
              " ' 上 ',\n",
              " ' 川 ',\n",
              " ' 新 ',\n",
              " ' か ',\n",
              " ' 部 ',\n",
              " ' がある ',\n",
              " ' 同 ',\n",
              " ' ズ ',\n",
              " ' り ',\n",
              " ' 20 ',\n",
              " ' 元 ',\n",
              " ' ため ',\n",
              " ' ク ',\n",
              " ' 16 ',\n",
              " ' へ ',\n",
              " ' ド ',\n",
              " ' 名 ',\n",
              " ' 同年 ',\n",
              " ' まで ',\n",
              " ' m ',\n",
              " ' 年には ',\n",
              " ' 駅 ',\n",
              " ' 家 ',\n",
              " ' ラ ',\n",
              " ' 、「 ',\n",
              " ' 郡 ',\n",
              " ' しかし ',\n",
              " ' 国 ',\n",
              " ' 線 ',\n",
              " ' カ ',\n",
              " ' される ',\n",
              " ' 東 ',\n",
              " ' A ',\n",
              " ' 高 ',\n",
              " ' s ',\n",
              " ' 道 ',\n",
              " ' となり ',\n",
              " ' 18 ',\n",
              " ' 号 ',\n",
              " ' ナ ',\n",
              " ' い ',\n",
              " ' 性 ',\n",
              " ' 三 ',\n",
              " ' 大学 ',\n",
              " ' などの ',\n",
              " ' 14 ',\n",
              " ' 村 ',\n",
              " ' 南 ',\n",
              " ' 年から ',\n",
              " ' 30 ',\n",
              " ' B ',\n",
              " ' だった ',\n",
              " ' 法 ',\n",
              " ' への ',\n",
              " ' において ',\n",
              " ' 西 ',\n",
              " ' 下 ',\n",
              " ' 島 ',\n",
              " ' されている ',\n",
              " ' 田 ',\n",
              " ' 者 ',\n",
              " ' 小 ',\n",
              " ' リ ',\n",
              " ' 分 ',\n",
              " ' にも ',\n",
              " ' S ',\n",
              " ' 水 ',\n",
              " ' マ ',\n",
              " ' ら ',\n",
              " ' 東京 ',\n",
              " ' タ ',\n",
              " ' 13 ',\n",
              " ' 明治 ',\n",
              " ' ていた ',\n",
              " ' 会 ',\n",
              " ' していた ',\n",
              " ' ル ',\n",
              " ' 平成 ',\n",
              " ' コ ',\n",
              " ' 北 ',\n",
              " ' 人物 ',\n",
              " ' 内 ',\n",
              " ' 生 ',\n",
              " ' 歴史 ',\n",
              " ' / ',\n",
              " ' 位 ',\n",
              " ' 前 ',\n",
              " ' お ',\n",
              " ' C ',\n",
              " ' ノ ',\n",
              " ' なお ',\n",
              " ' にて ',\n",
              " ' 王 ',\n",
              " ' 的な ',\n",
              " ' 地 ',\n",
              " ' 作品 ',\n",
              " ' み ',\n",
              " ' 映画 ',\n",
              " ' でも ',\n",
              " ' 経歴 ',\n",
              " ' 代 ',\n",
              " ' エ ',\n",
              " ' となっている ',\n",
              " ' および ',\n",
              " ' にある ',\n",
              " ' 戦 ',\n",
              " ' 数 ',\n",
              " ' 19 ',\n",
              " ' 正 ',\n",
              " ' 城 ',\n",
              " ' 金 ',\n",
              " ' 約 ',\n",
              " ' 野 ',\n",
              " ' 等 ',\n",
              " ' 現 ',\n",
              " ' とは ',\n",
              " ' サ ',\n",
              " ' 後に ',\n",
              " ' 』( ',\n",
              " ' 州 ',\n",
              " ' = ',\n",
              " ' レ ',\n",
              " ' 寺 ',\n",
              " ' き ',\n",
              " ' ダ ',\n",
              " ' 的 ',\n",
              " ' オ ',\n",
              " ' M ',\n",
              " ' った ',\n",
              " ' 神 ',\n",
              " ' 木 ',\n",
              " ' バ ',\n",
              " ' イ ',\n",
              " ' く ',\n",
              " ' 時 ',\n",
              " ' つ ',\n",
              " ' 県 ',\n",
              " ' 25 ',\n",
              " ' 化 ',\n",
              " ' デ ',\n",
              " ' 全 ',\n",
              " ' 放送 ',\n",
              " ' 二 ',\n",
              " ' フ ',\n",
              " ' F ',\n",
              " ' 文 ',\n",
              " ' 光 ',\n",
              " ' ム ',\n",
              " ' ロ ',\n",
              " ' 型 ',\n",
              " ' 手 ',\n",
              " ' 来歴 ',\n",
              " ' 平 ',\n",
              " ' 勝 ',\n",
              " ' R ',\n",
              " ' 学 ',\n",
              " ' シ ',\n",
              " ' D ',\n",
              " ' における ',\n",
              " ' 現在の ',\n",
              " ' 24 ',\n",
              " ' 21 ',\n",
              " ' 旧 ',\n",
              " ' マン ',\n",
              " ' 区 ',\n",
              " ' 石 ',\n",
              " ' 日本の ',\n",
              " ' ており ',\n",
              " ' 原 ',\n",
              " ' ジ ',\n",
              " ' 社 ',\n",
              " ' 海 ',\n",
              " ' ザ ',\n",
              " ' G ',\n",
              " ' こと ',\n",
              " ' \" ',\n",
              " ' 曲 ',\n",
              " ' 軍 ',\n",
              " ' があり ',\n",
              " ' 物 ',\n",
              " ' 作 ',\n",
              " ' 22 ',\n",
              " ' 世界 ',\n",
              " ' 行 ',\n",
              " ' 形 ',\n",
              " ' T ',\n",
              " ' キ ',\n",
              " ' リー ',\n",
              " ' 公 ',\n",
              " ' アルバム ',\n",
              " ' 面 ',\n",
              " ' ガ ',\n",
              " ' とともに ',\n",
              " ' 外 ',\n",
              " ' 音楽 ',\n",
              " ' さらに ',\n",
              " ' チ ',\n",
              " ' 23 ',\n",
              " ' 語 ',\n",
              " ' 立 ',\n",
              " ' P ',\n",
              " ' 和 ',\n",
              " ' 所 ',\n",
              " ' 賞 ',\n",
              " ' ティ ',\n",
              " ' さ ',\n",
              " ' 代表 ',\n",
              " ' 見 ',\n",
              " ' 力 ',\n",
              " ' 橋 ',\n",
              " ' 番組 ',\n",
              " ' を経て ',\n",
              " ' ミ ',\n",
              " ' 美 ',\n",
              " ' シリーズ ',\n",
              " ' ディ ',\n",
              " ' 28 ',\n",
              " ' 花 ',\n",
              " ' 式 ',\n",
              " ' 活動 ',\n",
              " ' J ',\n",
              " ' 地域 ',\n",
              " ' 局 ',\n",
              " ' す ',\n",
              " ' う ',\n",
              " ' 当時 ',\n",
              " ' ニ ',\n",
              " ' W ',\n",
              " ' 国際 ',\n",
              " ' 用 ',\n",
              " ' 系 ',\n",
              " ' 世 ',\n",
              " ' ネ ',\n",
              " ' 間 ',\n",
              " ' 26 ',\n",
              " ' 27 ',\n",
              " ' ゲーム ',\n",
              " ' 鉄道 ',\n",
              " ' こ ',\n",
              " ' 義 ',\n",
              " ' ツ ',\n",
              " ' 明 ',\n",
              " ' 度 ',\n",
              " ' 重 ',\n",
              " ' 及び ',\n",
              " ' ブ ',\n",
              " ' 研究 ',\n",
              " ' 学校 ',\n",
              " ' 出 ',\n",
              " ' 29 ',\n",
              " ' プ ',\n",
              " ' 車 ',\n",
              " ' K ',\n",
              " ' と共に ',\n",
              " ' 」( ',\n",
              " ' 天 ',\n",
              " ' ン ',\n",
              " ' 各 ',\n",
              " ' パ ',\n",
              " ' だ ',\n",
              " ' 馬 ',\n",
              " ' 教育 ',\n",
              " ' 氏 ',\n",
              " ' 2012 ',\n",
              " ' N ',\n",
              " ' 事業 ',\n",
              " ' グ ',\n",
              " ' 体 ',\n",
              " ' プロ ',\n",
              " ' 2009 ',\n",
              " ' イン ',\n",
              " ' チーム ',\n",
              " ' 信 ',\n",
              " ' ビ ',\n",
              " ' 2010 ',\n",
              " ' ない ',\n",
              " ' って ',\n",
              " ' について ',\n",
              " ' いた ',\n",
              " ' H ',\n",
              " ' セ ',\n",
              " ' 着 ',\n",
              " ' 級 ',\n",
              " ' ケ ',\n",
              " ' 2007 ',\n",
              " ' 安 ',\n",
              " ' 屋 ',\n",
              " ' 台 ',\n",
              " ' 2014 ',\n",
              " ' 目 ',\n",
              " ' L ',\n",
              " ' 版 ',\n",
              " ' 書 ',\n",
              " ' ウ ',\n",
              " ' のみ ',\n",
              " ' 頭 ',\n",
              " ' 万 ',\n",
              " ' 2015 ',\n",
              " ' ハ ',\n",
              " ' 、『 ',\n",
              " ' 年間 ',\n",
              " ' 2013 ',\n",
              " ' 宮 ',\n",
              " ' 2008 ',\n",
              " ' 風 ',\n",
              " ' 谷 ',\n",
              " ' V ',\n",
              " ' メ ',\n",
              " ' 期 ',\n",
              " ' ! ',\n",
              " ' 2011 ',\n",
              " ' 方 ',\n",
              " ' 英 ',\n",
              " ' そして ',\n",
              " ' 出身 ',\n",
              " ' 話 ',\n",
              " ' 機 ',\n",
              " ' 事 ',\n",
              " ' km ',\n",
              " ' シュ ',\n",
              " ' 開発 ',\n",
              " ' 流 ',\n",
              " ' 時間 ',\n",
              " ' 初 ',\n",
              " ' になった ',\n",
              " ' リーグ ',\n",
              " ' E ',\n",
              " ' 直 ',\n",
              " ' 年まで ',\n",
              " ' とも ',\n",
              " ' 通 ',\n",
              " ' 中国 ',\n",
              " ' アメリカ ',\n",
              " ' テ ',\n",
              " ' ある ',\n",
              " ' 船 ',\n",
              " ' 点 ',\n",
              " ' 場 ',\n",
              " ' 文化 ',\n",
              " ' だが ',\n",
              " ' 特に ',\n",
              " ' これは ',\n",
              " ' 2017 ',\n",
              " ' クラブ ',\n",
              " ' FC ',\n",
              " ' になる ',\n",
              " ' 院 ',\n",
              " ' アル ',\n",
              " ' 白 ',\n",
              " ' モ ',\n",
              " ' 成 ',\n",
              " ' 40 ',\n",
              " ' 科 ',\n",
              " ' 星 ',\n",
              " ' 100 ',\n",
              " ' 多くの ',\n",
              " ' ～ ',\n",
              " ' 」「 ',\n",
              " ' 2016 ',\n",
              " ' 無 ',\n",
              " ' すると ',\n",
              " ' 選手 ',\n",
              " ' とする ',\n",
              " ' 翌 ',\n",
              " ' バー ',\n",
              " ' 地区 ',\n",
              " ' を持つ ',\n",
              " ' 種 ',\n",
              " ' 史 ',\n",
              " ' ま ',\n",
              " ' 両 ',\n",
              " ' 大会 ',\n",
              " ' 50 ',\n",
              " ' 館 ',\n",
              " ' 真 ',\n",
              " ' 事件 ',\n",
              " ' 口 ',\n",
              " ' 監督 ',\n",
              " ' シングル ',\n",
              " ' 高校 ',\n",
              " ' 清 ',\n",
              " ' 政 ',\n",
              " ' 2006 ',\n",
              " ' グループ ',\n",
              " ' 31 ',\n",
              " ' ゴ ',\n",
              " ' 株式会社 ',\n",
              " ' 次 ',\n",
              " ' 不 ',\n",
              " ' t ',\n",
              " ' フランス ',\n",
              " ' 歳 ',\n",
              " ' アン ',\n",
              " ' 吉 ',\n",
              " ' 略歴 ',\n",
              " ' 2018 ',\n",
              " ' カー ',\n",
              " ' 人の ',\n",
              " ' 現在は ',\n",
              " ' 官 ',\n",
              " ' 組 ',\n",
              " ' バス ',\n",
              " ' テレビ ',\n",
              " ' 実 ',\n",
              " ' 井 ',\n",
              " ' できる ',\n",
              " ' 情報 ',\n",
              " ' ボ ',\n",
              " ' 地方 ',\n",
              " ' トン ',\n",
              " ' U ',\n",
              " ' y ',\n",
              " ' サン ',\n",
              " ' 小学校 ',\n",
              " ' 中学校 ',\n",
              " ' つの ',\n",
              " ' 門 ',\n",
              " ' ヤ ',\n",
              " ' 主 ',\n",
              " ' 男 ',\n",
              " ' 番 ',\n",
              " ' 時代 ',\n",
              " ' 江 ',\n",
              " ' 会社 ',\n",
              " ' 問題 ',\n",
              " ' 役 ',\n",
              " ' 大正 ',\n",
              " ' 中央 ',\n",
              " ' 尾 ',\n",
              " ' 都市 ',\n",
              " ' 室 ',\n",
              " ' しており ',\n",
              " ' 店 ',\n",
              " ' 施設 ',\n",
              " ' 計画 ',\n",
              " ' のため ',\n",
              " ' 制 ',\n",
              " ' 雄 ',\n",
              " ' 気 ',\n",
              " ' 試合 ',\n",
              " ' 女 ',\n",
              " ' 2000 ',\n",
              " ' 心 ',\n",
              " ' I ',\n",
              " ' 久 ',\n",
              " ' と呼ばれる ',\n",
              " ' 大阪 ',\n",
              " ' ドイツ ',\n",
              " ' め ',\n",
              " ' 協会 ',\n",
              " ' 朝 ',\n",
              " ' 宗 ',\n",
              " ' ヒ ',\n",
              " ' 教授 ',\n",
              " ' en ',\n",
              " ' を務めた ',\n",
              " ' 父 ',\n",
              " ' ラン ',\n",
              " ' それ ',\n",
              " ' センター ',\n",
              " ' ー ',\n",
              " ' 論 ',\n",
              " ' 春 ',\n",
              " ' 女子 ',\n",
              " ' X ',\n",
              " ' を行う ',\n",
              " ' ながら ',\n",
              " ' 巻 ',\n",
              " ' 生まれ ',\n",
              " ' 座 ',\n",
              " ' 〜 ',\n",
              " ' 関係 ',\n",
              " ' n ',\n",
              " ' システム ',\n",
              " ' 別 ',\n",
              " \" ' \",\n",
              " ' 色 ',\n",
              " ' シー ',\n",
              " ' 高等学校 ',\n",
              " ' 当時の ',\n",
              " ' 武 ',\n",
              " ' としては ',\n",
              " ' 他の ',\n",
              " ' ラー ',\n",
              " ' 2005 ',\n",
              " ' に関する ',\n",
              " ' 治 ',\n",
              " ' 愛 ',\n",
              " ' バンド ',\n",
              " ' じ ',\n",
              " ' 道路 ',\n",
              " ' 的に ',\n",
              " ' 歌 ',\n",
              " ' ことから ',\n",
              " ' 主に ',\n",
              " ' CD ',\n",
              " ' i ',\n",
              " ' に対して ',\n",
              " ' とされる ',\n",
              " ' 松 ',\n",
              " ' 林 ',\n",
              " ' 女性 ',\n",
              " ' 現在 ',\n",
              " ' ソ ',\n",
              " ' ほか ',\n",
              " ' シーズン ',\n",
              " ' 総 ',\n",
              " ' 社会 ',\n",
              " ' 音 ',\n",
              " ' モデル ',\n",
              " ' デビュー ',\n",
              " ' 領 ',\n",
              " ' ポ ',\n",
              " ' 多 ',\n",
              " ' コン ',\n",
              " ' ち ',\n",
              " ' 派 ',\n",
              " ' 経済 ',\n",
              " ' 定 ',\n",
              " ' 古 ',\n",
              " ' 御 ',\n",
              " ' O ',\n",
              " ' 他 ',\n",
              " ' 校 ',\n",
              " ' 神社 ',\n",
              " ' 権 ',\n",
              " ' 夫 ',\n",
              " ' されていた ',\n",
              " ' 日から ',\n",
              " ' ず ',\n",
              " ' 2004 ',\n",
              " ' 太 ',\n",
              " ' 丸 ',\n",
              " ' または ',\n",
              " ' 之 ',\n",
              " ' 月には ',\n",
              " ' 北海道 ',\n",
              " ' 円 ',\n",
              " ' 全国 ',\n",
              " ' 地理 ',\n",
              " ' 組織 ',\n",
              " ' ただし ',\n",
              " ' といった ',\n",
              " ' イギリス ',\n",
              " ' 当 ',\n",
              " ' 彼は ',\n",
              " ' があった ',\n",
              " ' 業 ',\n",
              " ' リン ',\n",
              " ' 京都 ',\n",
              " '  of ',\n",
              " ' スト ',\n",
              " ' 忠 ',\n",
              " ' 通り ',\n",
              " ' 構造 ',\n",
              " ' 生涯 ',\n",
              " ' 沢 ',\n",
              " ' 五 ',\n",
              " ' 入 ',\n",
              " ' む ',\n",
              " ' 管理 ',\n",
              " ' er ',\n",
              " ' 先 ',\n",
              " ' 技術 ',\n",
              " ' 英語 ',\n",
              " ' スポーツ ',\n",
              " ' 黒 ',\n",
              " ' cm ',\n",
              " ' 四 ',\n",
              " ' ホーム ',\n",
              " ' 兵 ',\n",
              " ' スター ',\n",
              " ' え ',\n",
              " ' ワ ',\n",
              " ' 徳 ',\n",
              " ' 岩 ',\n",
              " ' 後の ',\n",
              " ' 60 ',\n",
              " ' ソン ',\n",
              " ' 有 ',\n",
              " ' ニー ',\n",
              " ' 企業 ',\n",
              " ' ター ',\n",
              " ' を受け ',\n",
              " ' 制作 ',\n",
              " ' 条 ',\n",
              " ' 国道 ',\n",
              " '  S ',\n",
              " ' 生活 ',\n",
              " ' ために ',\n",
              " ' 士 ',\n",
              " ' 群 ',\n",
              " ' 津 ',\n",
              " ' マー ',\n",
              " ' e ',\n",
              " ' 字 ',\n",
              " ' Y ',\n",
              " ' ロー ',\n",
              " ' ベ ',\n",
              " ' 時に ',\n",
              " ' エル ',\n",
              " ' a ',\n",
              " ' 再び ',\n",
              " ' ピ ',\n",
              " ' 記録 ',\n",
              " ' 建設 ',\n",
              " ' については ',\n",
              " ' 八 ',\n",
              " ' 環境 ',\n",
              " ' もある ',\n",
              " ' 食 ',\n",
              " ' 特徴 ',\n",
              " ' 藩 ',\n",
              " ' っている ',\n",
              " ' 里 ',\n",
              " ' られた ',\n",
              " ' ものの ',\n",
              " ' & ',\n",
              " ' 発 ',\n",
              " ' あ ',\n",
              " ' 2003 ',\n",
              " ' これ ',\n",
              " ' 職 ',\n",
              " ' 広 ',\n",
              " ' 機能 ',\n",
              " ' したが ',\n",
              " ' 戦争 ',\n",
              " ' ドラマ ',\n",
              " ' を行った ',\n",
              " ' 販売 ',\n",
              " ' ように ',\n",
              " ' カル ',\n",
              " ' 年より ',\n",
              " ' ほど ',\n",
              " ' 企画 ',\n",
              " ' 守 ',\n",
              " ' に対する ',\n",
              " ' 堂 ',\n",
              " ' 月から ',\n",
              " ' 出場 ',\n",
              " ' ジャ ',\n",
              " ' 品 ',\n",
              " ' ヴィ ',\n",
              " ' アー ',\n",
              " ' 公園 ',\n",
              " ' サービス ',\n",
              " ' 団体 ',\n",
              " ' 戸 ',\n",
              " ' 森 ',\n",
              " ' 優勝 ',\n",
              " ' 例 ',\n",
              " ' レース ',\n",
              " ' 隊 ',\n",
              " ' 総合 ',\n",
              " ' ギ ',\n",
              " ' ファ ',\n",
              " ' ファン ',\n",
              " ' 永 ',\n",
              " ' 葉 ',\n",
              " ' 対 ',\n",
              " ' であったが ',\n",
              " ' h ',\n",
              " ' 々 ',\n",
              " ' 港 ',\n",
              " ' ビル ',\n",
              " ' 千 ',\n",
              " ' 母 ',\n",
              " ' )」 ',\n",
              " ' ヴァ ',\n",
              " ' 民 ',\n",
              " ' 類 ',\n",
              " ' 以降 ',\n",
              " ' 良 ',\n",
              " ' ことを ',\n",
              " ' 運動 ',\n",
              " ' 2002 ',\n",
              " ' 調査 ',\n",
              " ' 基 ',\n",
              " ' 利 ',\n",
              " ' 初の ',\n",
              " ' へと ',\n",
              " ' 発売 ',\n",
              " ' 様々な ',\n",
              " ' ナー ',\n",
              " ' いる ',\n",
              " ' 中に ',\n",
              " ' 当初は ',\n",
              " ' 2019 ',\n",
              " ' ではなく ',\n",
              " ' ジョン ',\n",
              " ' 土 ',\n",
              " ' 空 ',\n",
              " ' 路 ',\n",
              " ' 聖 ',\n",
              " ' を中心に ',\n",
              " ' とした ',\n",
              " ' 路線 ',\n",
              " ' よ ',\n",
              " ' 保 ',\n",
              " ' 日には ',\n",
              " ' 同じ ',\n",
              " ' イベント ',\n",
              " ' 夏 ',\n",
              " ' 出演 ',\n",
              " ' 足 ',\n",
              " ' 相 ',\n",
              " ' 米 ',\n",
              " ' 太郎 ',\n",
              " ' ぎ ',\n",
              " ' ラジオ ',\n",
              " ' 戦で ',\n",
              " ' 2001 ',\n",
              " ' 量 ',\n",
              " ' のうち ',\n",
              " ' ストーリー ',\n",
              " ' 都 ',\n",
              " ' 交通 ',\n",
              " ' 海軍 ',\n",
              " ' 夜 ',\n",
              " ' 身 ',\n",
              " ' カン ',\n",
              " ' 合 ',\n",
              " ' を含む ',\n",
              " ' のために ',\n",
              " ' 経 ',\n",
              " ' レン ',\n",
              " ' 師 ',\n",
              " ' 志 ',\n",
              " ' 産業 ',\n",
              " ' 波 ',\n",
              " ' mm ',\n",
              " ' 誌 ',\n",
              " ' 兼 ',\n",
              " ' 親 ',\n",
              " ' 連 ',\n",
              " ' に伴い ',\n",
              " ' 第一 ',\n",
              " ' 集 ',\n",
              " ' 草 ',\n",
              " ' 雑誌 ',\n",
              " ' わ ',\n",
              " ' 秒 ',\n",
              " ' 死 ',\n",
              " ' タイ ',\n",
              " ' 崎 ',\n",
              " ' 記 ',\n",
              " ' 開催 ',\n",
              " ' ” ',\n",
              " ' d ',\n",
              " ' 卒業 ',\n",
              " ' 党 ',\n",
              " ' 左 ',\n",
              " ' 知 ',\n",
              " ' させた ',\n",
              " ' 命 ',\n",
              " ' になり ',\n",
              " ' 大きな ',\n",
              " ' 委員会 ',\n",
              " ' 角 ',\n",
              " ' になっている ',\n",
              " ' ペ ',\n",
              " ' 根 ',\n",
              " ' ベル ',\n",
              " ' 80 ',\n",
              " ' ホ ',\n",
              " ' al ',\n",
              " ' 達 ',\n",
              " ' 赤 ',\n",
              " ' 特別 ',\n",
              " ' 豊 ',\n",
              " ' エン ',\n",
              " ' 彼の ',\n",
              " ' ダー ',\n",
              " ' 団 ',\n",
              " ' ば ',\n",
              " ' 六 ',\n",
              " ' 銀行 ',\n",
              " ' ロシア ',\n",
              " ' 解説 ',\n",
              " ' 自 ',\n",
              " ' ジュ ',\n",
              " ' 所属 ',\n",
              " ' シャ ',\n",
              " ' ヨーロッパ ',\n",
              " ' のような ',\n",
              " ' 物語 ',\n",
              " ' デザイン ',\n",
              " ' レイ ',\n",
              " ' メンバー ',\n",
              " ' 側 ',\n",
              " ' 状態 ',\n",
              " ' イタリア ',\n",
              " ' を受けた ',\n",
              " ' 車両 ',\n",
              " ' 場所 ',\n",
              " '  B ',\n",
              " ' などで ',\n",
              " ' 今 ',\n",
              " ' 半 ',\n",
              " ' アニメ ',\n",
              " ' 副 ',\n",
              " ' ヌ ',\n",
              " ' 利用 ',\n",
              " ' 生産 ',\n",
              " '  A ',\n",
              " ' NHK ',\n",
              " ' 最初の ',\n",
              " ' 彦 ',\n",
              " ' st ',\n",
              " ' 司 ',\n",
              " ' 政治 ',\n",
              " ' 員 ',\n",
              " ' リア ',\n",
              " ' 経営 ',\n",
              " ' 一方 ',\n",
              " ' ライブ ',\n",
              " ' んだ ',\n",
              " ' 湖 ',\n",
              " ' オー ',\n",
              " ' ことが ',\n",
              " ' 担当 ',\n",
              " ' 工場 ',\n",
              " ' ゲ ',\n",
              " ' されており ',\n",
              " ' 横 ',\n",
              " ' 評価 ',\n",
              " ' 郷 ',\n",
              " ' ことで ',\n",
              " ' 年度 ',\n",
              " ' 以後 ',\n",
              " ' 以上 ',\n",
              " ' させる ',\n",
              " ' リング ',\n",
              " ' そのため ',\n",
              " ' 研究所 ',\n",
              " ' 初代 ',\n",
              " ' 人間 ',\n",
              " ' 契約 ',\n",
              " ' を務める ',\n",
              " ' 部門 ',\n",
              " ' スーパー ',\n",
              " ' 人が ',\n",
              " ' 弾 ',\n",
              " ' さん ',\n",
              " ' 坂 ',\n",
              " ' 器 ',\n",
              " ' 元年 ',\n",
              " ' 進 ',\n",
              " ' にあった ',\n",
              " ' 秀 ',\n",
              " ' 教会 ',\n",
              " ' シン ',\n",
              " ' 病 ',\n",
              " ' in ',\n",
              " ' タイトル ',\n",
              " ' r ',\n",
              " ' 0 ',\n",
              " ' こう ',\n",
              " ' よりも ',\n",
              " ' b ',\n",
              " ' 右 ',\n",
              " ' 岡 ',\n",
              " ' 教 ',\n",
              " ' 省 ',\n",
              " ' 等の ',\n",
              " ' パリ ',\n",
              " ' 再 ',\n",
              " ' 七 ',\n",
              " ' 国家 ',\n",
              " ' c ',\n",
              " ' o ',\n",
              " ' ッ ',\n",
              " ' 電 ',\n",
              " ' れ ',\n",
              " ' 35 ',\n",
              " ' バン ',\n",
              " ' ランド ',\n",
              " ' よう ',\n",
              " ' ing ',\n",
              " ' 公演 ',\n",
              " ' 章 ',\n",
              " ' ものである ',\n",
              " ' 45 ',\n",
              " ' Z ',\n",
              " ' エンジン ',\n",
              " ' 薬 ',\n",
              " ' 伝 ',\n",
              " ' を行い ',\n",
              " ' サー ',\n",
              " ' されて ',\n",
              " ' 内容 ',\n",
              " ' 学園 ',\n",
              " ' 歳で ',\n",
              " ' 質 ',\n",
              " ' 70 ',\n",
              " ' それぞれ ',\n",
              " ' 製 ',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASYWPVCPzQZI"
      },
      "source": [
        "# Training our own tokenisation with google SentencePiece\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbuvEZLLzUVK"
      },
      "source": [
        "By reading the paper for Wiki-40b (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/18cd66cc7d31ce4c724cef1d2755b417f74de27c.pdf), it is clear that they do not use anything extra except the statistics based SentencePiece for tokenisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMBvznr_9WQ4"
      },
      "source": [
        "https://github.com/google/sentencepiece\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "https://github.com/google/sentencepiece/tree/master/python\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "All options for training can be viewed here: https://github.com/google/sentencepiece/blob/master/doc/options.md Note this was found by reading the readme on the main page\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWU2xb3MzqSq",
        "cellView": "form",
        "outputId": "d6a2a402-87be-4e41-d2c3-e53807b6a406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Install\n",
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 26.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 20.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 18.3MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 13.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 12.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 12.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 12.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 12.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 12.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 12.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "--2021-02-11 02:16:08--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-02-11 02:16:08 (17.9 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x3YzJBWdrjb",
        "outputId": "f6237e44-5e2d-4112-edab-70d305c7106a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Download the test folder from github\n",
        "!mkdir test\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt -O /content/test/botchan.txt\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/test/test_model.model -O /content/test/test_model.model\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘test’: File exists\n",
            "--2021-02-11 02:28:12--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘/content/test/botchan.txt’\n",
            "\n",
            "/content/test/botch 100%[===================>] 272.25K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-02-11 02:28:12 (16.6 MB/s) - ‘/content/test/botchan.txt’ saved [278779/278779]\n",
            "\n",
            "--2021-02-11 02:28:12--  https://raw.githubusercontent.com/google/sentencepiece/master/python/test/test_model.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 253165 (247K) [application/octet-stream]\n",
            "Saving to: ‘/content/test/test_model.model’\n",
            "\n",
            "/content/test/test_ 100%[===================>] 247.23K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-02-11 02:28:12 (21.3 MB/s) - ‘/content/test/test_model.model’ saved [253165/253165]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRQJt9ZDhAXw"
      },
      "source": [
        "## Tokenisation exploration of pretrained example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmQXf4KrgH4t"
      },
      "source": [
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor(model_file='test/test_model.model')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmluH6MLgMdK",
        "outputId": "7a41f9be-3bea-45dd-9223-76825c0dc177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.encode('This is a test')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[284, 47, 11, 4, 15, 400]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uDtSO7_gMhq",
        "outputId": "42599cd4-70ec-46ad-9745-947eaafc142a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.encode(['This is a test', 'Hello world'], out_type=int)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[284, 47, 11, 4, 15, 400], [151, 88, 21, 887]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQAxh_1RgMmf",
        "outputId": "65dc82bb-5bb8-44c5-82d4-fdd2a66ca488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.encode('This is a test', out_type=str)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁This', '▁is', '▁a', '▁', 't', 'est']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVNmFEoKgMtd",
        "outputId": "372dffd2-cc9b-461a-c2be-401963c7c4b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.encode(['This is a test', 'Hello world'], out_type=str)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁This', '▁is', '▁a', '▁', 't', 'est'], ['▁He', 'll', 'o', '▁world']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9bIi6pygeMf",
        "outputId": "9b1dcabf-2def-4d8a-e641-8f09cf386ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for _ in range(10):\n",
        "  print(sp.encode('This is a test', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁T', 'h', 'is', '▁is', '▁a', '▁', 't', 'est']\n",
            "['▁', 'This', '▁is', '▁a', '▁', 'te', 's', 't']\n",
            "['▁T', 'h', 'i', 's', '▁is', '▁', 'a', '▁', 'te', 'st']\n",
            "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁a', '▁', 't', 'e', 'st']\n",
            "['▁', 'This', '▁', 'i', 's', '▁', 'a', '▁', 'te', 's', 't']\n",
            "['▁This', '▁is', '▁', 'a', '▁', 'te', 's', 't']\n",
            "['▁This', '▁', 'i', 's', '▁', 'a', '▁', 't', 'e', 's', 't']\n",
            "['▁T', 'h', 'i', 's', '▁', 'is', '▁a', '▁', 'te', 's', 't']\n",
            "['▁', 'T', 'h', 'is', '▁', 'i', 's', '▁a', '▁', 'te', 'st']\n",
            "['▁This', '▁', 'is', '▁a', '▁', 'te', 's', 't']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6a0aw73gqAY",
        "outputId": "e94a4aff-47b4-4265-b567-a4aea665dc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sp.decode([284, 47, 11, 4, 15, 400])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is a test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9BZ5OwYgudn",
        "outputId": "54d8d59c-de6e-4524-942b-304a821f6ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.decode([[284, 47, 11, 4, 15, 400], [151, 88, 21, 887]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a test', 'Hello world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjW3HFV2gvef",
        "outputId": "0ba81cbe-c572-45fd-ccca-815efe68e7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sp.decode(['▁', 'This', '▁', 'is', '▁a', '▁', 't', 'e', 'st'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is a test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gGiiS2Ugvgv",
        "outputId": "cc418b88-80bb-40f3-b1dd-890ed66362c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.decode([['▁This', '▁is', '▁a', '▁', 't', 'est'], ['▁He', 'll', 'o', '▁world']])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a test', 'Hello world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks3KN6wXgviy",
        "outputId": "0f02af0c-61ef-4993-c6d4-f5bf2579ecba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.get_piece_size()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiGz57g4gvlS",
        "outputId": "9a38a802-de45-42b3-d4b6-997c10384e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sp.id_to_piece(2)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTHH86Grg0Nn",
        "outputId": "1e37a27f-d598-4f2d-de0d-a0607bc9afc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.id_to_piece([2, 3, 4])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>', '\\r', '▁']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvuchg0Xg0QM",
        "outputId": "e88b3341-91a6-40a7-c959-2445b89f9c97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.piece_to_id('<s>')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba3cEG39g0Si",
        "outputId": "f322641d-0263-4ce2-d188-f094ad3e3f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.piece_to_id(['</s>', '\\r', '▁'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spTbztJ5g3Lw",
        "outputId": "537a1b5e-9724-4926-d499-09a6c2897127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(sp)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewK0qqCAg4b_",
        "outputId": "3e545750-5bde-4191-a3be-faa79f9e58a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp['</s>']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXxq4WDThoxj"
      },
      "source": [
        "## Training example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPD4GgQKhuO-"
      },
      "source": [
        "import sentencepiece as spm\n",
        "#spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000, user_defined_symbols=['foo', 'bar'], hard_vocab_limit = False)\n",
        "#spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000, user_defined_symbols=['foo', 'bar'])\n",
        "spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lya7khXGlHkg"
      },
      "source": [
        "sp = spm.SentencePieceProcessor(model_file='m.model')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHD2th7mlLzz",
        "outputId": "9e7bd502-97c6-4f1d-ae5a-9b46d998251f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.vocab_size()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9zi1Pyb5LAw",
        "outputId": "a67f35ea-2bbf-4b15-a854-b4c17a4c6cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(sp)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9cFBC-W5ujo",
        "outputId": "e8c5772a-152a-4bf5-ac10-bab4a46ed7cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp.id_to_piece([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<s>',\n",
              " '</s>',\n",
              " ',',\n",
              " '▁',\n",
              " '.',\n",
              " '▁the',\n",
              " 's',\n",
              " '▁I',\n",
              " '▁to',\n",
              " '▁a',\n",
              " 'ed',\n",
              " 'e',\n",
              " 't',\n",
              " '▁and',\n",
              " '▁of',\n",
              " 'ing',\n",
              " 'a']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y6NJ0Zv8aeK",
        "outputId": "db2d4e95-be71-4f55-b8ce-07eb63bf5a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=1000, user_defined_symbols=[' ', 'bar'])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-91396b1778b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test/botchan.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_defined_symbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Internal: /sentencepiece/python/bundled/sentencepiece/src/trainer_interface.cc(552) [!sp->piece().empty()] "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAwiUVAO2E77"
      },
      "source": [
        "# Training our own tokenisation with tensorflow text encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46R2gXL7vId"
      },
      "source": [
        "https://github.com/tensorflow/text/blob/master/docs/api_docs/python/text.md\n",
        "\n",
        "https://blog.tensorflow.org/2019/06/introducing-tftext.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt00SoZ2IYp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}