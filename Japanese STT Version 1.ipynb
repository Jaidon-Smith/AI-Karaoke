{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOlZ2ck2sI8OoodDNrMN15t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/Japanese%20STT%20Version%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvMVV72-thw"
      },
      "source": [
        "# Exploration of Tensorflow CTC Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCcNyRFYAGQb"
      },
      "source": [
        "# Sentence piece for grapheme based BPE https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4ir3AiAwZf"
      },
      "source": [
        "Upon reading the github docs, there appears to be some tensorflow integration if you search 'Sentencepiece'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTgKetySp8z"
      },
      "source": [
        "# Exploring pretained tokenisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eyNrmEY8bbs6"
      },
      "source": [
        "#@title Install dependencies\n",
        "!pip install --quiet tensorflow-text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_gEuvjnXjfMG"
      },
      "source": [
        "#@title Import dependencies"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NnAXoCtiSude"
      },
      "source": [
        "#@title Original hub code\n",
        "!pip install tensorflow-text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "n_layer = 12\n",
        "d_model = 768\n",
        "max_gen_len = 128\n",
        "\n",
        "def generate(module, inputs, mems):\n",
        "  \"\"\"Generate text.\"\"\"\n",
        "  inputs = tf.dtypes.cast(inputs, tf.int64)\n",
        "  generation_input_dict = dict(input_tokens=inputs)\n",
        "  mems_dict = {}\n",
        "  for i in range(n_layer):\n",
        "    mems_dict[\"mem_{}\".format(i)] = mems[i]\n",
        "  generation_input_dict.update(mems_dict)\n",
        "\n",
        "  generation_outputs = module(generation_input_dict, signature=\"prediction\",\n",
        "                              as_dict=True)\n",
        "  probs = generation_outputs[\"probs\"]\n",
        "\n",
        "  new_mems = []\n",
        "  for i in range(n_layer):\n",
        "    new_mems.append(generation_outputs[\"new_mem_{}\".format(i)])\n",
        "\n",
        "  return probs, new_mems\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  text = [\"\\n_START_ARTICLE_\\nしのぶ・まさみshow'05 恋してラララ\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\"]\n",
        "\n",
        "  # Word embeddings.\n",
        "  embeddings = module(dict(text=text), signature=\"word_embeddings\",\n",
        "                      as_dict=True)\n",
        "  embeddings = embeddings[\"word_embeddings\"]\n",
        "\n",
        "  # Activations at each layer.\n",
        "  activations = module(dict(text=text),signature=\"activations\", as_dict=True)\n",
        "  activations = activations[\"activations\"]\n",
        "\n",
        "  # Negative log likelihood of the text, and perplexity.\n",
        "  neg_log_likelihood = module(dict(text=text), signature=\"neg_log_likelihood\",\n",
        "                              as_dict=True)\n",
        "  neg_log_likelihood = neg_log_likelihood[\"neg_log_likelihood\"]\n",
        "  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))\n",
        "\n",
        "  # Tokenization and detokenization with the sentencepiece model.\n",
        "  token_ids = module(dict(text=text), signature=\"tokenization\", as_dict=True)\n",
        "  token_ids = token_ids[\"token_ids\"]\n",
        "\n",
        "  detoken_text = module(dict(token_ids=token_ids), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "  # Generation\n",
        "  mems_np = [np.zeros([1, 0, d_model], dtype=np.float32) for _ in range(n_layer)]\n",
        "  inputs_np = token_ids\n",
        "  sampled_ids = []\n",
        "  for step in range(max_gen_len):\n",
        "    probs, mems_np = generate(module, inputs_np, mems_np)\n",
        "    sampled_id = tf.random.categorical(tf.math.log(probs[0]), num_samples=1, dtype=tf.int32)\n",
        "    sampled_id = tf.squeeze(sampled_id)\n",
        "\n",
        "    sampled_ids.append(sampled_id)\n",
        "    inputs_np = tf.reshape(sampled_id, [1, 1])\n",
        "\n",
        "  sampled_ids = tf.expand_dims(sampled_ids, axis=0)\n",
        "  generated_text = module(dict(token_ids=sampled_ids),\n",
        "                          signature=\"detokenization\", as_dict=True)\n",
        "  generated_text = generated_text[\"text\"]\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text = session.run([\n",
        "    embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text, generated_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Itn-XLBvjn6S"
      },
      "source": [
        "#@title Original hub code without generation\n",
        "!pip install tensorflow-text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "n_layer = 12\n",
        "d_model = 768\n",
        "max_gen_len = 128\n",
        "\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  text = [\"\\n_START_ARTICLE_\\nしのぶ・まさみshow'05 恋してラララ\\n_START_SECTION_\\n概要\\n_START_PARAGRAPH_\\n『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\"]\n",
        "\n",
        "  # Word embeddings.\n",
        "  embeddings = module(dict(text=text), signature=\"word_embeddings\",\n",
        "                      as_dict=True)\n",
        "  embeddings = embeddings[\"word_embeddings\"]\n",
        "\n",
        "  # Activations at each layer.\n",
        "  activations = module(dict(text=text),signature=\"activations\", as_dict=True)\n",
        "  activations = activations[\"activations\"]\n",
        "\n",
        "  # Negative log likelihood of the text, and perplexity.\n",
        "  neg_log_likelihood = module(dict(text=text), signature=\"neg_log_likelihood\",\n",
        "                              as_dict=True)\n",
        "  neg_log_likelihood = neg_log_likelihood[\"neg_log_likelihood\"]\n",
        "  ppl = tf.exp(tf.reduce_mean(neg_log_likelihood, axis=1))\n",
        "\n",
        "  # Tokenization and detokenization with the sentencepiece model.\n",
        "  token_ids = module(dict(text=text), signature=\"tokenization\", as_dict=True)\n",
        "  token_ids = token_ids[\"token_ids\"]\n",
        "\n",
        "  detoken_text = module(dict(token_ids=token_ids), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text = session.run([\n",
        "    embeddings, neg_log_likelihood, ppl, activations, token_ids, detoken_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-skOhvXxkchi",
        "outputId": "de20679a-bf38-49dc-ed48-a6c4d1ef466b"
      },
      "source": [
        "token_ids"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   13,     3,    13,    32,     7,  1060,    12,  6708,   198,\n",
              "         4888,  6824,   577,  8469,    13,  1824,    65,   125, 12974,\n",
              "           13,     4,    13,    54,    13,     5,    13,    33,  3322,\n",
              "         9505, 20236,    35,     7, 16745,   219,  3174,  6761, 11421,\n",
              "           19,     8, 10110,  3549,     7,    53,  1202,    32,     7,\n",
              "         1060,    20,   546,    84,  1967,   315,    15,  4871, 13775,\n",
              "         2624,    19,  2702,    27,   296,   736, 14103,   175,     9,\n",
              "         2819,    10,   542]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VH65JJJtkgup",
        "outputId": "ab9fa654-604a-44b2-9f47-5d9004c6d42b"
      },
      "source": [
        "detoken_text[0].decode()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"_START_ARTICLE_ しのぶ・まさみshow'05 恋してラララ _START_SECTION_ 概要 _START_PARAGRAPH_ 『上海ルーキーSHOW』の打ち切り後に放送された年末特番で、同番組MCの大竹しのぶと久本雅美が恋愛にまつわるテーマでトークや音楽企画を展開していた。基本は女\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaD8O8jetED7",
        "outputId": "c7752f15-80af-4f1b-8763-3cca905d9cf2"
      },
      "source": [
        "token_ids.shape\n",
        "num_tokens = 10000\n",
        "token_explorer = list(range(num_tokens))\n",
        "for i in range(num_tokens - 1):\n",
        "  token_explorer.insert(num_tokens - 1 - i, 0)\n",
        "token_explorer = np.array([token_explorer])\n",
        "token_explorer"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    1, ..., 9998,    0, 9999]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6aAXqlt0Wa",
        "outputId": "4c9252f0-0092-42d9-b4d1-ac022c18992e"
      },
      "source": [
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  module = hub.Module(\"https://tfhub.dev/google/wiki40b-lm-ja/1\")\n",
        "  detoken_text = module(dict(token_ids=token_explorer), signature=\"detokenization\",\n",
        "                        as_dict=True)\n",
        "  detoken_text = detoken_text[\"text\"]\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(),\n",
        "                      tf.tables_initializer()])\n",
        "\n",
        "# Initialize session.\n",
        "with tf.Session(graph=g) as session:\n",
        "  session.run(init_op)\n",
        "  detoken_text = session.run([\n",
        "    detoken_text])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAPZs7ahvrm-",
        "outputId": "aa3f7306-a4d9-4fff-c6b0-9857579da0e3"
      },
      "source": [
        "detoken_text[0][0].decode().split('⁇')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ',\n",
              " '  ',\n",
              " '  ',\n",
              " '  ',\n",
              " ' _START_ARTICLE_ ',\n",
              " ' _START_SECTION_ ',\n",
              " ' _START_PARAGRAPH_ ',\n",
              " ' _NEWLINE_ ',\n",
              " ' の ',\n",
              " ' 、 ',\n",
              " ' 。 ',\n",
              " ' は ',\n",
              " ' 年 ',\n",
              " ' ・ ',\n",
              " '   ',\n",
              " ' ) ',\n",
              " ' が ',\n",
              " ' ( ',\n",
              " ' に ',\n",
              " ' を ',\n",
              " ' で ',\n",
              " ' と ',\n",
              " ' 月 ',\n",
              " ' 」 ',\n",
              " ' 「 ',\n",
              " ' 2 ',\n",
              " ' 1 ',\n",
              " ' から ',\n",
              " ' や ',\n",
              " ' 3 ',\n",
              " ' 日 ',\n",
              " ' である ',\n",
              " ' した ',\n",
              " ' し ',\n",
              " ' 『 ',\n",
              " ' も ',\n",
              " ' 』 ',\n",
              " ' として ',\n",
              " ' 4 ',\n",
              " ' する ',\n",
              " ' 年に ',\n",
              " ' では ',\n",
              " ' 5 ',\n",
              " ' た ',\n",
              " ' 6 ',\n",
              " ' には ',\n",
              " ' また ',\n",
              " ' 7 ',\n",
              " ' 10 ',\n",
              " ' など ',\n",
              " ' 第 ',\n",
              " ' 8 ',\n",
              " ' 9 ',\n",
              " ' この ',\n",
              " ' 大 ',\n",
              " ' 概要 ',\n",
              " ' る ',\n",
              " ' 12 ',\n",
              " ' という ',\n",
              " ' ス ',\n",
              " '  ( ',\n",
              " ' された ',\n",
              " ' その ',\n",
              " ' て ',\n",
              " ' - ',\n",
              " ' 日に ',\n",
              " ' して ',\n",
              " ' している ',\n",
              " ' 11 ',\n",
              " ' 人 ',\n",
              " ' となった ',\n",
              " ' な ',\n",
              " ' 市 ',\n",
              " '  - ',\n",
              " ' ている ',\n",
              " ' 日本 ',\n",
              " ' : ',\n",
              " ' 中 ',\n",
              " ' 山 ',\n",
              " ' 町 ',\n",
              " ' 一 ',\n",
              " ' により ',\n",
              " ' 回 ',\n",
              " ' であった ',\n",
              " ' による ',\n",
              " ' 本 ',\n",
              " ' 昭和 ',\n",
              " ' . ',\n",
              " ' され ',\n",
              " ' その後 ',\n",
              " ' となる ',\n",
              " ' によって ',\n",
              " ' 後 ',\n",
              " ' ア ',\n",
              " ' であり ',\n",
              " ' 子 ',\n",
              " ' , ',\n",
              " ' 月に ',\n",
              " ' 15 ',\n",
              " ' 長 ',\n",
              " ' より ',\n",
              " ' ト ',\n",
              " ' 17 ',\n",
              " ' 上 ',\n",
              " ' 川 ',\n",
              " ' 新 ',\n",
              " ' か ',\n",
              " ' 部 ',\n",
              " ' がある ',\n",
              " ' 同 ',\n",
              " ' ズ ',\n",
              " ' り ',\n",
              " ' 20 ',\n",
              " ' 元 ',\n",
              " ' ため ',\n",
              " ' ク ',\n",
              " ' 16 ',\n",
              " ' へ ',\n",
              " ' ド ',\n",
              " ' 名 ',\n",
              " ' 同年 ',\n",
              " ' まで ',\n",
              " ' m ',\n",
              " ' 年には ',\n",
              " ' 駅 ',\n",
              " ' 家 ',\n",
              " ' ラ ',\n",
              " ' 、「 ',\n",
              " ' 郡 ',\n",
              " ' しかし ',\n",
              " ' 国 ',\n",
              " ' 線 ',\n",
              " ' カ ',\n",
              " ' される ',\n",
              " ' 東 ',\n",
              " ' A ',\n",
              " ' 高 ',\n",
              " ' s ',\n",
              " ' 道 ',\n",
              " ' となり ',\n",
              " ' 18 ',\n",
              " ' 号 ',\n",
              " ' ナ ',\n",
              " ' い ',\n",
              " ' 性 ',\n",
              " ' 三 ',\n",
              " ' 大学 ',\n",
              " ' などの ',\n",
              " ' 14 ',\n",
              " ' 村 ',\n",
              " ' 南 ',\n",
              " ' 年から ',\n",
              " ' 30 ',\n",
              " ' B ',\n",
              " ' だった ',\n",
              " ' 法 ',\n",
              " ' への ',\n",
              " ' において ',\n",
              " ' 西 ',\n",
              " ' 下 ',\n",
              " ' 島 ',\n",
              " ' されている ',\n",
              " ' 田 ',\n",
              " ' 者 ',\n",
              " ' 小 ',\n",
              " ' リ ',\n",
              " ' 分 ',\n",
              " ' にも ',\n",
              " ' S ',\n",
              " ' 水 ',\n",
              " ' マ ',\n",
              " ' ら ',\n",
              " ' 東京 ',\n",
              " ' タ ',\n",
              " ' 13 ',\n",
              " ' 明治 ',\n",
              " ' ていた ',\n",
              " ' 会 ',\n",
              " ' していた ',\n",
              " ' ル ',\n",
              " ' 平成 ',\n",
              " ' コ ',\n",
              " ' 北 ',\n",
              " ' 人物 ',\n",
              " ' 内 ',\n",
              " ' 生 ',\n",
              " ' 歴史 ',\n",
              " ' / ',\n",
              " ' 位 ',\n",
              " ' 前 ',\n",
              " ' お ',\n",
              " ' C ',\n",
              " ' ノ ',\n",
              " ' なお ',\n",
              " ' にて ',\n",
              " ' 王 ',\n",
              " ' 的な ',\n",
              " ' 地 ',\n",
              " ' 作品 ',\n",
              " ' み ',\n",
              " ' 映画 ',\n",
              " ' でも ',\n",
              " ' 経歴 ',\n",
              " ' 代 ',\n",
              " ' エ ',\n",
              " ' となっている ',\n",
              " ' および ',\n",
              " ' にある ',\n",
              " ' 戦 ',\n",
              " ' 数 ',\n",
              " ' 19 ',\n",
              " ' 正 ',\n",
              " ' 城 ',\n",
              " ' 金 ',\n",
              " ' 約 ',\n",
              " ' 野 ',\n",
              " ' 等 ',\n",
              " ' 現 ',\n",
              " ' とは ',\n",
              " ' サ ',\n",
              " ' 後に ',\n",
              " ' 』( ',\n",
              " ' 州 ',\n",
              " ' = ',\n",
              " ' レ ',\n",
              " ' 寺 ',\n",
              " ' き ',\n",
              " ' ダ ',\n",
              " ' 的 ',\n",
              " ' オ ',\n",
              " ' M ',\n",
              " ' った ',\n",
              " ' 神 ',\n",
              " ' 木 ',\n",
              " ' バ ',\n",
              " ' イ ',\n",
              " ' く ',\n",
              " ' 時 ',\n",
              " ' つ ',\n",
              " ' 県 ',\n",
              " ' 25 ',\n",
              " ' 化 ',\n",
              " ' デ ',\n",
              " ' 全 ',\n",
              " ' 放送 ',\n",
              " ' 二 ',\n",
              " ' フ ',\n",
              " ' F ',\n",
              " ' 文 ',\n",
              " ' 光 ',\n",
              " ' ム ',\n",
              " ' ロ ',\n",
              " ' 型 ',\n",
              " ' 手 ',\n",
              " ' 来歴 ',\n",
              " ' 平 ',\n",
              " ' 勝 ',\n",
              " ' R ',\n",
              " ' 学 ',\n",
              " ' シ ',\n",
              " ' D ',\n",
              " ' における ',\n",
              " ' 現在の ',\n",
              " ' 24 ',\n",
              " ' 21 ',\n",
              " ' 旧 ',\n",
              " ' マン ',\n",
              " ' 区 ',\n",
              " ' 石 ',\n",
              " ' 日本の ',\n",
              " ' ており ',\n",
              " ' 原 ',\n",
              " ' ジ ',\n",
              " ' 社 ',\n",
              " ' 海 ',\n",
              " ' ザ ',\n",
              " ' G ',\n",
              " ' こと ',\n",
              " ' \" ',\n",
              " ' 曲 ',\n",
              " ' 軍 ',\n",
              " ' があり ',\n",
              " ' 物 ',\n",
              " ' 作 ',\n",
              " ' 22 ',\n",
              " ' 世界 ',\n",
              " ' 行 ',\n",
              " ' 形 ',\n",
              " ' T ',\n",
              " ' キ ',\n",
              " ' リー ',\n",
              " ' 公 ',\n",
              " ' アルバム ',\n",
              " ' 面 ',\n",
              " ' ガ ',\n",
              " ' とともに ',\n",
              " ' 外 ',\n",
              " ' 音楽 ',\n",
              " ' さらに ',\n",
              " ' チ ',\n",
              " ' 23 ',\n",
              " ' 語 ',\n",
              " ' 立 ',\n",
              " ' P ',\n",
              " ' 和 ',\n",
              " ' 所 ',\n",
              " ' 賞 ',\n",
              " ' ティ ',\n",
              " ' さ ',\n",
              " ' 代表 ',\n",
              " ' 見 ',\n",
              " ' 力 ',\n",
              " ' 橋 ',\n",
              " ' 番組 ',\n",
              " ' を経て ',\n",
              " ' ミ ',\n",
              " ' 美 ',\n",
              " ' シリーズ ',\n",
              " ' ディ ',\n",
              " ' 28 ',\n",
              " ' 花 ',\n",
              " ' 式 ',\n",
              " ' 活動 ',\n",
              " ' J ',\n",
              " ' 地域 ',\n",
              " ' 局 ',\n",
              " ' す ',\n",
              " ' う ',\n",
              " ' 当時 ',\n",
              " ' ニ ',\n",
              " ' W ',\n",
              " ' 国際 ',\n",
              " ' 用 ',\n",
              " ' 系 ',\n",
              " ' 世 ',\n",
              " ' ネ ',\n",
              " ' 間 ',\n",
              " ' 26 ',\n",
              " ' 27 ',\n",
              " ' ゲーム ',\n",
              " ' 鉄道 ',\n",
              " ' こ ',\n",
              " ' 義 ',\n",
              " ' ツ ',\n",
              " ' 明 ',\n",
              " ' 度 ',\n",
              " ' 重 ',\n",
              " ' 及び ',\n",
              " ' ブ ',\n",
              " ' 研究 ',\n",
              " ' 学校 ',\n",
              " ' 出 ',\n",
              " ' 29 ',\n",
              " ' プ ',\n",
              " ' 車 ',\n",
              " ' K ',\n",
              " ' と共に ',\n",
              " ' 」( ',\n",
              " ' 天 ',\n",
              " ' ン ',\n",
              " ' 各 ',\n",
              " ' パ ',\n",
              " ' だ ',\n",
              " ' 馬 ',\n",
              " ' 教育 ',\n",
              " ' 氏 ',\n",
              " ' 2012 ',\n",
              " ' N ',\n",
              " ' 事業 ',\n",
              " ' グ ',\n",
              " ' 体 ',\n",
              " ' プロ ',\n",
              " ' 2009 ',\n",
              " ' イン ',\n",
              " ' チーム ',\n",
              " ' 信 ',\n",
              " ' ビ ',\n",
              " ' 2010 ',\n",
              " ' ない ',\n",
              " ' って ',\n",
              " ' について ',\n",
              " ' いた ',\n",
              " ' H ',\n",
              " ' セ ',\n",
              " ' 着 ',\n",
              " ' 級 ',\n",
              " ' ケ ',\n",
              " ' 2007 ',\n",
              " ' 安 ',\n",
              " ' 屋 ',\n",
              " ' 台 ',\n",
              " ' 2014 ',\n",
              " ' 目 ',\n",
              " ' L ',\n",
              " ' 版 ',\n",
              " ' 書 ',\n",
              " ' ウ ',\n",
              " ' のみ ',\n",
              " ' 頭 ',\n",
              " ' 万 ',\n",
              " ' 2015 ',\n",
              " ' ハ ',\n",
              " ' 、『 ',\n",
              " ' 年間 ',\n",
              " ' 2013 ',\n",
              " ' 宮 ',\n",
              " ' 2008 ',\n",
              " ' 風 ',\n",
              " ' 谷 ',\n",
              " ' V ',\n",
              " ' メ ',\n",
              " ' 期 ',\n",
              " ' ! ',\n",
              " ' 2011 ',\n",
              " ' 方 ',\n",
              " ' 英 ',\n",
              " ' そして ',\n",
              " ' 出身 ',\n",
              " ' 話 ',\n",
              " ' 機 ',\n",
              " ' 事 ',\n",
              " ' km ',\n",
              " ' シュ ',\n",
              " ' 開発 ',\n",
              " ' 流 ',\n",
              " ' 時間 ',\n",
              " ' 初 ',\n",
              " ' になった ',\n",
              " ' リーグ ',\n",
              " ' E ',\n",
              " ' 直 ',\n",
              " ' 年まで ',\n",
              " ' とも ',\n",
              " ' 通 ',\n",
              " ' 中国 ',\n",
              " ' アメリカ ',\n",
              " ' テ ',\n",
              " ' ある ',\n",
              " ' 船 ',\n",
              " ' 点 ',\n",
              " ' 場 ',\n",
              " ' 文化 ',\n",
              " ' だが ',\n",
              " ' 特に ',\n",
              " ' これは ',\n",
              " ' 2017 ',\n",
              " ' クラブ ',\n",
              " ' FC ',\n",
              " ' になる ',\n",
              " ' 院 ',\n",
              " ' アル ',\n",
              " ' 白 ',\n",
              " ' モ ',\n",
              " ' 成 ',\n",
              " ' 40 ',\n",
              " ' 科 ',\n",
              " ' 星 ',\n",
              " ' 100 ',\n",
              " ' 多くの ',\n",
              " ' ～ ',\n",
              " ' 」「 ',\n",
              " ' 2016 ',\n",
              " ' 無 ',\n",
              " ' すると ',\n",
              " ' 選手 ',\n",
              " ' とする ',\n",
              " ' 翌 ',\n",
              " ' バー ',\n",
              " ' 地区 ',\n",
              " ' を持つ ',\n",
              " ' 種 ',\n",
              " ' 史 ',\n",
              " ' ま ',\n",
              " ' 両 ',\n",
              " ' 大会 ',\n",
              " ' 50 ',\n",
              " ' 館 ',\n",
              " ' 真 ',\n",
              " ' 事件 ',\n",
              " ' 口 ',\n",
              " ' 監督 ',\n",
              " ' シングル ',\n",
              " ' 高校 ',\n",
              " ' 清 ',\n",
              " ' 政 ',\n",
              " ' 2006 ',\n",
              " ' グループ ',\n",
              " ' 31 ',\n",
              " ' ゴ ',\n",
              " ' 株式会社 ',\n",
              " ' 次 ',\n",
              " ' 不 ',\n",
              " ' t ',\n",
              " ' フランス ',\n",
              " ' 歳 ',\n",
              " ' アン ',\n",
              " ' 吉 ',\n",
              " ' 略歴 ',\n",
              " ' 2018 ',\n",
              " ' カー ',\n",
              " ' 人の ',\n",
              " ' 現在は ',\n",
              " ' 官 ',\n",
              " ' 組 ',\n",
              " ' バス ',\n",
              " ' テレビ ',\n",
              " ' 実 ',\n",
              " ' 井 ',\n",
              " ' できる ',\n",
              " ' 情報 ',\n",
              " ' ボ ',\n",
              " ' 地方 ',\n",
              " ' トン ',\n",
              " ' U ',\n",
              " ' y ',\n",
              " ' サン ',\n",
              " ' 小学校 ',\n",
              " ' 中学校 ',\n",
              " ' つの ',\n",
              " ' 門 ',\n",
              " ' ヤ ',\n",
              " ' 主 ',\n",
              " ' 男 ',\n",
              " ' 番 ',\n",
              " ' 時代 ',\n",
              " ' 江 ',\n",
              " ' 会社 ',\n",
              " ' 問題 ',\n",
              " ' 役 ',\n",
              " ' 大正 ',\n",
              " ' 中央 ',\n",
              " ' 尾 ',\n",
              " ' 都市 ',\n",
              " ' 室 ',\n",
              " ' しており ',\n",
              " ' 店 ',\n",
              " ' 施設 ',\n",
              " ' 計画 ',\n",
              " ' のため ',\n",
              " ' 制 ',\n",
              " ' 雄 ',\n",
              " ' 気 ',\n",
              " ' 試合 ',\n",
              " ' 女 ',\n",
              " ' 2000 ',\n",
              " ' 心 ',\n",
              " ' I ',\n",
              " ' 久 ',\n",
              " ' と呼ばれる ',\n",
              " ' 大阪 ',\n",
              " ' ドイツ ',\n",
              " ' め ',\n",
              " ' 協会 ',\n",
              " ' 朝 ',\n",
              " ' 宗 ',\n",
              " ' ヒ ',\n",
              " ' 教授 ',\n",
              " ' en ',\n",
              " ' を務めた ',\n",
              " ' 父 ',\n",
              " ' ラン ',\n",
              " ' それ ',\n",
              " ' センター ',\n",
              " ' ー ',\n",
              " ' 論 ',\n",
              " ' 春 ',\n",
              " ' 女子 ',\n",
              " ' X ',\n",
              " ' を行う ',\n",
              " ' ながら ',\n",
              " ' 巻 ',\n",
              " ' 生まれ ',\n",
              " ' 座 ',\n",
              " ' 〜 ',\n",
              " ' 関係 ',\n",
              " ' n ',\n",
              " ' システム ',\n",
              " ' 別 ',\n",
              " \" ' \",\n",
              " ' 色 ',\n",
              " ' シー ',\n",
              " ' 高等学校 ',\n",
              " ' 当時の ',\n",
              " ' 武 ',\n",
              " ' としては ',\n",
              " ' 他の ',\n",
              " ' ラー ',\n",
              " ' 2005 ',\n",
              " ' に関する ',\n",
              " ' 治 ',\n",
              " ' 愛 ',\n",
              " ' バンド ',\n",
              " ' じ ',\n",
              " ' 道路 ',\n",
              " ' 的に ',\n",
              " ' 歌 ',\n",
              " ' ことから ',\n",
              " ' 主に ',\n",
              " ' CD ',\n",
              " ' i ',\n",
              " ' に対して ',\n",
              " ' とされる ',\n",
              " ' 松 ',\n",
              " ' 林 ',\n",
              " ' 女性 ',\n",
              " ' 現在 ',\n",
              " ' ソ ',\n",
              " ' ほか ',\n",
              " ' シーズン ',\n",
              " ' 総 ',\n",
              " ' 社会 ',\n",
              " ' 音 ',\n",
              " ' モデル ',\n",
              " ' デビュー ',\n",
              " ' 領 ',\n",
              " ' ポ ',\n",
              " ' 多 ',\n",
              " ' コン ',\n",
              " ' ち ',\n",
              " ' 派 ',\n",
              " ' 経済 ',\n",
              " ' 定 ',\n",
              " ' 古 ',\n",
              " ' 御 ',\n",
              " ' O ',\n",
              " ' 他 ',\n",
              " ' 校 ',\n",
              " ' 神社 ',\n",
              " ' 権 ',\n",
              " ' 夫 ',\n",
              " ' されていた ',\n",
              " ' 日から ',\n",
              " ' ず ',\n",
              " ' 2004 ',\n",
              " ' 太 ',\n",
              " ' 丸 ',\n",
              " ' または ',\n",
              " ' 之 ',\n",
              " ' 月には ',\n",
              " ' 北海道 ',\n",
              " ' 円 ',\n",
              " ' 全国 ',\n",
              " ' 地理 ',\n",
              " ' 組織 ',\n",
              " ' ただし ',\n",
              " ' といった ',\n",
              " ' イギリス ',\n",
              " ' 当 ',\n",
              " ' 彼は ',\n",
              " ' があった ',\n",
              " ' 業 ',\n",
              " ' リン ',\n",
              " ' 京都 ',\n",
              " '  of ',\n",
              " ' スト ',\n",
              " ' 忠 ',\n",
              " ' 通り ',\n",
              " ' 構造 ',\n",
              " ' 生涯 ',\n",
              " ' 沢 ',\n",
              " ' 五 ',\n",
              " ' 入 ',\n",
              " ' む ',\n",
              " ' 管理 ',\n",
              " ' er ',\n",
              " ' 先 ',\n",
              " ' 技術 ',\n",
              " ' 英語 ',\n",
              " ' スポーツ ',\n",
              " ' 黒 ',\n",
              " ' cm ',\n",
              " ' 四 ',\n",
              " ' ホーム ',\n",
              " ' 兵 ',\n",
              " ' スター ',\n",
              " ' え ',\n",
              " ' ワ ',\n",
              " ' 徳 ',\n",
              " ' 岩 ',\n",
              " ' 後の ',\n",
              " ' 60 ',\n",
              " ' ソン ',\n",
              " ' 有 ',\n",
              " ' ニー ',\n",
              " ' 企業 ',\n",
              " ' ター ',\n",
              " ' を受け ',\n",
              " ' 制作 ',\n",
              " ' 条 ',\n",
              " ' 国道 ',\n",
              " '  S ',\n",
              " ' 生活 ',\n",
              " ' ために ',\n",
              " ' 士 ',\n",
              " ' 群 ',\n",
              " ' 津 ',\n",
              " ' マー ',\n",
              " ' e ',\n",
              " ' 字 ',\n",
              " ' Y ',\n",
              " ' ロー ',\n",
              " ' ベ ',\n",
              " ' 時に ',\n",
              " ' エル ',\n",
              " ' a ',\n",
              " ' 再び ',\n",
              " ' ピ ',\n",
              " ' 記録 ',\n",
              " ' 建設 ',\n",
              " ' については ',\n",
              " ' 八 ',\n",
              " ' 環境 ',\n",
              " ' もある ',\n",
              " ' 食 ',\n",
              " ' 特徴 ',\n",
              " ' 藩 ',\n",
              " ' っている ',\n",
              " ' 里 ',\n",
              " ' られた ',\n",
              " ' ものの ',\n",
              " ' & ',\n",
              " ' 発 ',\n",
              " ' あ ',\n",
              " ' 2003 ',\n",
              " ' これ ',\n",
              " ' 職 ',\n",
              " ' 広 ',\n",
              " ' 機能 ',\n",
              " ' したが ',\n",
              " ' 戦争 ',\n",
              " ' ドラマ ',\n",
              " ' を行った ',\n",
              " ' 販売 ',\n",
              " ' ように ',\n",
              " ' カル ',\n",
              " ' 年より ',\n",
              " ' ほど ',\n",
              " ' 企画 ',\n",
              " ' 守 ',\n",
              " ' に対する ',\n",
              " ' 堂 ',\n",
              " ' 月から ',\n",
              " ' 出場 ',\n",
              " ' ジャ ',\n",
              " ' 品 ',\n",
              " ' ヴィ ',\n",
              " ' アー ',\n",
              " ' 公園 ',\n",
              " ' サービス ',\n",
              " ' 団体 ',\n",
              " ' 戸 ',\n",
              " ' 森 ',\n",
              " ' 優勝 ',\n",
              " ' 例 ',\n",
              " ' レース ',\n",
              " ' 隊 ',\n",
              " ' 総合 ',\n",
              " ' ギ ',\n",
              " ' ファ ',\n",
              " ' ファン ',\n",
              " ' 永 ',\n",
              " ' 葉 ',\n",
              " ' 対 ',\n",
              " ' であったが ',\n",
              " ' h ',\n",
              " ' 々 ',\n",
              " ' 港 ',\n",
              " ' ビル ',\n",
              " ' 千 ',\n",
              " ' 母 ',\n",
              " ' )」 ',\n",
              " ' ヴァ ',\n",
              " ' 民 ',\n",
              " ' 類 ',\n",
              " ' 以降 ',\n",
              " ' 良 ',\n",
              " ' ことを ',\n",
              " ' 運動 ',\n",
              " ' 2002 ',\n",
              " ' 調査 ',\n",
              " ' 基 ',\n",
              " ' 利 ',\n",
              " ' 初の ',\n",
              " ' へと ',\n",
              " ' 発売 ',\n",
              " ' 様々な ',\n",
              " ' ナー ',\n",
              " ' いる ',\n",
              " ' 中に ',\n",
              " ' 当初は ',\n",
              " ' 2019 ',\n",
              " ' ではなく ',\n",
              " ' ジョン ',\n",
              " ' 土 ',\n",
              " ' 空 ',\n",
              " ' 路 ',\n",
              " ' 聖 ',\n",
              " ' を中心に ',\n",
              " ' とした ',\n",
              " ' 路線 ',\n",
              " ' よ ',\n",
              " ' 保 ',\n",
              " ' 日には ',\n",
              " ' 同じ ',\n",
              " ' イベント ',\n",
              " ' 夏 ',\n",
              " ' 出演 ',\n",
              " ' 足 ',\n",
              " ' 相 ',\n",
              " ' 米 ',\n",
              " ' 太郎 ',\n",
              " ' ぎ ',\n",
              " ' ラジオ ',\n",
              " ' 戦で ',\n",
              " ' 2001 ',\n",
              " ' 量 ',\n",
              " ' のうち ',\n",
              " ' ストーリー ',\n",
              " ' 都 ',\n",
              " ' 交通 ',\n",
              " ' 海軍 ',\n",
              " ' 夜 ',\n",
              " ' 身 ',\n",
              " ' カン ',\n",
              " ' 合 ',\n",
              " ' を含む ',\n",
              " ' のために ',\n",
              " ' 経 ',\n",
              " ' レン ',\n",
              " ' 師 ',\n",
              " ' 志 ',\n",
              " ' 産業 ',\n",
              " ' 波 ',\n",
              " ' mm ',\n",
              " ' 誌 ',\n",
              " ' 兼 ',\n",
              " ' 親 ',\n",
              " ' 連 ',\n",
              " ' に伴い ',\n",
              " ' 第一 ',\n",
              " ' 集 ',\n",
              " ' 草 ',\n",
              " ' 雑誌 ',\n",
              " ' わ ',\n",
              " ' 秒 ',\n",
              " ' 死 ',\n",
              " ' タイ ',\n",
              " ' 崎 ',\n",
              " ' 記 ',\n",
              " ' 開催 ',\n",
              " ' ” ',\n",
              " ' d ',\n",
              " ' 卒業 ',\n",
              " ' 党 ',\n",
              " ' 左 ',\n",
              " ' 知 ',\n",
              " ' させた ',\n",
              " ' 命 ',\n",
              " ' になり ',\n",
              " ' 大きな ',\n",
              " ' 委員会 ',\n",
              " ' 角 ',\n",
              " ' になっている ',\n",
              " ' ペ ',\n",
              " ' 根 ',\n",
              " ' ベル ',\n",
              " ' 80 ',\n",
              " ' ホ ',\n",
              " ' al ',\n",
              " ' 達 ',\n",
              " ' 赤 ',\n",
              " ' 特別 ',\n",
              " ' 豊 ',\n",
              " ' エン ',\n",
              " ' 彼の ',\n",
              " ' ダー ',\n",
              " ' 団 ',\n",
              " ' ば ',\n",
              " ' 六 ',\n",
              " ' 銀行 ',\n",
              " ' ロシア ',\n",
              " ' 解説 ',\n",
              " ' 自 ',\n",
              " ' ジュ ',\n",
              " ' 所属 ',\n",
              " ' シャ ',\n",
              " ' ヨーロッパ ',\n",
              " ' のような ',\n",
              " ' 物語 ',\n",
              " ' デザイン ',\n",
              " ' レイ ',\n",
              " ' メンバー ',\n",
              " ' 側 ',\n",
              " ' 状態 ',\n",
              " ' イタリア ',\n",
              " ' を受けた ',\n",
              " ' 車両 ',\n",
              " ' 場所 ',\n",
              " '  B ',\n",
              " ' などで ',\n",
              " ' 今 ',\n",
              " ' 半 ',\n",
              " ' アニメ ',\n",
              " ' 副 ',\n",
              " ' ヌ ',\n",
              " ' 利用 ',\n",
              " ' 生産 ',\n",
              " '  A ',\n",
              " ' NHK ',\n",
              " ' 最初の ',\n",
              " ' 彦 ',\n",
              " ' st ',\n",
              " ' 司 ',\n",
              " ' 政治 ',\n",
              " ' 員 ',\n",
              " ' リア ',\n",
              " ' 経営 ',\n",
              " ' 一方 ',\n",
              " ' ライブ ',\n",
              " ' んだ ',\n",
              " ' 湖 ',\n",
              " ' オー ',\n",
              " ' ことが ',\n",
              " ' 担当 ',\n",
              " ' 工場 ',\n",
              " ' ゲ ',\n",
              " ' されており ',\n",
              " ' 横 ',\n",
              " ' 評価 ',\n",
              " ' 郷 ',\n",
              " ' ことで ',\n",
              " ' 年度 ',\n",
              " ' 以後 ',\n",
              " ' 以上 ',\n",
              " ' させる ',\n",
              " ' リング ',\n",
              " ' そのため ',\n",
              " ' 研究所 ',\n",
              " ' 初代 ',\n",
              " ' 人間 ',\n",
              " ' 契約 ',\n",
              " ' を務める ',\n",
              " ' 部門 ',\n",
              " ' スーパー ',\n",
              " ' 人が ',\n",
              " ' 弾 ',\n",
              " ' さん ',\n",
              " ' 坂 ',\n",
              " ' 器 ',\n",
              " ' 元年 ',\n",
              " ' 進 ',\n",
              " ' にあった ',\n",
              " ' 秀 ',\n",
              " ' 教会 ',\n",
              " ' シン ',\n",
              " ' 病 ',\n",
              " ' in ',\n",
              " ' タイトル ',\n",
              " ' r ',\n",
              " ' 0 ',\n",
              " ' こう ',\n",
              " ' よりも ',\n",
              " ' b ',\n",
              " ' 右 ',\n",
              " ' 岡 ',\n",
              " ' 教 ',\n",
              " ' 省 ',\n",
              " ' 等の ',\n",
              " ' パリ ',\n",
              " ' 再 ',\n",
              " ' 七 ',\n",
              " ' 国家 ',\n",
              " ' c ',\n",
              " ' o ',\n",
              " ' ッ ',\n",
              " ' 電 ',\n",
              " ' れ ',\n",
              " ' 35 ',\n",
              " ' バン ',\n",
              " ' ランド ',\n",
              " ' よう ',\n",
              " ' ing ',\n",
              " ' 公演 ',\n",
              " ' 章 ',\n",
              " ' ものである ',\n",
              " ' 45 ',\n",
              " ' Z ',\n",
              " ' エンジン ',\n",
              " ' 薬 ',\n",
              " ' 伝 ',\n",
              " ' を行い ',\n",
              " ' サー ',\n",
              " ' されて ',\n",
              " ' 内容 ',\n",
              " ' 学園 ',\n",
              " ' 歳で ',\n",
              " ' 質 ',\n",
              " ' 70 ',\n",
              " ' それぞれ ',\n",
              " ' 製 ',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASYWPVCPzQZI"
      },
      "source": [
        "# Training our own tokenisation with google SentencePiece\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbuvEZLLzUVK"
      },
      "source": [
        "By reading the paper for Wiki-40b (https://storage.googleapis.com/pub-tools-public-publication-data/pdf/18cd66cc7d31ce4c724cef1d2755b417f74de27c.pdf), it is clear that they do not use anything extra except the statistics based SentencePiece for tokenisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMBvznr_9WQ4"
      },
      "source": [
        "https://github.com/google/sentencepiece\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "https://github.com/google/sentencepiece/tree/master/python\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWU2xb3MzqSq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAwiUVAO2E77"
      },
      "source": [
        "# Training our own tokenisation with tensorflow text encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46R2gXL7vId"
      },
      "source": [
        "https://github.com/tensorflow/text/blob/master/docs/api_docs/python/text.md\n",
        "\n",
        "https://blog.tensorflow.org/2019/06/introducing-tftext.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgt00SoZ2IYp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}