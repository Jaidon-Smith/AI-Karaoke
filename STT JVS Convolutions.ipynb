{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/STT%20JVS%20Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY28ZxsZiUPu"
      },
      "source": [
        "COLAB_ENVIRONMENT = True\n",
        "CLEAR_LOGS = True\n",
        "CLEAR_CHECKPOINTS = True"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gd6aJXBjCqA"
      },
      "source": [
        "if COLAB_ENVIRONMENT:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnmItTWZe0ay",
        "outputId": "942df983-53c2-4be5-d65c-ed4270e2cd07"
      },
      "source": [
        "if COLAB_ENVIRONMENT:\n",
        "  !pip install tfds-nightly\n",
        "  !pip install pydub\n",
        "  !pip install tensorflow-io\n",
        "else:\n",
        "  !pip install --user tfds-nightly\n",
        "  !pip install --user pydub\n",
        "  !pip install --user tensorflow-io"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.2.0.dev202103160106)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (20.3.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.1.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.28.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.10.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->tfds-nightly) (54.0.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: tensorflow<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.3.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2.4.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2.10.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.36.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (54.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.4.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.27.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.4.0->tensorflow-io) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaxFPa1ne2Xm"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLtjhXMepL-",
        "outputId": "9d5fd675-4e5e-4d77-c1c9-f66d718fdda3"
      },
      "source": [
        "!git clone https://github.com/Jaidon-Smith/public_datasets.git"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'public_datasets' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNJwUAvCiUPr"
      },
      "source": [
        "BATCH_SIZE = 8"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8OZoOWZCXvm"
      },
      "source": [
        "# Setting Up TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmSdYojCgDz"
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP5tugbE-vbI"
      },
      "source": [
        "logdir = 'gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/logs'\n",
        "checkpoint_path = 'gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/checkpoints'"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZrsymk7E5DH",
        "outputId": "8979a53a-dd11-49be-9790-21962eed9f3b"
      },
      "source": [
        "# Initialise Logs\n",
        "if CLEAR_LOGS:\n",
        "  !gsutil rm -r $logdir\n",
        "if CLEAR_CHECKPOINTS:\n",
        "    !gsutil rm -r $checkpoint_path"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/logs/#1615968844665942...\n",
            "Removing gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/logs/events.out.tfevents.1615968844.5ba3808dffa3.58.1203.v2#1615968845892151...\n",
            "/ [2 objects]                                                                   \n",
            "Operation completed over 2 objects.                                              \n",
            "CommandException: No URLs matched: gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VimJg7KRCb3G"
      },
      "source": [
        "# Creates a file writer for the log directory.\n",
        "file_writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkMA6i1TVy1S"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqg6ZtdIiUPx",
        "outputId": "f1c6943d-b2d5-46a1-828c-941d33c6f702"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq' -O hiragana_jsut.model"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-17 08:16:36--  https://docs.google.com/uc?export=download&id=1treUWLprQkcGOXeiFPofOvtk4iPZhsJq\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.143.102, 74.125.143.113, 74.125.143.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.143.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mnsmrjumca6ntsrph47n9pr3erkqhgcl/1615968975000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-03-17 08:16:37--  https://doc-0g-28-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mnsmrjumca6ntsrph47n9pr3erkqhgcl/1615968975000/04186398190322129029/*/1treUWLprQkcGOXeiFPofOvtk4iPZhsJq?e=download\n",
            "Resolving doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-0g-28-docs.googleusercontent.com (doc-0g-28-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 276245 (270K) [application/octet-stream]\n",
            "Saving to: ‘hiragana_jsut.model’\n",
            "\n",
            "hiragana_jsut.model 100%[===================>] 269.77K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-03-17 08:16:37 (140 MB/s) - ‘hiragana_jsut.model’ saved [276245/276245]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBFpGwTo9kUs",
        "outputId": "742a8e24-4734-414a-84b7-1e3b5f152fe6"
      },
      "source": [
        "!pip install --quiet tensorflow-text\n",
        "\n",
        "import tensorflow_text as text\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "model_file = 'hiragana_jsut.model'\n",
        "model = gfile.GFile(model_file, 'rb').read()\n",
        "\n",
        "tokenizer = text.SentencepieceTokenizer(model=model)\n",
        "\n",
        "input_vocab_size = tokenizer.vocab_size().numpy()\n",
        "print(input_vocab_size)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyeaq-FTCB7X",
        "outputId": "ce3aa464-8d49-4e90-f798-6efdeaa5235d"
      },
      "source": [
        "if COLAB_ENVIRONMENT:\n",
        "  !pip install pykakasi\n",
        "else:\n",
        "  !pip install --user pykakasi"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pykakasi in /usr/local/lib/python3.7/dist-packages (2.0.6)\n",
            "Requirement already satisfied: klepto in /usr/local/lib/python3.7/dist-packages (from pykakasi) (0.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from pykakasi) (3.7.2)\n",
            "Requirement already satisfied: dill>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from klepto->pykakasi) (0.3.3)\n",
            "Requirement already satisfied: pox>=0.2.9 in /usr/local/lib/python3.7/dist-packages (from klepto->pykakasi) (0.2.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->pykakasi) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->pykakasi) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWcfthvAEbZI"
      },
      "source": [
        "import pykakasi\n",
        "converter = pykakasi.kakasi()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "154oNA4E6hEg"
      },
      "source": [
        "def preprocess_text(old_text):\n",
        "  text = old_text\n",
        "  text = tokenizer.tokenize(text)\n",
        "  text = tf.pad(text, paddings=[[0 , 200 - tf.shape(text)[0]]])\n",
        "  return text"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiqQD4F8j8YJ"
      },
      "source": [
        "def preprocess_text_no_pad(old_text):\n",
        "  text = old_text\n",
        "  text = tokenizer.tokenize(text)\n",
        "  text = tf.shape(text)[0]\n",
        "  return text"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxIOOV5_pS58"
      },
      "source": [
        "def preprocess_audio(old_audio, original_sample_rate):\n",
        "  audio = old_audio/tf.int16.max\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
        "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
        "  audio = tf.pad(audio, paddings=[[0 , 500 - tf.shape(audio)[0]], [0,0]])\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  return audio"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCRZuRVb4H_d"
      },
      "source": [
        "def preprocess_audio_no_pad(old_audio, original_sample_rate):\n",
        "  audio = old_audio/tf.int16.max\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  audio = tfio.audio.resample(audio, original_sample_rate, 24000)\n",
        "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
        "  audio = tf.shape(audio)[0]\n",
        "  return audio"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSaiGkuUZYFL"
      },
      "source": [
        "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
        "\n",
        "jsut_dataset, info = tfds.load(\n",
        "                    \"jsut\",\n",
        "                    split=\"train\",\n",
        "                    data_dir='gs://general-304503/public_datasets',\n",
        "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
        "                    with_info = True)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4dWKtfoEyKy"
      },
      "source": [
        "def convert_to_kana(text):\n",
        "  #kana = converter.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')\n",
        "  new_text = text.numpy().decode('utf-8')\n",
        "  kana = converter.convert(new_text)\n",
        "  return ''.join([i['hira'] for i in kana])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsITXisWE81g"
      },
      "source": [
        "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": x[\"speech\"],\n",
        "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TEZzObuMtUW"
      },
      "source": [
        "jsut_dataset = jsut_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
        "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
        "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
        "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htZ7O_L6ZPMm"
      },
      "source": [
        "jsut_dataset = jsut_dataset.batch(BATCH_SIZE)\n",
        "jsut_dataset = jsut_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lm95V3vntL6"
      },
      "source": [
        "## jvs dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1GQRngMnrUx"
      },
      "source": [
        "download_config = tfds.download.DownloadConfig(manual_dir='gs://general-304503/public_datasets/downloads/manual')\n",
        "\n",
        "jvs_dataset, info = tfds.load(\n",
        "                    \"jvs\",\n",
        "                    split=\"train\",\n",
        "                    data_dir='gs://general-304503/public_datasets',\n",
        "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
        "                    with_info = True)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc2xw1UvnrUx"
      },
      "source": [
        "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": x[\"speech\"],\n",
        "                                 \"text\": tf.py_function(convert_to_kana, [x[\"text\"]], Tout=tf.string)})"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOMQs6x_nrUx"
      },
      "source": [
        "jvs_dataset = jvs_dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": preprocess_audio(x[\"speech\"], 48000),\n",
        "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"], 48000),\n",
        "                                 \"text\": tf.py_function(preprocess_text, [x[\"text\"]], Tout=tf.int32),\n",
        "                                 \"text_lengths\": tf.py_function(preprocess_text_no_pad, [x[\"text\"]], Tout=tf.int32)})"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv1ibtCpnrUy"
      },
      "source": [
        "jvs_dataset = jvs_dataset.batch(BATCH_SIZE)\n",
        "jvs_dataset = jvs_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5WxMKAI-CF3"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "ds = jvs_dataset.take(1)\n",
        "\n",
        "for i in ds:\n",
        "  data = i\n",
        "  break\n",
        "\n",
        "speech = data['speech']\n",
        "text = data['text']\n",
        "id = data['id']\n",
        "speech_lengths = data['speech_lengths']\n",
        "text_lengths = data['text_lengths']\n",
        "\n",
        "tokenizer.detokenize(text.numpy()[1]).numpy().decode('utf-8')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKBOlF6Jsvk",
        "outputId": "4f1d93aa-7bf7-4619-b080-d5b386fdf16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "ds = jvs_dataset.take(1)\n",
        " \n",
        "for i in ds:\n",
        "  data = i\n",
        "  break\n",
        " \n",
        "speech = data['speech']\n",
        "text = data['text']\n",
        "id = data['id']\n",
        "speech_lengths = data['speech_lengths']\n",
        "text_lengths = data['text_lengths']\n",
        " \n",
        "tokenizer.detokenize(text.numpy()[1]).numpy().decode('utf-8')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'かさいですべてのこんぴゅーたーでぃすくがだめになってしまったとき、かいしゃは、もうおてあげのじょうきょうだった ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-EgkkRmnrUy",
        "outputId": "6d357e48-43b6-48ee-8dbc-6a0ef5855b6b"
      },
      "source": [
        "input_vocab_size = tokenizer.vocab_size()\n",
        "print(input_vocab_size)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(2000, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iif4D8WN8Kq",
        "outputId": "80891ff1-a65d-45c7-a7cb-be335fa47801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['speech'].dtype"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adKKfXmOUd4I",
        "outputId": "756765f2-4206-482e-c102-f401c56f652c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['speech']"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 500, 1025), dtype=float32, numpy=\n",
              "array([[[-5.50772878e-04, -1.33253331e-03,  1.92459440e-03, ...,\n",
              "          7.29341991e-08, -1.69558916e-07,  1.65047823e-07],\n",
              "        [ 9.53423907e-04, -4.61390195e-03,  8.26186035e-03, ...,\n",
              "          3.77185643e-08, -9.08039510e-09,  3.41096893e-08],\n",
              "        [-1.93090911e-03,  1.39400014e-03, -2.83157127e-03, ...,\n",
              "         -1.81491487e-07,  5.20376489e-07, -4.83356416e-07],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              "\n",
              "       [[-1.98839419e-03,  8.59539141e-04, -3.49902222e-03, ...,\n",
              "         -6.01168722e-07,  6.68514986e-08,  1.76485628e-07],\n",
              "        [-6.57372118e-04, -2.24305922e-03,  6.72239438e-03, ...,\n",
              "          2.30502337e-07, -2.44472176e-08,  3.75439413e-08],\n",
              "        [-1.72782666e-03, -6.08694274e-04,  9.23134200e-03, ...,\n",
              "          2.37952918e-07,  1.22236088e-08, -1.28464308e-07],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              "\n",
              "       [[-1.88169419e-03,  1.46785169e-03,  2.64415424e-03, ...,\n",
              "          4.44706529e-08, -4.41214070e-08, -6.72298484e-08],\n",
              "        [-1.79735757e-03,  4.01822006e-04, -4.78963973e-03, ...,\n",
              "          5.77652827e-07, -1.99113856e-07, -5.10422979e-07],\n",
              "        [-2.44754599e-03,  1.74054853e-03,  3.79015040e-03, ...,\n",
              "          1.01246405e-06,  1.66473910e-08, -6.51576556e-07],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 6.29351125e-04,  2.11903010e-03, -2.23349873e-03, ...,\n",
              "         -1.04424544e-07,  1.04657374e-07, -2.52068276e-07],\n",
              "        [ 1.07659097e-03,  9.02306754e-04, -4.30826377e-03, ...,\n",
              "          1.88825652e-07,  6.41157385e-08, -1.14552677e-07],\n",
              "        [-9.55712982e-04,  2.11151154e-03, -2.45284545e-03, ...,\n",
              "          5.26197255e-08, -8.85920599e-08,  2.17027264e-07],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              "\n",
              "       [[-1.00326235e-03,  3.23418109e-03, -3.64895118e-03, ...,\n",
              "          1.32946298e-07,  4.13274392e-08, -1.03143975e-07],\n",
              "        [ 3.61293001e-04,  6.41226827e-04,  3.21813580e-03, ...,\n",
              "          1.59721822e-07, -1.16706360e-07,  1.02474587e-07],\n",
              "        [-1.26640056e-03, -8.67407303e-04, -1.76302698e-02, ...,\n",
              "         -1.08964741e-07, -2.40106601e-08,  7.04894774e-08],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
              "\n",
              "       [[-5.52701531e-04, -2.01765122e-03,  1.44016203e-02, ...,\n",
              "          2.56579369e-07, -2.76602805e-07,  2.37836502e-07],\n",
              "        [-1.14276214e-03, -2.84691062e-03, -1.06035750e-02, ...,\n",
              "         -2.38418579e-07,  1.77416950e-07, -8.91159289e-08],\n",
              "        [-5.45163406e-04,  4.87423327e-04,  2.00129878e-02, ...,\n",
              "         -1.48080289e-07,  8.48231139e-08,  1.13650458e-07],\n",
              "        ...,\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-sf-IocJyTV"
      },
      "source": [
        "#encoder(data['speech'])"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cjiiyX82XU5"
      },
      "source": [
        "# Gated Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCSRf4_0wZHZ"
      },
      "source": [
        "d_model = 512"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIkO24ZTsYng"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFvfDRy_sgBc"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaKn44ztspmJ"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "validation_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
        "instant_loss = tf.keras.metrics.Mean(name='instant_loss')\n",
        "\n",
        "epoch_train_loss = tf.keras.metrics.Mean(name='epoch_train_loss')"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onNbBPkQaY3I"
      },
      "source": [
        "class GatedConvolution(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters, kernel_size, dropout_rate, padding='causal'):\n",
        "    super(GatedConvolution, self).__init__()\n",
        "\n",
        "    self.convolution = tf.keras.layers.Conv1D(\n",
        "    filters=filters, kernel_size=kernel_size, padding=padding,\n",
        "    )\n",
        "\n",
        "    self.gated = tf.keras.layers.Conv1D(\n",
        "    filters=filters, kernel_size=kernel_size, padding=padding,\n",
        "    )\n",
        "\n",
        "    self.multiply = tf.keras.layers.Multiply()\n",
        "    \n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "  def call(self, x, training):\n",
        "\n",
        "    convolution_output = self.convolution(x)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    gate_output = tf.keras.activations.sigmoid(self.gated(x))\n",
        "    \n",
        "    output = self.multiply([convolution_output, gate_output])\n",
        "\n",
        "    output = self.dropout(output, training=training)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN4BtD00aN6N"
      },
      "source": [
        "class GatedConvolutionalEncoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(GatedConvolutionalEncoder, self).__init__()\n",
        "\n",
        "    self.num_convolutions = 14\n",
        "  \n",
        "    self.convolutions = [GatedConvolution(filters=100, kernel_size=3, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=100, kernel_size=4, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=100, kernel_size=5, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=125, kernel_size=6, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=125, kernel_size=7, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=150, kernel_size=8, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=175, kernel_size=9, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=200, kernel_size=10, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=225, kernel_size=11, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=250, kernel_size=12, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=250, kernel_size=13, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=250, kernel_size=14, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=300, kernel_size=15, padding='causal', dropout_rate=0.25),\n",
        "                        GatedConvolution(filters=300, kernel_size=21, padding='causal', dropout_rate=0.25)]\n",
        "\n",
        "    self.fc1 = tf.keras.layers.Dense(1000)\n",
        "    self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
        "\n",
        "  def call(self, inp, training=True):\n",
        "    output = inp\n",
        "    for i in range(self.num_convolutions):\n",
        "      output = self.convolutions[i](output, training=training)\n",
        "    #output = self.convolutions[0]\n",
        "    #output = self.convolutions[1]\n",
        "\n",
        "    output = self.fc1(output)\n",
        "    output = self.final_layer(output)\n",
        "    return output\n",
        "\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkF8khEZiUQG"
      },
      "source": [
        "encoder = GatedConvolutionalEncoder()"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zalc_nxXGevD"
      },
      "source": [
        "#encoder.summary()"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y1kDSm9st_Q"
      },
      "source": [
        "batch_counter = tf.Variable(-1, name=\"batch_counter\", dtype=tf.int32)\n",
        "epoch_counter = tf.Variable(0, name=\"epoch_counter\", dtype=tf.int32)\n",
        "\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           optimizer=optimizer,\n",
        "                          batch_counter=batch_counter,\n",
        "                           epoch_counter=epoch_counter)\n",
        "\n",
        "#ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "#                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXw3bhf7tGfJ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(speech, text, speech_lengths, text_lengths):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = encoder(speech)\n",
        "\n",
        "    labels = text\n",
        "    label_length = text_lengths\n",
        "    logit_length = speech_lengths\n",
        "    unique = tf.nn.ctc_unique_labels(text)\n",
        "\n",
        "\n",
        "    loss = tf.nn.ctc_loss(\n",
        "        labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
        "    )\n",
        "\n",
        "  gradients = tape.gradient(loss, encoder.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  instant_loss(loss)\n",
        "  epoch_train_loss(loss)\n",
        "  return logits\n",
        "    "
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgTWTimuzAtQ"
      },
      "source": [
        "@tf.function\n",
        "def validation_step(speech, text, speech_lengths, text_lengths):\n",
        "  logits = encoder(speech)\n",
        "\n",
        "  labels = text\n",
        "  label_length = text_lengths\n",
        "  logit_length = speech_lengths\n",
        "  unique = tf.nn.ctc_unique_labels(text)\n",
        "\n",
        "\n",
        "  loss = tf.nn.ctc_loss(\n",
        "      labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
        "  )\n",
        "\n",
        "\n",
        "  validation_loss(loss)\n",
        "  return logits"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdWdYE11Kea"
      },
      "source": [
        "EPOCHS = 5\n",
        "validate_period = 30\n",
        "save_period = 52\n",
        "print_period = 1\n",
        "train_period = 30"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KDwGC1X1hXi"
      },
      "source": [
        "batch_offset = batch_counter.numpy() + 1\n",
        "epoch_offset = epoch_counter.numpy()"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu1Rd8Qr5SW4",
        "outputId": "0bcfa78d-b39d-49fb-e3f6-06e837da5c83"
      },
      "source": [
        "jvs_dataset.cardinality()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=1873>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4TQSjEf24f5"
      },
      "source": [
        "dataset = jvs_dataset.take(100)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KNjHup-X4i",
        "outputId": "d54cef79-a60a-4656-a6af-bdb6851b1d13"
      },
      "source": [
        "list(range(1,3))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AhEzjP3ztJYz",
        "outputId": "8009ed4c-db01-4ddb-9115-bb98b07d904e"
      },
      "source": [
        "# Resume at the batch we left off at if there was some interruption\n",
        "if batch_counter != 0 or epoch_counter != 0:\n",
        "    dataset = dataset.skip(batch_offset)\n",
        "    print(\"Resuming from batch:\", batch_offset, 'epoch:', epoch_offset)\n",
        "\n",
        "for epoch in range(epoch_offset, EPOCHS):\n",
        "  epoch_counter.assign(epoch)\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  validation_loss.reset_states()\n",
        "  instant_loss.reset_states()\n",
        "\n",
        "  \n",
        "  for (batch, data) in enumerate(dataset):\n",
        "    batch += batch_offset\n",
        "    batch_counter.assign(batch)\n",
        "    print(batch)\n",
        "    #if batch == 9:\n",
        "    #  print(tokenizer.detokenize(data['text']).numpy()[0].decode('utf-8'))\n",
        "    #  print(data['text'].numpy()[0])\n",
        "    #  continue\n",
        "    \n",
        "    #texts = list([i for i in tokenizer.detokenize(tf.boolean_mask(data['text'], data['text'] != 0)).numpy()])\n",
        "    #print(ids)\n",
        "    \n",
        "    ids = list(data['id'].numpy())\n",
        "    texts = []\n",
        "    for i in range(len(ids)):\n",
        "        texts.append(tokenizer.detokenize(tf.boolean_mask(data['text'][i], data['text'][i] != 0)).numpy().decode('utf-8'))\n",
        "    text = ''\n",
        "    for i in range(len(ids)):\n",
        "        text += 'id: {}'.format(ids[i]) + '\\n' + 'text: {}'.format(texts[i]) + '\\n'\n",
        "    #print(text)\n",
        "\n",
        "    logits = train_step(data['speech'], data['text'], data['speech_lengths'], data['text_lengths'])\n",
        "    # Prevent predicting 0\n",
        "    \n",
        "\n",
        "\n",
        "    if (batch+1) % print_period == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {instant_loss.result():.4f}')\n",
        "      instant_loss.reset_states()\n",
        "\n",
        "    if (batch+1) % train_period == 0:\n",
        "      with file_writer.as_default():\n",
        "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=batch)\n",
        "        \n",
        "        \n",
        "        \n",
        "      print(f'Training Print: Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
        "      train_loss.reset_states()\n",
        "\n",
        "    if (batch + 1) % save_period == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print (f'Saving checkpoint for batch {batch} at {ckpt_save_path}')\n",
        "\n",
        "      instance_repeat = tf.repeat(tf.expand_dims(tf.range(2000), axis=0), 500, axis=0)\n",
        "      batch_repeat = tf.repeat(tf.expand_dims(instance_repeat, axis=0), BATCH_SIZE, axis=0)\n",
        "      logits_zero_removed = tf.where(batch_repeat != 0, logits, -np.inf)\n",
        "      logits = logits_zero_removed\n",
        "    \n",
        "      tokenized = tf.math.argmax(logits, axis=2)\n",
        "      tokenized = tf.cast(tokenized, tf.int32)\n",
        "      detokenized = tokenizer.detokenize(tokenized).numpy()\n",
        "    #print(detokenized.numpy()[0].decode('utf-8'))\n",
        "\n",
        "      ids = list(data['id'].numpy())\n",
        "      texts = []\n",
        "      for i in range(len(ids)):\n",
        "        texts.append(tokenizer.detokenize(tf.boolean_mask(data['text'][i], data['text'][i] != 0)).numpy().decode('utf-8'))\n",
        "      text = ''\n",
        "      for i in range(len(ids)):\n",
        "        text += 'id: {}'.format(ids[i]) + '\\n\\n' + 'text: {}'.format(texts[i]) + '\\n\\n' + 'predictions: {}'.format(detokenized[i].decode('utf-8')) + '\\n\\n\\n'\n",
        "        #print(text)\n",
        "      with file_writer.as_default():\n",
        "        tf.summary.text(\"Training Text. Epoch: {}, Batch: {}\".format(epoch, batch), text, step=batch)\n",
        "\n",
        "    if (batch + 1) % validate_period == 0:\n",
        "      for (val_batch, val_data) in enumerate(jsut_dataset.take(5)):\n",
        "        validation_step(val_data['speech'], val_data['text'], val_data['speech_lengths'], val_data['text_lengths'])\n",
        "      with file_writer.as_default():\n",
        "        tf.summary.scalar(\"validation_loss\", validation_loss.result(), step=batch)\n",
        "      print(f'Validation Print: Epoch {epoch + 1} Batch {batch} Loss {validation_loss.result():.4f}')\n",
        "      validation_loss.reset_states()\n",
        "\n",
        "  \n",
        "  batch_offset = 0\n",
        "  print(f'Epoch {epoch + 1} Loss {epoch_train_loss.result():.4f}')\n",
        "  epoch_train_loss.reset_states()\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resuming from batch: 0 epoch: 0\n",
            "0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1196: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1196: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 561.9199\n",
            "1\n",
            "Epoch 1 Batch 1 Loss 536.9805\n",
            "2\n",
            "Epoch 1 Batch 2 Loss 580.2170\n",
            "3\n",
            "Epoch 1 Batch 3 Loss 545.3163\n",
            "4\n",
            "Epoch 1 Batch 4 Loss 577.1151\n",
            "5\n",
            "Epoch 1 Batch 5 Loss 728.5364\n",
            "6\n",
            "Epoch 1 Batch 6 Loss 440.2622\n",
            "7\n",
            "Epoch 1 Batch 7 Loss 490.1438\n",
            "8\n",
            "Epoch 1 Batch 8 Loss 654.2244\n",
            "9\n",
            "Epoch 1 Batch 9 Loss 518.5106\n",
            "10\n",
            "Epoch 1 Batch 10 Loss 484.0682\n",
            "11\n",
            "Epoch 1 Batch 11 Loss 481.7744\n",
            "12\n",
            "Epoch 1 Batch 12 Loss 652.9542\n",
            "13\n",
            "Epoch 1 Batch 13 Loss 494.5081\n",
            "14\n",
            "Epoch 1 Batch 14 Loss 698.2598\n",
            "15\n",
            "Epoch 1 Batch 15 Loss 485.7914\n",
            "16\n",
            "Epoch 1 Batch 16 Loss 601.5153\n",
            "17\n",
            "Epoch 1 Batch 17 Loss 502.5918\n",
            "18\n",
            "Epoch 1 Batch 18 Loss 587.0679\n",
            "19\n",
            "Epoch 1 Batch 19 Loss 478.9898\n",
            "20\n",
            "Epoch 1 Batch 20 Loss 420.2528\n",
            "21\n",
            "Epoch 1 Batch 21 Loss 694.9532\n",
            "22\n",
            "Epoch 1 Batch 22 Loss 443.5592\n",
            "23\n",
            "Epoch 1 Batch 23 Loss 585.6325\n",
            "24\n",
            "Epoch 1 Batch 24 Loss 654.7162\n",
            "25\n",
            "Epoch 1 Batch 25 Loss 609.1465\n",
            "26\n",
            "Epoch 1 Batch 26 Loss 551.1705\n",
            "27\n",
            "Epoch 1 Batch 27 Loss 618.1853\n",
            "28\n",
            "Epoch 1 Batch 28 Loss 529.0765\n",
            "29\n",
            "Epoch 1 Batch 29 Loss 545.0487\n",
            "Training Print: Epoch 1 Batch 29 Loss 558.4163\n",
            "Validation Print: Epoch 1 Batch 29 Loss 830.2653\n",
            "30\n",
            "Epoch 1 Batch 30 Loss 458.0191\n",
            "31\n",
            "Epoch 1 Batch 31 Loss 603.0057\n",
            "32\n",
            "Epoch 1 Batch 32 Loss 524.4880\n",
            "33\n",
            "Epoch 1 Batch 33 Loss 583.5133\n",
            "34\n",
            "Epoch 1 Batch 34 Loss 538.8198\n",
            "35\n",
            "Epoch 1 Batch 35 Loss 645.7625\n",
            "36\n",
            "Epoch 1 Batch 36 Loss 534.3562\n",
            "37\n",
            "Epoch 1 Batch 37 Loss 583.4882\n",
            "38\n",
            "Epoch 1 Batch 38 Loss 714.4110\n",
            "39\n",
            "Epoch 1 Batch 39 Loss 541.0518\n",
            "40\n",
            "Epoch 1 Batch 40 Loss 532.4218\n",
            "41\n",
            "Epoch 1 Batch 41 Loss 493.6116\n",
            "42\n",
            "Epoch 1 Batch 42 Loss 650.6986\n",
            "43\n",
            "Epoch 1 Batch 43 Loss 607.8605\n",
            "44\n",
            "Epoch 1 Batch 44 Loss 523.3032\n",
            "45\n",
            "Epoch 1 Batch 45 Loss 482.3890\n",
            "46\n",
            "Epoch 1 Batch 46 Loss 730.5187\n",
            "47\n",
            "Epoch 1 Batch 47 Loss 571.5495\n",
            "48\n",
            "Epoch 1 Batch 48 Loss 591.9197\n",
            "49\n",
            "Epoch 1 Batch 49 Loss 611.5458\n",
            "50\n",
            "Epoch 1 Batch 50 Loss 580.6838\n",
            "51\n",
            "Epoch 1 Batch 51 Loss 552.9093\n",
            "Saving checkpoint for batch 51 at gs://general-304503/notebook_logs/jvs_gated_convolution_stt_17_3_21/checkpoints/ckpt-1\n",
            "52\n",
            "Epoch 1 Batch 52 Loss 438.3950\n",
            "53\n",
            "Epoch 1 Batch 53 Loss 579.4410\n",
            "54\n",
            "Epoch 1 Batch 54 Loss 561.8834\n",
            "55\n",
            "Epoch 1 Batch 55 Loss 599.7366\n",
            "56\n",
            "Epoch 1 Batch 56 Loss 515.2162\n",
            "57\n",
            "Epoch 1 Batch 57 Loss 664.7024\n",
            "58\n",
            "Epoch 1 Batch 58 Loss 577.5564\n",
            "59\n",
            "Epoch 1 Batch 59 Loss 552.9669\n",
            "Training Print: Epoch 1 Batch 59 Loss 571.5408\n",
            "Validation Print: Epoch 1 Batch 59 Loss 829.5195\n",
            "60\n",
            "Epoch 1 Batch 60 Loss 793.0981\n",
            "61\n",
            "Epoch 1 Batch 61 Loss 489.4505\n",
            "62\n",
            "Epoch 1 Batch 62 Loss 529.7913\n",
            "63\n",
            "Epoch 1 Batch 63 Loss 579.8091\n",
            "64\n",
            "Epoch 1 Batch 64 Loss 554.2700\n",
            "65\n",
            "Epoch 1 Batch 65 Loss 550.6578\n",
            "66\n",
            "Epoch 1 Batch 66 Loss 676.0197\n",
            "67\n",
            "Epoch 1 Batch 67 Loss 464.1745\n",
            "68\n",
            "Epoch 1 Batch 68 Loss 422.8482\n",
            "69\n",
            "Epoch 1 Batch 69 Loss 735.7275\n",
            "70\n",
            "Epoch 1 Batch 70 Loss 616.8819\n",
            "71\n",
            "Epoch 1 Batch 71 Loss 476.3018\n",
            "72\n",
            "Epoch 1 Batch 72 Loss 507.8022\n",
            "73\n",
            "Epoch 1 Batch 73 Loss 534.8264\n",
            "74\n",
            "Epoch 1 Batch 74 Loss 510.7762\n",
            "75\n",
            "Epoch 1 Batch 75 Loss 466.2048\n",
            "76\n",
            "Epoch 1 Batch 76 Loss 470.1240\n",
            "77\n",
            "Epoch 1 Batch 77 Loss 627.0848\n",
            "78\n",
            "Epoch 1 Batch 78 Loss 695.5281\n",
            "79\n",
            "Epoch 1 Batch 79 Loss 564.9001\n",
            "80\n",
            "Epoch 1 Batch 80 Loss 635.7692\n",
            "81\n",
            "Epoch 1 Batch 81 Loss 466.1776\n",
            "82\n",
            "Epoch 1 Batch 82 Loss 524.3036\n",
            "83\n",
            "Epoch 1 Batch 83 Loss 564.4175\n",
            "84\n",
            "Epoch 1 Batch 84 Loss 577.0398\n",
            "85\n",
            "Epoch 1 Batch 85 Loss 498.6969\n",
            "86\n",
            "Epoch 1 Batch 86 Loss 620.6577\n",
            "87\n",
            "Epoch 1 Batch 87 Loss 622.4708\n",
            "88\n",
            "Epoch 1 Batch 88 Loss 663.8967\n",
            "89\n",
            "Epoch 1 Batch 89 Loss 480.2009\n",
            "Training Print: Epoch 1 Batch 89 Loss 563.9969\n",
            "Validation Print: Epoch 1 Batch 89 Loss 801.2660\n",
            "90\n",
            "Epoch 1 Batch 90 Loss 522.2930\n",
            "91\n",
            "Epoch 1 Batch 91 Loss 608.9547\n",
            "92\n",
            "Epoch 1 Batch 92 Loss 600.1740\n",
            "93\n",
            "Epoch 1 Batch 93 Loss 570.4360\n",
            "94\n",
            "Epoch 1 Batch 94 Loss 502.7295\n",
            "95\n",
            "Epoch 1 Batch 95 Loss 511.2305\n",
            "96\n",
            "Epoch 1 Batch 96 Loss 538.6935\n",
            "97\n",
            "Epoch 1 Batch 97 Loss 536.2304\n",
            "98\n",
            "Epoch 1 Batch 98 Loss 493.1051\n",
            "99\n",
            "Epoch 1 Batch 99 Loss 573.5338\n",
            "Epoch 1 Loss 562.7600\n",
            "Time taken for 1 epoch: 156.30 secs\n",
            "\n",
            "0\n",
            "Epoch 2 Batch 0 Loss 475.6767\n",
            "1\n",
            "Epoch 2 Batch 1 Loss 448.5142\n",
            "2\n",
            "Epoch 2 Batch 2 Loss 478.5241\n",
            "3\n",
            "Epoch 2 Batch 3 Loss 403.8262\n",
            "4\n",
            "Epoch 2 Batch 4 Loss 397.8218\n",
            "5\n",
            "Epoch 2 Batch 5 Loss 397.9753\n",
            "6\n",
            "Epoch 2 Batch 6 Loss 313.5042\n",
            "7\n",
            "Epoch 2 Batch 7 Loss 289.3540\n",
            "8\n",
            "Epoch 2 Batch 8 Loss 290.8978\n",
            "9\n",
            "Epoch 2 Batch 9 Loss 301.9494\n",
            "10\n",
            "Epoch 2 Batch 10 Loss 266.8377\n",
            "11\n",
            "Epoch 2 Batch 11 Loss 248.9035\n",
            "12\n",
            "Epoch 2 Batch 12 Loss 257.5023\n",
            "13\n",
            "Epoch 2 Batch 13 Loss 245.1356\n",
            "14\n",
            "Epoch 2 Batch 14 Loss 258.6847\n",
            "15\n",
            "Epoch 2 Batch 15 Loss 261.0613\n",
            "16\n",
            "Epoch 2 Batch 16 Loss 257.8328\n",
            "17\n",
            "Epoch 2 Batch 17 Loss 264.2850\n",
            "18\n",
            "Epoch 2 Batch 18 Loss 277.1445\n",
            "19\n",
            "Epoch 2 Batch 19 Loss 270.6355\n",
            "20\n",
            "Epoch 2 Batch 20 Loss 258.9568\n",
            "21\n",
            "Epoch 2 Batch 21 Loss 309.6650\n",
            "22\n",
            "Epoch 2 Batch 22 Loss 282.2741\n",
            "23\n",
            "Epoch 2 Batch 23 Loss 292.3411\n",
            "24\n",
            "Epoch 2 Batch 24 Loss 301.4820\n",
            "25\n",
            "Epoch 2 Batch 25 Loss 306.3448\n",
            "26\n",
            "Epoch 2 Batch 26 Loss 296.9182\n",
            "27\n",
            "Epoch 2 Batch 27 Loss 302.7516\n",
            "28\n",
            "Epoch 2 Batch 28 Loss 289.1139\n",
            "29\n",
            "Epoch 2 Batch 29 Loss 286.3095\n",
            "Training Print: Epoch 2 Batch 29 Loss 311.0742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-858e7fd5c9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidate_period\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsut_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrdL4R_mSC5Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}