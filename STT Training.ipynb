{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6rnxhZ3efIbsKAd3/kArL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaidon-Smith/AI-Karaoke/blob/main/STT%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkRMEbJrdWhE",
        "outputId": "ad2da1c1-036d-47c6-c4d7-f35d524d7366"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnmItTWZe0ay",
        "outputId": "c5187f07-7003-4fc8-81e3-b01e545411eb"
      },
      "source": [
        "!pip install tfds-nightly\n",
        "!pip install pydub"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (4.2.0.dev202103110106)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (5.1.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (20.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.28.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tfds-nightly) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->tfds-nightly) (54.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tfds-nightly) (1.53.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaxFPa1ne2Xm"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLtjhXMepL-",
        "outputId": "fa1a4715-29c5-4453-a644-3e44b91f3a93"
      },
      "source": [
        "!git clone https://github.com/Jaidon-Smith/public_datasets.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'public_datasets' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkMA6i1TVy1S"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBFpGwTo9kUs",
        "outputId": "d96c0513-8846-4b91-8fe4-ce82eea71a61"
      },
      "source": [
        "!pip install --quiet tensorflow-text\n",
        "\n",
        "import tensorflow_text as text\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "model_file = '/gdrive/MyDrive/Japanese/Bible/letters_v2.model'\n",
        "model = gfile.GFile(model_file, 'rb').read()\n",
        "\n",
        "tokenizer = text.SentencepieceTokenizer(model=model)\n",
        "\n",
        "input_vocab_size = tokenizer.vocab_size().numpy()\n",
        "print(input_vocab_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 8.5MB/s \n",
            "\u001b[?25h13306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoclieqSYEcT"
      },
      "source": [
        "[This tutorial](https://www.tensorflow.org/tutorials/audio/simple_audio) has some good resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CEiDbcZrLzK"
      },
      "source": [
        "import pickle\n",
        "jsut_speech_lengths = pickle.load(open('/gdrive/MyDrive/datasets/jsut_speech_lengths', \"rb\"))\n",
        "jsut_text_lengths = pickle.load(open('/gdrive/MyDrive/datasets/jsut_text_lengths', \"rb\"))"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE-dmHaOEBG7",
        "outputId": "8d3b57cc-4857-421b-e2c3-d5a80ed45335"
      },
      "source": [
        "tokenizer.tokenize(tf.constant(\"英語が分かりますか\"))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=\n",
              "array([13305,  1383,   231,    16,   100,    21,    23,    12,    17,\n",
              "          21], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhEjKJA_V2FF"
      },
      "source": [
        "#def preprocess_audio(old_audio):\n",
        "#  audio = old_audio/tf.int16.max\n",
        "#  audio = tf.cast(audio, tf.float32)\n",
        "#  audio = tfio.audio.resample(audio, 48000, 24000)\n",
        "#  audio = tf.signal.stft(audio, frame_length=511, frame_step=256)\n",
        "#  return audio"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWcfthvAEbZI"
      },
      "source": [
        "import pykakasi\n",
        "converter = pykakasi.kakasi()"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "154oNA4E6hEg"
      },
      "source": [
        "def preprocess_text(old_text):\n",
        "  text = old_text\n",
        "  #text = old_text.numpy()\n",
        "  #text = \n",
        "  #text = text.decode('utf-8')\n",
        "  text = tokenizer.tokenize(text)\n",
        "  text = tf.boolean_mask(text, text != 13305)\n",
        "  #text = text.to_tensor()\n",
        "  #text = tf.constant(text)\n",
        "  text = tf.pad(text, paddings=[[0 , 200 - tf.shape(text)[0]]])\n",
        "  return text"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiqQD4F8j8YJ"
      },
      "source": [
        "def preprocess_text_no_pad(old_text):\n",
        "  text = old_text\n",
        "  #text = old_text.numpy()\n",
        "  #text = \n",
        "  #text = text.decode('utf-8')\n",
        "  text = tokenizer.tokenize(text)\n",
        "  text = tf.boolean_mask(text, text != 13305)\n",
        "  #text = text.to_tensor()\n",
        "  #text = tf.constant(text)\n",
        "  #text = tf.pad(text, paddings=[[0 , 200 - tf.shape(text)[0]]])\n",
        "  text = tf.shape(text)[0]\n",
        "  return text"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zRglPY6J-A0"
      },
      "source": [
        "# For loop that I don't think will work\n",
        "#def preprocess_text(old_text):\n",
        "#  batch_size = \n",
        "#  output = \n",
        "#  text = old_text.numpy().decode('utf-8')\n",
        "#  text = tokenizer.tokenize(text)\n",
        "#  text = tf.constant(text)\n",
        "#  text = tf.pad(text, paddings=[[0 , 2000 - tf.shape(text)[0]]])\n",
        "#  return text"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dds39VdzGE6W"
      },
      "source": [
        "#def preprocess_text_batches(old_text):\n",
        "#  return tf.map_fn(\n",
        "#    preprocess_text, old_text\n",
        "#)"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aKiQLe764QV"
      },
      "source": [
        "#text.numpy()[0].decode('utf-8')"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsVdkAhG4u6n"
      },
      "source": [
        "#dataset = dataset.map(lambda x: {\"id\": x[\"id\"], \"speech\": preprocess_audio(x[\"speech\"]), \"text\": x[\"text\"]})"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tx0_eBn4xRd"
      },
      "source": [
        "#jsut_speech_lengths = {}\n",
        "\n",
        "#for i, data in enumerate(dataset):\n",
        "  \n",
        "#  jsut_speech_lengths[data['id'].ref()] = data['speech'].shape[0]\n",
        "#  break"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxIOOV5_pS58"
      },
      "source": [
        "def preprocess_audio(old_audio):\n",
        "  audio = old_audio/tf.int16.max\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  audio = tfio.audio.resample(audio, 48000, 24000)\n",
        "\n",
        "  #audio = tf.signal.stft(audio, frame_length=511, frame_step=256)\n",
        "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
        "\n",
        "  audio = tf.pad(audio, paddings=[[0 , 500 - tf.shape(audio)[0]], [0,0]])\n",
        "  return audio"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCRZuRVb4H_d"
      },
      "source": [
        "def preprocess_audio_no_pad(old_audio):\n",
        "  audio = old_audio/tf.int16.max\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  audio = tfio.audio.resample(audio, 48000, 24000)\n",
        "\n",
        "  \n",
        "  #audio = tf.signal.stft(audio, frame_length=511, frame_step=256)\n",
        "  audio = tf.signal.stft(audio, frame_length=2047, frame_step=1024)\n",
        "\n",
        "  #audio = tf.pad(audio, paddings=[[0 , 2000 - tf.shape(audio)[0]], [0,0]])\n",
        "  audio = tf.shape(audio)[0]\n",
        "  return audio"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31QwbiiB2EbV"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def preprocess_audio_no_pad(old_audio):\n",
        "  audio = old_audio/tf.int16.max\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "  audio = tfio.audio.resample(audio, 48000, 24000)\n",
        "  audio = tf.signal.stft(audio, frame_length=511, frame_step=256)\n",
        "  #audio = tf.pad(audio, paddings=[[0 , 2000 - tf.shape(audio)[0]], [0,0]])\n",
        "  speech_lengths = tf.map_fn(lambda x: tf.shape(x)[0], audio, fn_output_signature=tf.int32)\n",
        "  return speech_lengths\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSaiGkuUZYFL"
      },
      "source": [
        "download_config = tfds.download.DownloadConfig(manual_dir='/gdrive/MyDrive/datasets/public_datasets/downloads/manual')\n",
        "\n",
        "dataset, info = tfds.load(\n",
        "                    \"jsut\",\n",
        "                    split=\"train\",\n",
        "                    data_dir='/gdrive/MyDrive/datasets/public_datasets',\n",
        "                    download_and_prepare_kwargs={\"download_config\": download_config},\n",
        "                    with_info = True)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4dWKtfoEyKy"
      },
      "source": [
        "def convert_to_kana(text):\n",
        "  kana = converter.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')\n",
        "  return ''.join([i['hira'] for i in kana])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsITXisWE81g"
      },
      "source": [
        "dataset = dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": x[\"speech\"],\n",
        "                                 \"text\": convert_to_kana(x[\"text\"])})"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMO7sg0EDKwA"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "dataset = dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech_lengths\": tf.py_function(func=preprocess_audio_no_pad, inp=[x[\"speech\"]],Tout=tf.int32),\n",
        "                                 \"speech\": preprocess_audio(x[\"speech\"]),\n",
        "                                 \"text\": preprocess_text(x[\"text\"])})\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_93d16BUsv"
      },
      "source": [
        "dataset = dataset.map(lambda x: {\"id\": x[\"id\"],\n",
        "                                 \"speech\": preprocess_audio(x[\"speech\"]),\n",
        "                                 \"speech_lengths\": preprocess_audio_no_pad(x[\"speech\"]),\n",
        "                                 \"text\": preprocess_text(x[\"text\"]),\n",
        "                                 \"text_lengths\": preprocess_text_no_pad(x[\"text\"])})"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx4B8-QkZcar"
      },
      "source": [
        "#dataset = dataset.map(lambda x: {\"id\": x[\"id\"], \"speech\": preprocess_audio(x[\"speech\"]), \"text\": x[\"text\"]})"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htZ7O_L6ZPMm"
      },
      "source": [
        "dataset = dataset.batch(8)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXIFo8SjmVQF"
      },
      "source": [
        "#max = 0\n",
        "\n",
        "#for i, data in enumerate(dataset):\n",
        "#  value = data['speech'].shape[0]\n",
        "#  if value > max:\n",
        "#    max = value\n",
        "#    print(\"new max is:\", value, \"with id:\", data['id'].numpy())"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X-RuP9EZzE_"
      },
      "source": [
        "ds = dataset.take(1)"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0TKYDlcZ3As"
      },
      "source": [
        "for i in ds:\n",
        "  data = i\n",
        "  break"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucj0KrtIlpD6",
        "outputId": "d46542b7-7267-4d48-930d-6bddf99b74a4"
      },
      "source": [
        "print(data[\"speech_lengths\"])"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([222], shape=(1,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdJoPZ8UaIoD",
        "outputId": "8b4e8c9a-9abf-4302-ff91-654d001a120c"
      },
      "source": [
        "print(tf.shape(data['text']).numpy())"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1 200]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwN7enziSxrh",
        "outputId": "cbc8995b-215a-40d0-8251-d98c5567b0cb"
      },
      "source": [
        "print(data['text'][0][:100])"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[  35  318    7   22   24   15   33 1051   20   57   31  213  206   39\n",
            "    7    3    6   17   15   22   20   77   33    1    5   33    8   13\n",
            "   15    4    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0], shape=(100,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKIzjgLrXmIV",
        "outputId": "7efe7a5c-b44c-4eee-cd29-da5d6e78fac7"
      },
      "source": [
        "#input_vocab_size = tokenizer.vocab_size() - 1\n",
        "input_vocab_size = 2000\n",
        "print(input_vocab_size)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTYxxxCjZcmb"
      },
      "source": [
        "speech = data['speech']\n",
        "text = data['text']\n",
        "id = data['id']\n",
        "speech_lengths = data['speech_lengths']\n",
        "text_lengths = data['text_lengths']"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rh0CHdEFTb0",
        "outputId": "77840d2d-8593-4148-c8fd-c6f8c83ef6da"
      },
      "source": [
        "text_lengths"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([61, 46, 27, 45, 18, 69, 25, 26], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfMbccnjFX-1",
        "outputId": "24baa417-4298-47d8-c53d-1cd46e484bc5"
      },
      "source": [
        "speech_lengths"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([222, 183,  99, 169,  69, 246, 100, 108], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cjiiyX82XU5"
      },
      "source": [
        "# Using Transformer Encoder Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCSRf4_0wZHZ"
      },
      "source": [
        "d_model = 512"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1HYBRd6NQu"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fHkHLsK22m5"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWLWmM3MPr5V"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR9vt3slYa6Z"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpqOGPpKYfuf"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lvYoMO4j1w"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.embedding = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training=True):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Makes input larger by a constant\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIkO24ZTsYng"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFvfDRy_sgBc"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaKn44ztspmJ"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVPnnLizwA3m"
      },
      "source": [
        "transformer = tf.keras.models.Sequential([\n",
        " \n",
        "  Encoder(num_layers=6,\n",
        "          d_model=d_model,\n",
        "          num_heads=8,\n",
        "          dff=2048, \n",
        "          maximum_position_encoding=10000),\n",
        " \n",
        "  tf.keras.layers.Dense(input_vocab_size)\n",
        "])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y1kDSm9st_Q"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Db9L3YS2qCw",
        "outputId": "95722cdf-7187-41f7-f4bf-bf50dc0dcd35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transformer(speech)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 500, 2000), dtype=float32, numpy=\n",
              "array([[[-0.23670131,  0.5035986 , -0.9696934 , ..., -0.32356334,\n",
              "         -0.15642008,  0.52041733],\n",
              "        [-0.20735277,  0.04722196, -0.7190511 , ...,  0.11990781,\n",
              "         -0.16057464,  0.84492683],\n",
              "        [-0.40871394,  0.49407247, -0.8547423 , ...,  0.5248531 ,\n",
              "          0.26138884,  0.70770925],\n",
              "        ...,\n",
              "        [-0.6523657 ,  0.62752515, -1.0001107 , ..., -0.03352382,\n",
              "          0.5254311 ,  0.93971825],\n",
              "        [ 0.24591252,  0.28143832, -0.72985864, ...,  0.2522912 ,\n",
              "          0.13842534,  1.1001192 ],\n",
              "        [-0.24758309,  0.37546644, -0.4372609 , ...,  0.19912373,\n",
              "         -0.05195087,  0.9075059 ]],\n",
              "\n",
              "       [[-0.04687387,  0.8616662 , -0.72437745, ...,  0.87763244,\n",
              "          0.66539514,  0.42403227],\n",
              "        [ 0.04295494,  0.01894596,  0.05669194, ...,  0.29154584,\n",
              "          0.2664561 ,  0.11589174],\n",
              "        [-0.20177966,  0.8460218 , -0.57802176, ...,  0.494439  ,\n",
              "          0.25340444,  0.18832803],\n",
              "        ...,\n",
              "        [-0.10708526,  0.2885273 , -0.0777889 , ...,  0.5886873 ,\n",
              "          0.14002101,  0.36312154],\n",
              "        [ 0.02586749,  0.38371184, -0.45090806, ...,  0.749949  ,\n",
              "          0.4194494 ,  0.13323137],\n",
              "        [-0.10878297,  0.22868237, -0.38662517, ...,  0.4601162 ,\n",
              "          0.5221067 ,  0.28106183]],\n",
              "\n",
              "       [[-0.67062724,  0.79766536, -0.16977152, ...,  1.271769  ,\n",
              "          0.45655447,  0.5224305 ],\n",
              "        [-0.9038615 ,  0.68407226, -0.17609334, ...,  0.66379863,\n",
              "          0.11467358,  0.47056037],\n",
              "        [-0.09935051, -0.24808848, -0.20421448, ...,  1.178142  ,\n",
              "          0.20212178,  0.42410892],\n",
              "        ...,\n",
              "        [-0.40554863,  0.70284134,  0.33002543, ...,  1.2386822 ,\n",
              "         -0.01775204,  0.68609387],\n",
              "        [-1.0362575 ,  0.5680555 , -0.11538772, ...,  0.9920026 ,\n",
              "          0.38108903,  0.51394933],\n",
              "        [-0.68839467,  0.37858927, -0.34435058, ...,  1.0558093 ,\n",
              "          0.8240428 ,  0.75955015]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.2170223 , -0.00740816, -0.08657457, ...,  0.5292024 ,\n",
              "          0.40997505,  0.41985983],\n",
              "        [-0.24702238,  0.41933322,  0.06273278, ...,  0.48894286,\n",
              "          0.5271079 ,  0.30086702],\n",
              "        [-0.11265878,  0.2721701 , -0.3455366 , ...,  0.56882155,\n",
              "         -0.06792264,  0.61775464],\n",
              "        ...,\n",
              "        [-0.09597986,  0.33987224, -0.1279369 , ...,  0.86919904,\n",
              "          0.1746868 ,  0.48036152],\n",
              "        [-0.53253627,  0.3855224 ,  0.31099403, ...,  0.25432307,\n",
              "          0.141164  ,  0.921125  ],\n",
              "        [-0.11043879,  0.4421406 , -0.54275316, ...,  0.8772275 ,\n",
              "          0.07321943,  0.88351953]],\n",
              "\n",
              "       [[-0.29321063,  0.04827142, -0.07601801, ...,  0.86008036,\n",
              "          0.59280515,  0.8311357 ],\n",
              "        [ 0.04778203,  0.45276874, -0.36241305, ...,  0.5834295 ,\n",
              "          0.61234796,  1.0311403 ],\n",
              "        [ 0.23240104,  0.26803994,  0.20018959, ...,  0.14050414,\n",
              "          0.5726181 ,  0.8199185 ],\n",
              "        ...,\n",
              "        [-0.49607715,  0.6289018 , -0.21585928, ...,  0.24613945,\n",
              "          0.36824536,  0.94891727],\n",
              "        [-0.01309425, -0.17720453, -0.2521893 , ...,  0.33842582,\n",
              "          0.32744256,  1.292201  ],\n",
              "        [-0.13868734, -0.05952802,  0.23044676, ...,  0.20127097,\n",
              "          0.6497846 ,  0.60688365]],\n",
              "\n",
              "       [[-1.3600357 ,  0.46526414, -0.39776823, ...,  0.36368027,\n",
              "          0.47651643,  0.64979076],\n",
              "        [-1.1974528 ,  0.07865082, -0.36029804, ...,  0.5826772 ,\n",
              "          0.4449163 ,  0.33217937],\n",
              "        [-0.8484713 ,  1.115928  , -0.9435223 , ...,  1.0377984 ,\n",
              "          0.40096515,  0.69551814],\n",
              "        ...,\n",
              "        [-0.6084854 ,  0.44398367, -0.3556827 , ...,  0.85039663,\n",
              "         -0.03364396,  0.70600116],\n",
              "        [-0.32737535,  0.22018637, -0.6730174 , ...,  1.1698229 ,\n",
              "          0.19273098,  0.56345004],\n",
              "        [-0.9822253 ,  0.6092771 , -0.83141756, ...,  0.69835776,\n",
              "          0.06658751,  0.6597123 ]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzGkkFBc2sIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXw3bhf7tGfJ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(speech, text, speech_lengths, text_lengths):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = transformer(speech)\n",
        "\n",
        "    labels = text\n",
        "    label_length = text_lengths\n",
        "    logit_length = speech_lengths\n",
        "    unique = tf.nn.ctc_unique_labels(text)\n",
        "\n",
        "\n",
        "    loss = tf.nn.ctc_loss(\n",
        "        labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
        "    )\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdWdYE11Kea"
      },
      "source": [
        "EPOCHS = 5"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhEzjP3ztJYz",
        "outputId": "941d6e6e-7fe8-48cf-a279-e10b91cf953a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, data) in enumerate(dataset):\n",
        "    #print(batch)\n",
        "    #if batch == 9:\n",
        "    #  print(tokenizer.detokenize(data['text']).numpy()[0].decode('utf-8'))\n",
        "    #  print(data['text'].numpy()[0])\n",
        "    #  continue\n",
        "\n",
        "    train_step(data['speech'], data['text'], data['speech_lengths'], data['text_lengths'])\n",
        "\n",
        "    if batch % 1 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
        "\n",
        "  break\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 174.3032\n",
            "Epoch 1 Batch 1 Loss 174.5954\n",
            "Epoch 1 Batch 2 Loss 173.0407\n",
            "Epoch 1 Batch 3 Loss 173.3390\n",
            "Epoch 1 Batch 4 Loss 171.6476\n",
            "Epoch 1 Batch 5 Loss 171.2476\n",
            "Epoch 1 Batch 6 Loss 170.6131\n",
            "Epoch 1 Batch 7 Loss 169.4121\n",
            "Epoch 1 Batch 8 Loss 168.5527\n",
            "Epoch 1 Batch 9 Loss 167.4898\n",
            "Epoch 1 Batch 10 Loss 166.2850\n",
            "Epoch 1 Batch 11 Loss 165.1365\n",
            "Epoch 1 Batch 12 Loss 164.1532\n",
            "Epoch 1 Batch 13 Loss 163.0261\n",
            "Epoch 1 Batch 14 Loss 162.3440\n",
            "Epoch 1 Batch 15 Loss 161.8528\n",
            "Epoch 1 Batch 16 Loss 160.9754\n",
            "Epoch 1 Batch 17 Loss 160.1003\n",
            "Epoch 1 Batch 18 Loss 159.4562\n",
            "Epoch 1 Batch 19 Loss 158.6361\n",
            "Epoch 1 Batch 20 Loss 157.8520\n",
            "Epoch 1 Batch 21 Loss 157.1543\n",
            "Epoch 1 Batch 22 Loss 156.2693\n",
            "Epoch 1 Batch 23 Loss 155.9056\n",
            "Epoch 1 Batch 24 Loss 155.1221\n",
            "Epoch 1 Batch 25 Loss 154.5388\n",
            "Epoch 1 Batch 26 Loss 153.9061\n",
            "Epoch 1 Batch 27 Loss 153.1711\n",
            "Epoch 1 Batch 28 Loss 152.6128\n",
            "Epoch 1 Batch 29 Loss 152.1205\n",
            "Epoch 1 Batch 30 Loss 151.5712\n",
            "Epoch 1 Batch 31 Loss 151.0845\n",
            "Epoch 1 Batch 32 Loss 150.3888\n",
            "Epoch 1 Batch 33 Loss 149.7721\n",
            "Epoch 1 Batch 34 Loss 149.0922\n",
            "Epoch 1 Batch 35 Loss 148.4458\n",
            "Epoch 1 Batch 36 Loss 147.8318\n",
            "Epoch 1 Batch 37 Loss 147.6396\n",
            "Epoch 1 Batch 38 Loss 147.0694\n",
            "Epoch 1 Batch 39 Loss 146.5414\n",
            "Epoch 1 Batch 40 Loss 145.9715\n",
            "Epoch 1 Batch 41 Loss 145.4586\n",
            "Epoch 1 Batch 42 Loss 144.9349\n",
            "Epoch 1 Batch 43 Loss 144.5237\n",
            "Epoch 1 Batch 44 Loss 144.0923\n",
            "Epoch 1 Batch 45 Loss 143.6391\n",
            "Epoch 1 Batch 46 Loss 143.1522\n",
            "Epoch 1 Batch 47 Loss 142.6344\n",
            "Epoch 1 Batch 48 Loss 142.1134\n",
            "Epoch 1 Batch 49 Loss 141.6612\n",
            "Epoch 1 Batch 50 Loss 141.2048\n",
            "Epoch 1 Batch 51 Loss 140.7175\n",
            "Epoch 1 Batch 52 Loss 140.2083\n",
            "Epoch 1 Batch 53 Loss 139.8528\n",
            "Epoch 1 Batch 54 Loss 139.3976\n",
            "Epoch 1 Batch 55 Loss 139.0345\n",
            "Epoch 1 Batch 56 Loss 138.5948\n",
            "Epoch 1 Batch 57 Loss 138.1905\n",
            "Epoch 1 Batch 58 Loss 137.7906\n",
            "Epoch 1 Batch 59 Loss 137.4182\n",
            "Epoch 1 Batch 60 Loss 137.0964\n",
            "Epoch 1 Batch 61 Loss 136.7419\n",
            "Epoch 1 Batch 62 Loss 136.3586\n",
            "Epoch 1 Batch 63 Loss 136.0051\n",
            "Epoch 1 Batch 64 Loss 135.6105\n",
            "Epoch 1 Batch 65 Loss 135.2744\n",
            "Epoch 1 Batch 66 Loss 134.9597\n",
            "Epoch 1 Batch 67 Loss 134.6603\n",
            "Epoch 1 Batch 68 Loss 134.2707\n",
            "Epoch 1 Batch 69 Loss 133.8926\n",
            "Epoch 1 Batch 70 Loss 133.5439\n",
            "Epoch 1 Batch 71 Loss 133.1999\n",
            "Epoch 1 Batch 72 Loss 132.8540\n",
            "Epoch 1 Batch 73 Loss 132.5200\n",
            "Epoch 1 Batch 74 Loss 132.1865\n",
            "Epoch 1 Batch 75 Loss 131.8633\n",
            "Epoch 1 Batch 76 Loss 131.5599\n",
            "Epoch 1 Batch 77 Loss 131.2446\n",
            "Epoch 1 Batch 78 Loss 130.9573\n",
            "Epoch 1 Batch 79 Loss 130.6845\n",
            "Epoch 1 Batch 80 Loss 130.3878\n",
            "Epoch 1 Batch 81 Loss 130.1151\n",
            "Epoch 1 Batch 82 Loss 129.8389\n",
            "Epoch 1 Batch 83 Loss 129.5799\n",
            "Epoch 1 Batch 84 Loss 129.3005\n",
            "Epoch 1 Batch 85 Loss 129.0872\n",
            "Epoch 1 Batch 86 Loss 128.8135\n",
            "Epoch 1 Batch 87 Loss 128.5444\n",
            "Epoch 1 Batch 88 Loss 128.2859\n",
            "Epoch 1 Batch 89 Loss 128.0194\n",
            "Epoch 1 Batch 90 Loss 127.7615\n",
            "Epoch 1 Batch 91 Loss 127.5551\n",
            "Epoch 1 Batch 92 Loss 127.2975\n",
            "Epoch 1 Batch 93 Loss 127.0512\n",
            "Epoch 1 Batch 94 Loss 126.8139\n",
            "Epoch 1 Batch 95 Loss 126.5902\n",
            "Epoch 1 Batch 96 Loss 126.3367\n",
            "Epoch 1 Batch 97 Loss 126.0896\n",
            "Epoch 1 Batch 98 Loss 125.8673\n",
            "Epoch 1 Batch 99 Loss 125.6438\n",
            "Epoch 1 Batch 100 Loss 125.4173\n",
            "Epoch 1 Batch 101 Loss 125.1923\n",
            "Epoch 1 Batch 102 Loss 125.0108\n",
            "Epoch 1 Batch 103 Loss 124.7888\n",
            "Epoch 1 Batch 104 Loss 124.5648\n",
            "Epoch 1 Batch 105 Loss 124.3462\n",
            "Epoch 1 Batch 106 Loss 124.1455\n",
            "Epoch 1 Batch 107 Loss 123.9392\n",
            "Epoch 1 Batch 108 Loss 123.7623\n",
            "Epoch 1 Batch 109 Loss 123.5631\n",
            "Epoch 1 Batch 110 Loss 123.3709\n",
            "Epoch 1 Batch 111 Loss 123.1680\n",
            "Epoch 1 Batch 112 Loss 122.9694\n",
            "Epoch 1 Batch 113 Loss 122.8116\n",
            "Epoch 1 Batch 114 Loss 122.6133\n",
            "Epoch 1 Batch 115 Loss 122.4517\n",
            "Epoch 1 Batch 116 Loss 122.2580\n",
            "Epoch 1 Batch 117 Loss 122.0682\n",
            "Epoch 1 Batch 118 Loss 121.8806\n",
            "Epoch 1 Batch 119 Loss 121.7084\n",
            "Epoch 1 Batch 120 Loss 121.5942\n",
            "Epoch 1 Batch 121 Loss 121.4050\n",
            "Epoch 1 Batch 122 Loss 121.2187\n",
            "Epoch 1 Batch 123 Loss 121.0474\n",
            "Epoch 1 Batch 124 Loss 120.8919\n",
            "Epoch 1 Batch 125 Loss 120.7315\n",
            "Epoch 1 Batch 126 Loss 120.5762\n",
            "Epoch 1 Batch 127 Loss 120.3998\n",
            "Epoch 1 Batch 128 Loss 120.2479\n",
            "Epoch 1 Batch 129 Loss 120.1055\n",
            "Epoch 1 Batch 130 Loss 119.9400\n",
            "Epoch 1 Batch 131 Loss 119.7890\n",
            "Epoch 1 Batch 132 Loss 119.6576\n",
            "Epoch 1 Batch 133 Loss 119.5063\n",
            "Epoch 1 Batch 134 Loss 119.3820\n",
            "Epoch 1 Batch 135 Loss 119.2435\n",
            "Epoch 1 Batch 136 Loss 119.1068\n",
            "Epoch 1 Batch 137 Loss 119.0019\n",
            "Epoch 1 Batch 138 Loss 118.8853\n",
            "Epoch 1 Batch 139 Loss 118.7556\n",
            "Epoch 1 Batch 140 Loss 118.6250\n",
            "Epoch 1 Batch 141 Loss 118.4832\n",
            "Epoch 1 Batch 142 Loss 118.3705\n",
            "Epoch 1 Batch 143 Loss 118.2258\n",
            "Epoch 1 Batch 144 Loss 118.0879\n",
            "Epoch 1 Batch 145 Loss 117.9848\n",
            "Epoch 1 Batch 146 Loss 117.9191\n",
            "Epoch 1 Batch 147 Loss 117.8060\n",
            "Epoch 1 Batch 148 Loss 117.6965\n",
            "Epoch 1 Batch 149 Loss 117.5895\n",
            "Epoch 1 Batch 150 Loss 117.5261\n",
            "Epoch 1 Batch 151 Loss 117.4345\n",
            "Epoch 1 Batch 152 Loss 117.3113\n",
            "Epoch 1 Batch 153 Loss 117.1815\n",
            "Epoch 1 Batch 154 Loss 117.0555\n",
            "Epoch 1 Batch 155 Loss 116.9483\n",
            "Epoch 1 Batch 156 Loss 116.8714\n",
            "Epoch 1 Batch 157 Loss 116.7496\n",
            "Epoch 1 Batch 158 Loss 116.6479\n",
            "Epoch 1 Batch 159 Loss 116.5318\n",
            "Epoch 1 Batch 160 Loss 116.4184\n",
            "Epoch 1 Batch 161 Loss 116.2990\n",
            "Epoch 1 Batch 162 Loss 116.1812\n",
            "Epoch 1 Batch 163 Loss 116.0791\n",
            "Epoch 1 Batch 164 Loss 115.9715\n",
            "Epoch 1 Batch 165 Loss 115.8827\n",
            "Epoch 1 Batch 166 Loss 115.7868\n",
            "Epoch 1 Batch 167 Loss 115.6919\n",
            "Epoch 1 Batch 168 Loss 115.5830\n",
            "Epoch 1 Batch 169 Loss 115.4818\n",
            "Epoch 1 Batch 170 Loss 115.3801\n",
            "Epoch 1 Batch 171 Loss 115.2874\n",
            "Epoch 1 Batch 172 Loss 115.1882\n",
            "Epoch 1 Batch 173 Loss 115.0984\n",
            "Epoch 1 Batch 174 Loss 115.0139\n",
            "Epoch 1 Batch 175 Loss 114.9275\n",
            "Epoch 1 Batch 176 Loss 114.8209\n",
            "Epoch 1 Batch 177 Loss 114.7188\n",
            "Epoch 1 Batch 178 Loss 114.6147\n",
            "Epoch 1 Batch 179 Loss 114.5103\n",
            "Epoch 1 Batch 180 Loss 114.4222\n",
            "Epoch 1 Batch 181 Loss 114.3344\n",
            "Epoch 1 Batch 182 Loss 114.2416\n",
            "Epoch 1 Batch 183 Loss 114.1505\n",
            "Epoch 1 Batch 184 Loss 114.0724\n",
            "Epoch 1 Batch 185 Loss 113.9869\n",
            "Epoch 1 Batch 186 Loss 113.8955\n",
            "Epoch 1 Batch 187 Loss 113.8066\n",
            "Epoch 1 Batch 188 Loss 113.7108\n",
            "Epoch 1 Batch 189 Loss 113.6320\n",
            "Epoch 1 Batch 190 Loss 113.5453\n",
            "Epoch 1 Batch 191 Loss 113.4575\n",
            "Epoch 1 Batch 192 Loss 113.3790\n",
            "Epoch 1 Batch 193 Loss 113.2878\n",
            "Epoch 1 Batch 194 Loss 113.2064\n",
            "Epoch 1 Batch 195 Loss 113.1358\n",
            "Epoch 1 Batch 196 Loss 113.0514\n",
            "Epoch 1 Batch 197 Loss 112.9707\n",
            "Epoch 1 Batch 198 Loss 112.8864\n",
            "Epoch 1 Batch 199 Loss 112.8147\n",
            "Epoch 1 Batch 200 Loss 112.7425\n",
            "Epoch 1 Batch 201 Loss 112.6717\n",
            "Epoch 1 Batch 202 Loss 112.5887\n",
            "Epoch 1 Batch 203 Loss 112.5090\n",
            "Epoch 1 Batch 204 Loss 112.4300\n",
            "Epoch 1 Batch 205 Loss 112.3551\n",
            "Epoch 1 Batch 206 Loss 112.2827\n",
            "Epoch 1 Batch 207 Loss 112.2106\n",
            "Epoch 1 Batch 208 Loss 112.1392\n",
            "Epoch 1 Batch 209 Loss 112.0659\n",
            "Epoch 1 Batch 210 Loss 111.9918\n",
            "Epoch 1 Batch 211 Loss 111.9161\n",
            "Epoch 1 Batch 212 Loss 111.8491\n",
            "Epoch 1 Batch 213 Loss 111.7728\n",
            "Epoch 1 Batch 214 Loss 111.7080\n",
            "Epoch 1 Batch 215 Loss 111.6346\n",
            "Epoch 1 Batch 216 Loss 111.5643\n",
            "Epoch 1 Batch 217 Loss 111.5007\n",
            "Epoch 1 Batch 218 Loss 111.4288\n",
            "Epoch 1 Batch 219 Loss 111.3569\n",
            "Epoch 1 Batch 220 Loss 111.2935\n",
            "Epoch 1 Batch 221 Loss 111.2239\n",
            "Epoch 1 Batch 222 Loss 111.1523\n",
            "Epoch 1 Batch 223 Loss 111.0834\n",
            "Epoch 1 Batch 224 Loss 111.0219\n",
            "Epoch 1 Batch 225 Loss 110.9538\n",
            "Epoch 1 Batch 226 Loss 110.8949\n",
            "Epoch 1 Batch 227 Loss 110.8445\n",
            "Epoch 1 Batch 228 Loss 110.7998\n",
            "Epoch 1 Batch 229 Loss 110.7323\n",
            "Epoch 1 Batch 230 Loss 110.6797\n",
            "Epoch 1 Batch 231 Loss 110.6347\n",
            "Epoch 1 Batch 232 Loss 110.5765\n",
            "Epoch 1 Batch 233 Loss 110.5112\n",
            "Epoch 1 Batch 234 Loss 110.4619\n",
            "Epoch 1 Batch 235 Loss 110.3979\n",
            "Epoch 1 Batch 236 Loss 110.3359\n",
            "Epoch 1 Batch 237 Loss 110.2760\n",
            "Epoch 1 Batch 238 Loss 110.2090\n",
            "Epoch 1 Batch 239 Loss 110.1443\n",
            "Epoch 1 Batch 240 Loss 110.0833\n",
            "Epoch 1 Batch 241 Loss 110.0237\n",
            "Epoch 1 Batch 242 Loss 109.9645\n",
            "Epoch 1 Batch 243 Loss 109.9019\n",
            "Epoch 1 Batch 244 Loss 109.8479\n",
            "Epoch 1 Batch 245 Loss 109.7843\n",
            "Epoch 1 Batch 246 Loss 109.7216\n",
            "Epoch 1 Batch 247 Loss 109.6651\n",
            "Epoch 1 Batch 248 Loss 109.6050\n",
            "Epoch 1 Batch 249 Loss 109.5493\n",
            "Epoch 1 Batch 250 Loss 109.4945\n",
            "Epoch 1 Batch 251 Loss 109.4461\n",
            "Epoch 1 Batch 252 Loss 109.3916\n",
            "Epoch 1 Batch 253 Loss 109.3333\n",
            "Epoch 1 Batch 254 Loss 109.2736\n",
            "Epoch 1 Batch 255 Loss 109.2169\n",
            "Epoch 1 Batch 256 Loss 109.1615\n",
            "Epoch 1 Batch 257 Loss 109.1058\n",
            "Epoch 1 Batch 258 Loss 109.0501\n",
            "Epoch 1 Batch 259 Loss 108.9934\n",
            "Epoch 1 Batch 260 Loss 108.9469\n",
            "Epoch 1 Batch 261 Loss 108.8928\n",
            "Epoch 1 Batch 262 Loss 108.8429\n",
            "Epoch 1 Batch 263 Loss 108.7874\n",
            "Epoch 1 Batch 264 Loss 108.7372\n",
            "Epoch 1 Batch 265 Loss 108.6851\n",
            "Epoch 1 Batch 266 Loss 108.6335\n",
            "Epoch 1 Batch 267 Loss 108.5814\n",
            "Epoch 1 Batch 268 Loss 108.5303\n",
            "Epoch 1 Batch 269 Loss 108.4882\n",
            "Epoch 1 Batch 270 Loss 108.4369\n",
            "Epoch 1 Batch 271 Loss 108.3853\n",
            "Epoch 1 Batch 272 Loss 108.3401\n",
            "Epoch 1 Batch 273 Loss 108.2921\n",
            "Epoch 1 Batch 274 Loss 108.2384\n",
            "Epoch 1 Batch 275 Loss 108.1865\n",
            "Epoch 1 Batch 276 Loss 108.1420\n",
            "Epoch 1 Batch 277 Loss 108.0922\n",
            "Epoch 1 Batch 278 Loss 108.0416\n",
            "Epoch 1 Batch 279 Loss 107.9974\n",
            "Epoch 1 Batch 280 Loss 107.9475\n",
            "Epoch 1 Batch 281 Loss 107.8970\n",
            "Epoch 1 Batch 282 Loss 107.8513\n",
            "Epoch 1 Batch 283 Loss 107.8042\n",
            "Epoch 1 Batch 284 Loss 107.7571\n",
            "Epoch 1 Batch 285 Loss 107.7066\n",
            "Epoch 1 Batch 286 Loss 107.6570\n",
            "Epoch 1 Batch 287 Loss 107.6075\n",
            "Epoch 1 Batch 288 Loss 107.5612\n",
            "Epoch 1 Batch 289 Loss 107.5113\n",
            "Epoch 1 Batch 290 Loss 107.4646\n",
            "Epoch 1 Batch 291 Loss 107.4157\n",
            "Epoch 1 Batch 292 Loss 107.3678\n",
            "Epoch 1 Batch 293 Loss 107.3172\n",
            "Epoch 1 Batch 294 Loss 107.2669\n",
            "Epoch 1 Batch 295 Loss 107.2156\n",
            "Epoch 1 Batch 296 Loss 107.1641\n",
            "Epoch 1 Batch 297 Loss 107.1187\n",
            "Epoch 1 Batch 298 Loss 107.0695\n",
            "Epoch 1 Batch 299 Loss 107.0210\n",
            "Epoch 1 Batch 300 Loss 106.9728\n",
            "Epoch 1 Batch 301 Loss 106.9263\n",
            "Epoch 1 Batch 302 Loss 106.8749\n",
            "Epoch 1 Batch 303 Loss 106.8241\n",
            "Epoch 1 Batch 304 Loss 106.7739\n",
            "Epoch 1 Batch 305 Loss 106.7297\n",
            "Epoch 1 Batch 306 Loss 106.6834\n",
            "Epoch 1 Batch 307 Loss 106.6353\n",
            "Epoch 1 Batch 308 Loss 106.5916\n",
            "Epoch 1 Batch 309 Loss 106.5529\n",
            "Epoch 1 Batch 310 Loss 106.5098\n",
            "Epoch 1 Batch 311 Loss 106.4582\n",
            "Epoch 1 Batch 312 Loss 106.4130\n",
            "Epoch 1 Batch 313 Loss 106.3622\n",
            "Epoch 1 Batch 314 Loss 106.3200\n",
            "Epoch 1 Batch 315 Loss 106.2730\n",
            "Epoch 1 Batch 316 Loss 106.2239\n",
            "Epoch 1 Batch 317 Loss 106.1795\n",
            "Epoch 1 Batch 318 Loss 106.1349\n",
            "Epoch 1 Batch 319 Loss 106.0864\n",
            "Epoch 1 Batch 320 Loss 106.0442\n",
            "Epoch 1 Batch 321 Loss 105.9988\n",
            "Epoch 1 Batch 322 Loss 105.9535\n",
            "Epoch 1 Batch 323 Loss 105.9126\n",
            "Epoch 1 Batch 324 Loss 105.8660\n",
            "Epoch 1 Batch 325 Loss 105.8374\n",
            "Epoch 1 Batch 326 Loss 105.7914\n",
            "Epoch 1 Batch 327 Loss 105.7442\n",
            "Epoch 1 Batch 328 Loss 105.6972\n",
            "Epoch 1 Batch 329 Loss 105.6520\n",
            "Epoch 1 Batch 330 Loss 105.6102\n",
            "Epoch 1 Batch 331 Loss 105.5641\n",
            "Epoch 1 Batch 332 Loss 105.5230\n",
            "Epoch 1 Batch 333 Loss 105.4786\n",
            "Epoch 1 Batch 334 Loss 105.4333\n",
            "Epoch 1 Batch 335 Loss 105.3937\n",
            "Epoch 1 Batch 336 Loss 105.3517\n",
            "Epoch 1 Batch 337 Loss 105.3115\n",
            "Epoch 1 Batch 338 Loss 105.2683\n",
            "Epoch 1 Batch 339 Loss 105.2253\n",
            "Epoch 1 Batch 340 Loss 105.1919\n",
            "Epoch 1 Batch 341 Loss 105.1496\n",
            "Epoch 1 Batch 342 Loss 105.1040\n",
            "Epoch 1 Batch 343 Loss 105.0635\n",
            "Epoch 1 Batch 344 Loss 105.0236\n",
            "Epoch 1 Batch 345 Loss 104.9828\n",
            "Epoch 1 Batch 346 Loss 104.9394\n",
            "Epoch 1 Batch 347 Loss 104.8960\n",
            "Epoch 1 Batch 348 Loss 104.8545\n",
            "Epoch 1 Batch 349 Loss 104.8109\n",
            "Epoch 1 Batch 350 Loss 104.7672\n",
            "Epoch 1 Batch 351 Loss 104.7201\n",
            "Epoch 1 Batch 352 Loss 104.6768\n",
            "Epoch 1 Batch 353 Loss 104.6376\n",
            "Epoch 1 Batch 354 Loss 104.5924\n",
            "Epoch 1 Batch 355 Loss 104.5593\n",
            "Epoch 1 Batch 356 Loss 104.5188\n",
            "Epoch 1 Batch 357 Loss 104.4820\n",
            "Epoch 1 Batch 358 Loss 104.4366\n",
            "Epoch 1 Batch 359 Loss 104.3995\n",
            "Epoch 1 Batch 360 Loss 104.3620\n",
            "Epoch 1 Batch 361 Loss 104.3202\n",
            "Epoch 1 Batch 362 Loss 104.2794\n",
            "Epoch 1 Batch 363 Loss 104.2388\n",
            "Epoch 1 Batch 364 Loss 104.1983\n",
            "Epoch 1 Batch 365 Loss 104.1529\n",
            "Epoch 1 Batch 366 Loss 104.1125\n",
            "Epoch 1 Batch 367 Loss 104.0718\n",
            "Epoch 1 Batch 368 Loss 104.0359\n",
            "Epoch 1 Batch 369 Loss 103.9993\n",
            "Epoch 1 Batch 370 Loss 103.9596\n",
            "Epoch 1 Batch 371 Loss 103.9179\n",
            "Epoch 1 Batch 372 Loss 103.8872\n",
            "Epoch 1 Batch 373 Loss 103.8496\n",
            "Epoch 1 Batch 374 Loss 103.8129\n",
            "Epoch 1 Batch 375 Loss 103.7755\n",
            "Epoch 1 Batch 376 Loss 103.7387\n",
            "Epoch 1 Batch 377 Loss 103.7007\n",
            "Epoch 1 Batch 378 Loss 103.6659\n",
            "Epoch 1 Batch 379 Loss 103.6313\n",
            "Epoch 1 Batch 380 Loss 103.5914\n",
            "Epoch 1 Batch 381 Loss 103.5559\n",
            "Epoch 1 Batch 382 Loss 103.5190\n",
            "Epoch 1 Batch 383 Loss 103.4777\n",
            "Epoch 1 Batch 384 Loss 103.4373\n",
            "Epoch 1 Batch 385 Loss 103.3965\n",
            "Epoch 1 Batch 386 Loss 103.3565\n",
            "Epoch 1 Batch 387 Loss 103.3193\n",
            "Epoch 1 Batch 388 Loss 103.2835\n",
            "Epoch 1 Batch 389 Loss 103.2510\n",
            "Epoch 1 Batch 390 Loss 103.2145\n",
            "Epoch 1 Batch 391 Loss 103.1732\n",
            "Epoch 1 Batch 392 Loss 103.1340\n",
            "Epoch 1 Batch 393 Loss 103.0975\n",
            "Epoch 1 Batch 394 Loss 103.0613\n",
            "Epoch 1 Batch 395 Loss 103.0238\n",
            "Epoch 1 Batch 396 Loss 102.9858\n",
            "Epoch 1 Batch 397 Loss 102.9530\n",
            "Epoch 1 Batch 398 Loss 102.9162\n",
            "Epoch 1 Batch 399 Loss 102.8827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-226-181f6c19e8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#  continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speech_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNRSr_-N6PIN"
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kms_j2DFSuuC"
      },
      "source": [
        "## CTC Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlaNl6yQHH5T"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX7iY_45Z5Bc"
      },
      "source": [
        "#dense1 = tf.keras.layers.Dense(input_vocab_size)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpTmf2jMM3PM"
      },
      "source": [
        "dense1 = tf.keras.layers.Dense(2000)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Ix724hw3ED"
      },
      "source": [
        "dense1_speech = dense1(speech)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09GXSsbaEcEp",
        "outputId": "c672b732-ceba-46e3-acc6-106a6474c3d7"
      },
      "source": [
        "dense1_speech.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8, 500, 2000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh6J6szNDgZd"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "text_lengths = tf.map_fn(lambda x: jsut_text_lengths[x.numpy().decode('utf-8')], id, fn_output_signature=tf.int32)\n",
        "\n",
        "speech_lengths = tf.map_fn(lambda x: jsut_speech_lengths[x.numpy().decode('utf-8')], id, fn_output_signature=tf.int32)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b93DtOvAS7mA",
        "outputId": "370fcd49-1040-4dd9-c524-3ac201232666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "labels = text\n",
        "logits = dense1_speech\n",
        "label_length = text_lengths\n",
        "logit_length = speech_lengths\n",
        "unique = tf.nn.ctc_unique_labels(text)\n",
        "\n",
        "\n",
        "loss = tf.nn.ctc_loss(\n",
        "    labels=labels, logits=logits, label_length=label_length, logit_length=logit_length, logits_time_major=False, unique=unique,\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgFfg-U7NOnN",
        "outputId": "4229f3cc-cc76-4bd8-d54b-e005279add1a"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 8213451224441148177, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14674281152\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 5724101691350584524\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyeaq-FTCB7X",
        "outputId": "be43a59c-fdaf-455a-d760-c3902d276169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pykakasi"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pykakasi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/b3/a8dd0164de184fdd0fd06831b6178fcbc3d32b3df79e81591ce517fde426/pykakasi-2.0.6-py3-none-any.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from pykakasi) (3.7.0)\n",
            "Collecting klepto\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/6d/65756006b3c7b46cc30010312bc192fe04095079e0e2e47822148ea83bc1/klepto-0.2.0-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->pykakasi) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->pykakasi) (3.7.4.3)\n",
            "Requirement already satisfied: dill>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from klepto->pykakasi) (0.3.3)\n",
            "Collecting pox>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/06/600e1b1103336ce94cf01e63a6c7b134f0edccf59bbf99189c975e2f257e/pox-0.2.9-py2.py3-none-any.whl\n",
            "Installing collected packages: pox, klepto, pykakasi\n",
            "Successfully installed klepto-0.2.0 pox-0.2.9 pykakasi-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcQFbUjdCOE-"
      },
      "source": [
        "import pykakasi\n",
        "converter = pykakasi.kakasi()"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3hH4fN0CP4H"
      },
      "source": [
        ""
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAZCHeyTCd_l",
        "outputId": "56cb1818-40f3-43ef-e9ac-1d20cab368de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "kana = converter.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')\n",
        "''.join([i['hira'] for i in kana])"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'えぞにこもるきゅうばくふぐんにたいするこうげきのしきをとる。'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCB8rPpXCC1y",
        "outputId": "08302100-7ba8-4004-d20e-565ef58ad6a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kks.convert('蝦夷に籠もる旧幕府軍に対する攻撃の指揮を執る。')"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'hepburn': 'ezo',\n",
              "  'hira': 'えぞ',\n",
              "  'kana': 'エゾ',\n",
              "  'kunrei': 'ezo',\n",
              "  'orig': '蝦夷',\n",
              "  'passport': 'ezo'},\n",
              " {'hepburn': 'ni',\n",
              "  'hira': 'に',\n",
              "  'kana': 'ニ',\n",
              "  'kunrei': 'ni',\n",
              "  'orig': 'に',\n",
              "  'passport': 'ni'},\n",
              " {'hepburn': 'komo',\n",
              "  'hira': 'こも',\n",
              "  'kana': 'コモ',\n",
              "  'kunrei': 'komo',\n",
              "  'orig': '籠も',\n",
              "  'passport': 'komo'},\n",
              " {'hepburn': 'ru',\n",
              "  'hira': 'る',\n",
              "  'kana': 'ル',\n",
              "  'kunrei': 'ru',\n",
              "  'orig': 'る',\n",
              "  'passport': 'ru'},\n",
              " {'hepburn': 'kyuu',\n",
              "  'hira': 'きゅう',\n",
              "  'kana': 'キュウ',\n",
              "  'kunrei': 'kyuu',\n",
              "  'orig': '旧',\n",
              "  'passport': 'kyuu'},\n",
              " {'hepburn': 'bakufu',\n",
              "  'hira': 'ばくふ',\n",
              "  'kana': 'バクフ',\n",
              "  'kunrei': 'bakufu',\n",
              "  'orig': '幕府',\n",
              "  'passport': 'bakufu'},\n",
              " {'hepburn': 'gun',\n",
              "  'hira': 'ぐん',\n",
              "  'kana': 'グン',\n",
              "  'kunrei': 'gun',\n",
              "  'orig': '軍',\n",
              "  'passport': 'gun'},\n",
              " {'hepburn': 'ni',\n",
              "  'hira': 'に',\n",
              "  'kana': 'ニ',\n",
              "  'kunrei': 'ni',\n",
              "  'orig': 'に',\n",
              "  'passport': 'ni'},\n",
              " {'hepburn': 'taisuru',\n",
              "  'hira': 'たいする',\n",
              "  'kana': 'タイスル',\n",
              "  'kunrei': 'taisuru',\n",
              "  'orig': '対する',\n",
              "  'passport': 'taisuru'},\n",
              " {'hepburn': 'kougeki',\n",
              "  'hira': 'こうげき',\n",
              "  'kana': 'コウゲキ',\n",
              "  'kunrei': 'kougeki',\n",
              "  'orig': '攻撃',\n",
              "  'passport': 'kogeki'},\n",
              " {'hepburn': 'no',\n",
              "  'hira': 'の',\n",
              "  'kana': 'ノ',\n",
              "  'kunrei': 'no',\n",
              "  'orig': 'の',\n",
              "  'passport': 'no'},\n",
              " {'hepburn': 'shiki',\n",
              "  'hira': 'しき',\n",
              "  'kana': 'シキ',\n",
              "  'kunrei': 'siki',\n",
              "  'orig': '指揮',\n",
              "  'passport': 'shiki'},\n",
              " {'hepburn': 'wo',\n",
              "  'hira': 'を',\n",
              "  'kana': 'ヲ',\n",
              "  'kunrei': 'wo',\n",
              "  'orig': 'を',\n",
              "  'passport': 'wo'},\n",
              " {'hepburn': 'toru',\n",
              "  'hira': 'とる',\n",
              "  'kana': 'トル',\n",
              "  'kunrei': 'toru',\n",
              "  'orig': '執る',\n",
              "  'passport': 'toru'},\n",
              " {'hepburn': '.',\n",
              "  'hira': '。',\n",
              "  'kana': '。',\n",
              "  'kunrei': '.',\n",
              "  'orig': '。',\n",
              "  'passport': '.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvCWm5_LC7Ex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}